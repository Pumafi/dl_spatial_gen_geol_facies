{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pumafi/dl_spatial_gen_geol_facies/blob/main/ScoreBased_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Score-Based Generative Modeling (Tensorflow)\n",
        "\n",
        "Translation of \"Generative Modeling by Estimating Gradients of the Data Distribution\" by Yang Song from PyTorch to Tensorflow \n",
        "\n",
        "Source Tutorial : https://yang-song.net/blog/2021/score/\n",
        "\n",
        "Source PyTorch code : https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing#scrollTo=YyQtV7155Nht"
      ],
      "metadata": {
        "id": "B0RRu-tvfZiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm -U\n",
        "!python3 -m pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DYcAgH7jG3F",
        "outputId": "67ce4b12-fafa-4958-cab4-e6a4e658b259"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow_addons) (23.0)\n",
            "Collecting typeguard>=2.7\n",
            "  Downloading typeguard-3.0.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow_addons) (6.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow_addons) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.7->tensorflow_addons) (3.15.0)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.19.0 typeguard-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "LGFJUCPQapJr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "GbbDELMcxA-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianFourierProjection(tf.keras.layers.Layer):\n",
        "    \"\"\"Gaussian random features for encoding time steps.\"\"\"  \n",
        "    def __init__(self, embed_dim, scale=30.):\n",
        "        super().__init__()\n",
        "        # Randomly sample weights during initialization. These weights are fixed \n",
        "        # during optimization and are not trainable.\n",
        "        self.W = self.add_weight(shape=(embed_dim // 2,),\n",
        "                                 trainable=False,\n",
        "                                 initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.), name=\"GFP\") * tf.constant(scale, dtype=tf.float32)\n",
        "        \n",
        "    def call(self, x):\n",
        "        x_proj = tf.expand_dims(x, axis=-1) * tf.expand_dims(self.W, axis=0) * tf.constant(2., dtype=tf.float32) * tf.constant(np.pi, dtype=tf.float32)\n",
        "        y = tf.concat([tf.math.sin(x_proj), tf.cos(x_proj)], axis=-1)\n",
        "        return y # Probleme vient pas de là :()\n",
        "\n",
        "class CustomLinear(tf.keras.layers.Layer):\n",
        "    \"\"\"Rhaaah.\"\"\"  \n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.W = tf.random.uniform((input_dim, output_dim), minval=-tf.math.sqrt(1/input_dim), maxval=tf.math.sqrt(1/input_dim))\n",
        "        self.b = tf.random.uniform((1, output_dim, ), minval=-tf.math.sqrt(1/input_dim), maxval=tf.math.sqrt(1/input_dim))\n",
        "        \n",
        "    def call(self, x):\n",
        "        y = tf.tensordot(x, self.W, 1) + self.b\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class DenseFeatures(tf.keras.layers.Layer):\n",
        "    \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.dense = CustomLinear(input_dim, output_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        y = self.dense(x)\n",
        "        return tf.expand_dims(tf.expand_dims(y, axis=1), axis=1)\n",
        "\n",
        "# TODO : Dense that can be used in convolutions ? Concat ?\n",
        "# ==> Used to apply to channels\n",
        "\n",
        "class ScoreNet(tf.keras.Model):\n",
        "    \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
        "    def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        # Layers\n",
        "        ## Gaussian random features embedding layer for time\n",
        "        self.embeding = tf.keras.Sequential([GaussianFourierProjection(embed_dim=embed_dim),\n",
        "                                             CustomLinear(embed_dim, embed_dim)]) # Problem vient de la Dense ?\n",
        "        \n",
        "        # Dimension Reduction\n",
        "        self.conv1 = tf.keras.layers.Conv2D(channels[0], (3, 3), strides=1, use_bias=False)\n",
        "        self.dense1 = DenseFeatures(embed_dim, channels[0])\n",
        "        self.gnorm1 = tfa.layers.GroupNormalization(4, epsilon=1e-05)\n",
        "\n",
        "        self.conv2 = tf.keras.layers.Conv2D(channels[1], (3, 3), strides=2, use_bias=False)\n",
        "        self.dense2 = DenseFeatures(embed_dim, channels[1])\n",
        "        self.gnorm2 = tfa.layers.GroupNormalization(32, epsilon=1e-05)\n",
        "\n",
        "        self.conv3 = tf.keras.layers.Conv2D(channels[2], (3, 3), strides=2, use_bias=False)\n",
        "        self.dense3 = DenseFeatures(embed_dim, channels[2])\n",
        "        self.gnorm3 = tfa.layers.GroupNormalization(32, epsilon=1e-05)\n",
        "\n",
        "        self.conv4 = tf.keras.layers.Conv2D(channels[3], (3, 3), strides=2, use_bias=False)\n",
        "        self.dense4 = DenseFeatures(embed_dim, channels[3])\n",
        "        self.gnorm4 = tfa.layers.GroupNormalization(32, epsilon=1e-05)\n",
        "\n",
        "        # Dimension Reconstruction\n",
        "        self.tconv4 = tf.keras.layers.Conv2DTranspose(channels[2], (3, 3), strides=2, use_bias=False)\n",
        "        self.tdense4 = DenseFeatures(embed_dim, channels[2])\n",
        "        self.tgnorm4 = tfa.layers.GroupNormalization(32, epsilon=1e-05)\n",
        "\n",
        "        self.tconv3 = tf.keras.layers.Conv2DTranspose(channels[1], (3, 3), strides=2, use_bias=False, output_padding=1)\n",
        "        self.tdense3 = DenseFeatures(embed_dim, channels[1])\n",
        "        self.tgnorm3 = tfa.layers.GroupNormalization(32, epsilon=1e-05)\n",
        "\n",
        "        self.tconv2 = tf.keras.layers.Conv2DTranspose(channels[0], (3, 3), strides=2, use_bias=False, output_padding=1)\n",
        "        self.tdense2 = DenseFeatures(embed_dim, channels[0])\n",
        "        self.tgnorm2 = tfa.layers.GroupNormalization(32, epsilon=1e-05)\n",
        "        # Gradient explosion lol\n",
        "        self.tconv1 = tf.keras.layers.Conv2DTranspose(1, (3, 3), strides=1)\n",
        "\n",
        "        self.act = lambda x: x * tf.math.sigmoid(x)\n",
        "        self.marginal_prob_std = marginal_prob_std\n",
        "\n",
        "    def call(self, x, t): \n",
        "      # Obtain the Gaussian random feature embedding for t\n",
        "      embed = self.act(self.embeding(t))\n",
        "\n",
        "      # Encoding path\n",
        "      h1 = self.conv1(x) \n",
        "      ## Incorporate information from t\n",
        "      h1 += self.dense1(embed)\n",
        "      ## Group normalization\n",
        "      h1 = self.gnorm1(h1)\n",
        "      h1 = self.act(h1)\n",
        "      h2 = self.conv2(h1)\n",
        "      h2 += self.dense2(embed)\n",
        "      h2 = self.gnorm2(h2)\n",
        "      h2 = self.act(h2)\n",
        "      h3 = self.conv3(h2)\n",
        "      h3 += self.dense3(embed)\n",
        "      h3 = self.gnorm3(h3)\n",
        "      h3 = self.act(h3)\n",
        "      h4 = self.conv4(h3)\n",
        "      h4 += self.dense4(embed)\n",
        "      h4 = self.gnorm4(h4)\n",
        "      h4 = self.act(h4)\n",
        "\n",
        "      # Decoding path\n",
        "      h = self.tconv4(h4)\n",
        "      ## Skip connection from the encoding path\n",
        "      h += self.tdense4(embed)\n",
        "      h = self.tgnorm4(h)\n",
        "      h = self.act(h)\n",
        "\n",
        "      h = self.tconv3(tf.concat([h, h3], axis=-1))\n",
        "      h += self.tdense3(embed)\n",
        "      h = self.tgnorm3(h)\n",
        "      h = self.act(h)\n",
        "\n",
        "      h = self.tconv2(tf.concat([h, h2], axis=-1))\n",
        "      h += self.tdense2(embed)\n",
        "      h = self.tgnorm2(h)\n",
        "      h = self.act(h)\n",
        "\n",
        "      h = self.tconv1(tf.concat([h, h1], axis=-1))\n",
        "\n",
        "      # Normalize output\n",
        "      h = h / self.marginal_prob_std(tf.reshape(t, (x.shape[0], 1, 1, 1)))\n",
        "      h = tf.reshape(h, x.shape)\n",
        "      return h"
      ],
      "metadata": {
        "id": "To_EB5sRarZd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the SDE"
      ],
      "metadata": {
        "id": "7QIyDOIYw-zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "@tf.function\n",
        "def marginal_prob_std(t, sigma=25.):\n",
        "    \"\"\"Compute the mean and standard deviation of $p_{0t}(x(t) | x(0))$.\n",
        "\n",
        "    Args:    \n",
        "      t: A vector of time steps.\n",
        "      sigma: The $\\sigma$ in our SDE.  \n",
        "    \n",
        "    Returns:\n",
        "      The standard deviation.\n",
        "    \"\"\"\n",
        "    return tf.math.sqrt((sigma**(2 * t) - 1.) / 2 / tf.math.log(sigma))\n",
        "\n",
        "@tf.function\n",
        "def diffusion_coeff(t, sigma=25.):\n",
        "  \"\"\"Compute the diffusion coefficient of our SDE.\n",
        "\n",
        "  Args:\n",
        "    t: A vector of time steps.\n",
        "    sigma: The $\\sigma$ in our SDE.\n",
        "  \n",
        "  Returns:\n",
        "    The vector of diffusion coefficients.\n",
        "  \"\"\"\n",
        "  return sigma**t\n",
        "\n",
        "sigma =  25.0# @param {'type':'number'}\n",
        "\n",
        "\n",
        "marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma)\n",
        "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma)"
      ],
      "metadata": {
        "id": "SsMgvUauc4Bx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# std is clear\n",
        "# y not okay\n",
        "# \n",
        "import time"
      ],
      "metadata": {
        "id": "monCiXibm8t6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ],
      "metadata": {
        "id": "jEzL16Jdm0gc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def loss_fn(model, x, marginal_prob_std, eps=1e-5):\n",
        "    random_t = tf.random.uniform((x.shape[0],), minval=0., maxval=1.0) * (1. - eps) + eps\n",
        "    z = tf.random.normal(x.shape)\n",
        "    std = marginal_prob_std(random_t)\n",
        "    perturbed_x = x + z * tf.reshape(std, (-1, 1, 1, 1))\n",
        "    score = model(perturbed_x, random_t)\n",
        "    y = score * tf.reshape(std, (-1, 1, 1, 1))\n",
        "    y = y + z\n",
        "    loss = tf.math.reduce_mean(tf.reduce_sum(y**2, axis=[1, 2, 3]))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "cRb4OXXAefkz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the data"
      ],
      "metadata": {
        "id": "3FtXDamAxC8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist, mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = np.expand_dims(x_train.astype(\"float32\") / 255, axis=-1)\n",
        "x_test = np.expand_dims(x_test.astype(\"float32\") / 255, axis=-1)"
      ],
      "metadata": {
        "id": "gLOw1wF9xEqm",
        "outputId": "5e745168-940b-45ef-8f41-c1e9e3382647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkUsBdHqzvJt",
        "outputId": "5c142803-e76c-4c17-bcf4-5f0e04fa2c4b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## size of a mini-batch\n",
        "batch_size =  32 #@param {'type':'integer'}\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n"
      ],
      "metadata": {
        "id": "CXMO7WWo2XSe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "GnN1NZijw8vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm"
      ],
      "metadata": {
        "id": "XDHFpjUFxdno"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import notebook"
      ],
      "metadata": {
        "id": "548_lDzfASF_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs=50#@param {'type':'integer'}\n",
        "\n",
        "## learning rate\n",
        "lr=1e-4 #@param {'type':'number'}\n",
        "\n",
        "score_model = ScoreNet(marginal_prob_std=marginal_prob_std_fn)\n",
        "\n",
        "optimizer = tf.keras.optimizers.experimental.Adam(learning_rate=lr)\n",
        "#tqdm_epoch = notebook.trange(n_epochs) #tqdm.notebook.trange(n_epochs)\n",
        "\n",
        "\n",
        "#for epoch in tqdm_epoch:\n",
        "for x in x_train.repeat(n_epochs):\n",
        "  avg_loss = 0.\n",
        "  num_items = 0\n",
        "  #for x, _ in train_dataset:\n",
        "  print(\"\\n\")\n",
        "  with tf.GradientTape() as tape:\n",
        "      start = time.time()\n",
        "      loss = loss_fn(score_model, x, marginal_prob_std_fn)\n",
        "      end = time.time()\n",
        "      print(end - start)\n",
        "\n",
        "  avg_loss += loss.numpy() * x.shape[0]\n",
        "  num_items += x.shape[0]\n",
        "  start = time.time()\n",
        "  gradients = tape.gradient(loss, score_model.trainable_variables)\n",
        "  end = time.time()\n",
        "  print(end - start)\n",
        "\n",
        "  start = time.time()\n",
        "  optimizer.apply_gradients(zip(gradients, score_model.trainable_variables))\n",
        "  end = time.time()\n",
        "  print(end - start)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  #tqdm_epoch.set_description('Average Loss: {:5f}'.format(avg_loss / num_items))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "TG8lE65gqQDf",
        "outputId": "fbc9f13b-7474-4466-bf0c-9a6b46526b38"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6dd84e3788e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarginal_prob_std_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ec1de52c6c1c>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(model, x, marginal_prob_std, eps)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarginal_prob_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrandom_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarginal_prob_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 25616.156604: 16%\n",
        "# 25753.208589: 18%\n",
        "\n"
      ],
      "metadata": {
        "id": "htnd8_uExgjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def aef"
      ],
      "metadata": {
        "id": "OMnzPSVPjbnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fmUQwDZcjav0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}