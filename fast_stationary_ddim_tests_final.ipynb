{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pumafi/dl_spatial_gen_geol_facies/blob/main/fast_stationary_ddim_tests_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "TA0koXfT2e7k",
        "outputId": "e46a34ac-5148-4087-a4f7-d3a84e8fefbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul  6 13:43:41 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P0    28W /  70W |  14573MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## IDEAS\n",
        "1.   Normalization in embedding ?\n",
        "2.   Formula when I replace beta by t\n",
        "3.   Use keras inference for potential changes\n",
        "\n",
        "## What to try next?\n",
        "\n",
        "If you would like to dive in deeper to the topic, a recommend checking out\n",
        "[this repository](https://github.com/beresandras/clear-diffusion-keras) that I created in\n",
        "preparation for this code example, which implements a wider range of features in a\n",
        "similar style, such as:\n",
        "\n",
        "* stochastic sampling\n",
        "* second-order sampling based on the\n",
        "[differential equation view of DDIMs (Equation 13)](https://arxiv.org/abs/2010.02502)\n",
        "* more diffusion schedules\n",
        "* more network output types: predicting image or\n",
        "[velocity (Appendix D)](https://arxiv.org/abs/2202.00512) instead of noise\n",
        "* more datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "rge41L-HIY-i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqkfNOJcD0Ym"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install deel.lip\n",
        "!python -m pip install -i https://test.pypi.org/simple/ gstlearn"
      ],
      "metadata": {
        "id": "u03Nq4eK2ems",
        "outputId": "8481c294-1b73-4769-8b6e-45281d7bfcc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deel.lip in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deel.lip) (1.22.4)\n",
            "Requirement already satisfied: tensorflow~=2.2 in /usr/local/lib/python3.10/dist-packages (from deel.lip) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.2->deel.lip) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.2->deel.lip) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.2->deel.lip) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (3.2.2)\n",
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Requirement already satisfied: gstlearn in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gstlearn) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gstlearn) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gstlearn) (1.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from gstlearn) (5.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gstlearn) (1.5.3)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.10/dist-packages (from gstlearn) (0.13.2)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from gstlearn) (2.0.1)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas->gstlearn) (1.9.4.post1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas->gstlearn) (23.1)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas->gstlearn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gstlearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gstlearn) (2022.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gstlearn) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gstlearn) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gstlearn) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gstlearn) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gstlearn) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gstlearn) (3.1.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->gstlearn) (8.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->gstlearn) (23.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->gstlearn) (2023.5.7)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->gstlearn) (8.1.3)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->gstlearn) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->gstlearn) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->gstlearn) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RUNNING_IN_COLAB = True\n",
        "\n",
        "if RUNNING_IN_COLAB:\n",
        "    # Uses a private Auth Token, giving read and write access to repo\n",
        "    # TO DELETE IF REPO GOES PUBLIC\n",
        "    REPO_URL = 'https://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@github.com/Pumafi/flumy-wgan-mines'\n",
        "    BRANCH   = 'main'\n",
        "    REPO_DIR = 'flumy-wgan-mines'\n",
        "\n",
        "    from pathlib import Path\n",
        "\n",
        "    %cd /content\n",
        "\n",
        "    if Path(REPO_DIR).is_dir():\n",
        "      !rm -rf {REPO_DIR}\n",
        "\n",
        "    # Download the repository\n",
        "    if not Path(REPO_DIR).is_dir():\n",
        "        !git clone --branch {BRANCH} --depth=1 -- {REPO_URL} {REPO_DIR}\n",
        "\n",
        "    %cd {REPO_DIR}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqPH8VxsGGrb",
        "outputId": "20415c3b-d419-4b89-8d3c-474551c27a0e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'flumy-wgan-mines'...\n",
            "remote: Enumerating objects: 151, done.\u001b[K\n",
            "remote: Counting objects: 100% (151/151), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 151 (delta 30), reused 76 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (151/151), 162.71 MiB | 13.87 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n",
            "Updating files: 100% (126/126), done.\n",
            "/content/flumy-wgan-mines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3E8qnnCGH5K",
        "outputId": "073bc815-37ff-4f83-8985-dfa4bf5250a1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2VQcRg8KD0Yn"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from data.load_data import load_data\n",
        "from utils.visualisation import get_color_map\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "1eF1rmySGCzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful constants\n",
        "image_size = (64, 128)\n",
        "cmap, norm = get_color_map(number_of_categories=4)\n",
        "facies_names = np.array([\"Sand, Channel lag\", \"Sand, Point bar\", \"Silts, Levee\", \"Shale, Overbank\"])\n",
        "x = load_data(image_size[0], image_size[1], \"./data/horizontal/dataFlumyHoriz.csv\")\n",
        "x_train = x[:2760]\n",
        "x_test = x[2760:]"
      ],
      "metadata": {
        "id": "4yPyDmhUGCTa"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7-ElzPrD0Yq"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FBoSc7iCD0Ys"
      },
      "outputs": [],
      "source": [
        "# sampling\n",
        "min_signal_rate = 0.02\n",
        "max_signal_rate = 0.95\n",
        "\n",
        "# architecture\n",
        "widths = [32, 64, 128, 256]\n",
        "block_depth = 2\n",
        "\n",
        "# Data values embedding\n",
        "img_embed_size = 64\n",
        "categories_nb = 4\n",
        "\n",
        "# optimization\n",
        "batch_size = 30\n",
        "ema = 0.999\n",
        "learning_rate = 1e-4\n",
        "embeding_net_lr = 1e-3\n",
        "weight_decay = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diffusion Schedules"
      ],
      "metadata": {
        "id": "acL9rhHAdCCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "class DiffusionSchedule(ABC):\n",
        "    def __init__(self, start_log_snr, end_log_snr):\n",
        "        assert (\n",
        "            start_log_snr > end_log_snr\n",
        "        ), \"The starting SNR has to be higher than the final SNR.\"\n",
        "\n",
        "        self.end_beta = 20\n",
        "        self.start_beta = 0.1\n",
        "\n",
        "        self.start_snr = tf.exp(start_log_snr)\n",
        "        self.end_snr = tf.exp(end_log_snr)\n",
        "\n",
        "        #self.start_noise_power = 1.0 / (1.0 + self.start_snr)\n",
        "        #self.end_noise_power = 1.0 / (1.0 + self.end_snr)\n",
        "\n",
        "    def __call__(self, diffusion_times):\n",
        "        signal_powers = self.get_noise_powers(diffusion_times)\n",
        "        signal_rates = tf.math.exp(signal_powers)\n",
        "\n",
        "        noise_rates = (1 - signal_rates**2)**0.5\n",
        "\n",
        "\n",
        "        # the signal and noise power will always sum to one\n",
        "        #signal_powers = 1.0 - noise_powers\n",
        "\n",
        "        # the rates are the square roots of the powers\n",
        "        # variance**0.5 -> standard deviation\n",
        "        #signal_rates = signal_powers**0.5\n",
        "        #noise_rates = noise_powers**0.5\n",
        "\n",
        "\n",
        "        return noise_rates, signal_rates\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_noise_powers(self, diffusion_times):\n",
        "        pass\n",
        "\n",
        "\n",
        "class SignalStepLinearSchedule(DiffusionSchedule):\n",
        "    # the ratio between next-step and current signal powers decreases approximately linearly to 1\n",
        "    # similar to the \"linear schedule\" of DDPM https://arxiv.org/abs/2006.11239\n",
        "    def get_noise_powers(self, diffusion_times):\n",
        "\n",
        "        return -(self.end_beta - self.start_beta) / 4 * diffusion_times**2 - self.start_beta / 2 * diffusion_times\n",
        "\n",
        "        #return 1.0 - (1.0 - self.start_noise_power) * (\n",
        "        #    (1.0 - self.end_noise_power) / (1.0 - self.start_noise_power)\n",
        "        #) ** (diffusion_times**2)"
      ],
      "metadata": {
        "id": "Z5U9ClBmdDxI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "QxyZaS_UJ_YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianFourierProjection(tf.keras.layers.Layer):\n",
        "    \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
        "    def __init__(self, embed_dim, scale=30.):\n",
        "        super().__init__()\n",
        "        # Randomly sample weights during initialization. These weights are fixed\n",
        "        # during optimization and are not trainable.\n",
        "        self.W = self.add_weight(shape=(embed_dim // 2,),\n",
        "                                 trainable=False,\n",
        "                                 initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.), name=\"GFP\") * tf.constant(scale, dtype=tf.float32)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        x_proj = x * self.W * tf.constant(2., dtype=tf.float32) * tf.constant(np.pi, dtype=tf.float32)\n",
        "        y = tf.concat([tf.math.sin(x_proj), tf.cos(x_proj)], axis=-1)\n",
        "        return y # Probleme vient pas de lÃ  :()\n",
        "\n",
        "class CustomLinear(tf.keras.layers.Layer):\n",
        "    \"\"\"Rhaaah.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.W = tf.random.uniform((input_dim, output_dim), minval=-tf.math.sqrt(1/input_dim), maxval=tf.math.sqrt(1/input_dim))\n",
        "        self.b = tf.random.uniform((1, output_dim, ), minval=-tf.math.sqrt(1/input_dim), maxval=tf.math.sqrt(1/input_dim))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        y = tf.tensordot(x, self.W, 1) + self.b\n",
        "        y = tf.keras.activations.gelu(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "class EmbedLayerNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, img_embed_size):\n",
        "        super().__init__()\n",
        "        self.beta = self.add_weight(shape=(1, 1, 1, 1),\n",
        "                                    initializer=tf.keras.initializers.Zeros(),\n",
        "                                    dtype=tf.float32,\n",
        "                                    name=\"beta_embed_layer\",\n",
        "                                    trainable=True)\n",
        "        self.gamma = self.add_weight(shape=(1, 1, 1, 1),\n",
        "                                     initializer=tf.keras.initializers.Ones(),\n",
        "                                     dtype=tf.float32,\n",
        "                                     name=\"gamma_embed_layer\",\n",
        "                                     trainable=True)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "\n",
        "        mean, variance = tf.nn.moments(inputs, -1, keepdims=True)\n",
        "        outputs = tf.nn.batch_normalization(\n",
        "            inputs,\n",
        "            mean,\n",
        "            variance,\n",
        "            offset=self.beta,\n",
        "            scale=self.gamma,\n",
        "            variance_epsilon=1e-8,\n",
        "        )\n",
        "\n",
        "        return outputs #* self.gamma + self.beta\n",
        "\n",
        "@tf.function\n",
        "def embedding_normalization(logits):\n",
        "    # normalement vont avoir taille (batch_size, 64, 128, embedding_size)\n",
        "    # axis=-1 is embedding normalement\n",
        "    return (logits / tf.norm(logits, axis=-1, keepdims=True)) * tf.constant(np.sqrt(logits.shape[-1]), dtype=tf.float32)\n",
        "\n",
        "class NormalizedEmbedding(tf.keras.Model):\n",
        "    \"\"\"\"\"\"\n",
        "    def __init__(self, categories_nb, img_embed_size):\n",
        "        super().__init__()\n",
        "        self.embed_layer = tf.keras.layers.Embedding(categories_nb, img_embed_size)\n",
        "        self.embed_layer2 = layers.Conv2D(img_embed_size, kernel_size=1, padding=\"same\", activation=keras.activations.swish)\n",
        "        self.embed_layer3 = layers.Conv2D(img_embed_size, kernel_size=1, padding=\"same\", activation=keras.activations.swish)\n",
        "        self.embed_layer4 = layers.Conv2D(img_embed_size, kernel_size=1, activation=None)\n",
        "        self.layer_norm = EmbedLayerNormalization(img_embed_size=16)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        # I tested the embedding it's perfectly pixel by pixel\n",
        "        y = self.embed_layer(x)\n",
        "        y = self.embed_layer2(y)\n",
        "        y = self.embed_layer3(y)\n",
        "        y = self.embed_layer4(y)\n",
        "        #y = embedding_normalization(y)\n",
        "        y = self.layer_norm(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "Ej4nARwoGmme"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network(\n",
        "    image_size,\n",
        "    noise_embedding_max_frequency,\n",
        "    noise_embedding_dims,\n",
        "    image_embedding_dims,\n",
        "    block_depth,\n",
        "    widths,\n",
        "    attentions,\n",
        "    patch_size,\n",
        "    embed_size\n",
        "):\n",
        "    def EmbeddingLayer(embedding_max_frequency, embedding_dims):\n",
        "        def sinusoidal_embedding(x):\n",
        "            embedding_min_frequency = 1.0\n",
        "            frequencies = tf.exp(\n",
        "                tf.linspace(\n",
        "                    tf.math.log(embedding_min_frequency),\n",
        "                    tf.math.log(embedding_max_frequency),\n",
        "                    embedding_dims // 2,\n",
        "                )\n",
        "            )\n",
        "            angular_speeds = 2.0 * math.pi * frequencies\n",
        "            embeddings = tf.concat(\n",
        "                [\n",
        "                    tf.sin(angular_speeds * x),\n",
        "                    tf.cos(angular_speeds * x),\n",
        "                ],\n",
        "                axis=-1,\n",
        "            )\n",
        "            return embeddings\n",
        "\n",
        "        def forward(x):\n",
        "            x = layers.Lambda(sinusoidal_embedding)(x)\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def ResidualBlock(width, attention):\n",
        "        def forward(x):\n",
        "            x, n = x\n",
        "            input_width = x.shape[3]\n",
        "            if input_width == width:\n",
        "                residual = x\n",
        "            else:\n",
        "                residual = layers.Conv2D(width, kernel_size=1)(x)\n",
        "\n",
        "            n = layers.Dense(width)(n)\n",
        "\n",
        "            x = tfa.layers.GroupNormalization(groups=8)(x)\n",
        "            x = keras.activations.swish(x)\n",
        "            x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "\n",
        "            x = layers.Add()([x, n])\n",
        "\n",
        "            x = tfa.layers.GroupNormalization(groups=8)(x)\n",
        "            x = keras.activations.swish(x)\n",
        "            x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "\n",
        "            x = layers.Add()([residual, x])\n",
        "\n",
        "            if attention:\n",
        "                residual = x\n",
        "                x = tfa.layers.GroupNormalization(groups=8, center=False, scale=False)(\n",
        "                    x\n",
        "                )\n",
        "                x = layers.MultiHeadAttention(\n",
        "                    num_heads=4, key_dim=width, attention_axes=(1, 2)\n",
        "                )(x, x)\n",
        "\n",
        "                x = layers.Add()([residual, x])\n",
        "\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def DownBlock(block_depth, width, attention):\n",
        "        def forward(x):\n",
        "            x, n, skips = x\n",
        "            for _ in range(block_depth):\n",
        "                x = ResidualBlock(width, attention)([x, n])\n",
        "                skips.append(x)\n",
        "            x = layers.AveragePooling2D(pool_size=2)(x)\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def UpBlock(block_depth, width, attention):\n",
        "        def forward(x):\n",
        "            x, n, skips = x\n",
        "            x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
        "            for _ in range(block_depth):\n",
        "                x = layers.Concatenate()([x, skips.pop()])\n",
        "                x = ResidualBlock(width, attention)([x, n])\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    images = keras.Input(shape=(None, None, embed_size))\n",
        "    noise_powers = keras.Input(shape=(1, 1, 1))\n",
        "    mask = keras.Input(shape=(None, None, 1))\n",
        "    conditioning_pixels = keras.Input(shape=(None, None, embed_size))\n",
        "\n",
        "    x = tf.keras.layers.Concatenate(axis=-1)([images, mask, conditioning_pixels])\n",
        "\n",
        "    x = layers.Conv2D(image_embedding_dims, kernel_size=patch_size, strides=patch_size)(\n",
        "        x\n",
        "    )\n",
        "\n",
        "    # NOISE EMBEDDING\n",
        "    #print(noise_powers)\n",
        "    n = EmbeddingLayer(noise_embedding_max_frequency, noise_embedding_dims)(\n",
        "        noise_powers\n",
        "    )\n",
        "    n = layers.Dense(noise_embedding_dims, activation=keras.activations.swish)(n)\n",
        "    n = layers.Dense(noise_embedding_dims, activation=keras.activations.swish)(n)\n",
        "\n",
        "    skips = []\n",
        "    for width, attention in zip(widths[:-1], attentions[:-1]):\n",
        "        x = DownBlock(block_depth, width, attention)([x, n, skips])\n",
        "\n",
        "    for _ in range(block_depth):\n",
        "        x = ResidualBlock(widths[-1], attentions[-1])([x, n])\n",
        "\n",
        "    for width, attention in zip(widths[-2::-1], attentions[-2::-1]):\n",
        "        x = UpBlock(block_depth, width, attention)([x, n, skips])\n",
        "\n",
        "    x = layers.Conv2DTranspose(\n",
        "        4, kernel_size=patch_size, strides=patch_size, kernel_initializer=\"zeros\", activation=\"softmax\"\n",
        "    )(x)\n",
        "\n",
        "    return keras.Model([images, noise_powers, mask, conditioning_pixels], x, name=\"residual_unet\")"
      ],
      "metadata": {
        "id": "IgeH9voVvsN5"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sampling\n",
        "def make_mask(image_to_condition):\n",
        "    nb_conditioning_points = np.random.randint(low=1.0, high=image_to_condition.shape[0] * image_to_condition.shape[1])\n",
        "    random_x_coordinates = np.random.choice(image_to_condition.shape[0], nb_conditioning_points)\n",
        "    random_y_coordinates = np.random.choice(image_to_condition.shape[1], nb_conditioning_points)\n",
        "    mask = np.zeros((image_to_condition.shape[0], image_to_condition.shape[1], 1))\n",
        "    mask[random_x_coordinates, random_y_coordinates, :] = 1\n",
        "    return mask"
      ],
      "metadata": {
        "id": "XEMZlA-xdLcT"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "lZ37576YD0Y5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DiffusionModel(keras.Model):\n",
        "    def __init__(self, image_size, widths, block_depth, img_embed_size,\n",
        "                 categories_nb, embedding_lr=1e-3, batch_size=30,\n",
        "                 large_model=False):\n",
        "        super().__init__()\n",
        "\n",
        "        noise_embedding_max_frequency = 1000.0\n",
        "        noise_embedding_dims = 64\n",
        "        image_embedding_dims = 64\n",
        "        block_depth = 2\n",
        "\n",
        "        if large_model:\n",
        "            widths = [64, 128, 256, 512]\n",
        "            attentions = [False, False, True, True]\n",
        "        else:\n",
        "            widths = [64, 96, 128, 256]\n",
        "            attentions = [False, False, False, False]\n",
        "\n",
        "        patch_size = 1\n",
        "\n",
        "        self.diffusion_schedule = SignalStepLinearSchedule(start_log_snr=3.0, end_log_snr=-10.0,)\n",
        "        #self.diffusion_schedule = CosineSchedule(start_log_snr=3.0, end_log_snr=-10.0,)\n",
        "        self.network = get_network(image_size, noise_embedding_max_frequency,\n",
        "                                   noise_embedding_dims, image_embedding_dims,\n",
        "                                   block_depth, widths, attentions, patch_size,\n",
        "                                   img_embed_size)\n",
        "\n",
        "        self.ema_network = keras.models.clone_model(self.network)\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Embedding\n",
        "        self.img_embed_size = img_embed_size\n",
        "        self.embedding_layer = NormalizedEmbedding(categories_nb, img_embed_size)\n",
        "        self.emb_optimiser = tf.keras.optimizers.legacy.Adam(learning_rate=embedding_lr)\n",
        "\n",
        "    def compile(self, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n",
        "        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.image_loss_tracker, self.noise_loss_tracker]\n",
        "\n",
        "    def denoise(self, noisy_images, noise_rates, signal_rates, training, mask=None, pixels=None):\n",
        "        # the exponential moving average weights are used at evaluation\n",
        "\n",
        "        if mask is None or pixels is None:\n",
        "            mask = tf.zeros(noisy_images.shape)\n",
        "            pixels = tf.zeros(noisy_images.shape)\n",
        "\n",
        "        if training:\n",
        "            network = self.network\n",
        "        else:\n",
        "            network = self.ema_network\n",
        "\n",
        "        # predict noise component and calculate the image component using it\n",
        "        pred_images = network([noisy_images, noise_rates**2, mask, pixels], training=training)\n",
        "\n",
        "\n",
        "        int_encoded_img = tf.argmax(pred_images, axis=-1)\n",
        "        embed_pred_images = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "        pred_noises = (noisy_images - signal_rates * embed_pred_images) / noise_rates\n",
        "\n",
        "        return pred_images, pred_noises\n",
        "\n",
        "    def train_step(self, images):\n",
        "        # normalize images to have standard deviation of 1, like the noises\n",
        "        noises = tf.random.normal(shape=(self.batch_size, self.image_size[0], self.image_size[1], self.img_embed_size))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = tf.random.uniform(\n",
        "            shape=(self.batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        mask_uncondi = tf.zeros((self.batch_size // 2, images.shape[1], images.shape[2], 1))\n",
        "        mask_condi = tf.map_fn(make_mask, images[self.batch_size // 2:])\n",
        "        mask = tf.concat([mask_uncondi, mask_condi], axis=0)\n",
        "\n",
        "\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        # mix the images with noises accordingly\n",
        "        int_encoded_img = tf.argmax(images, axis=-1)\n",
        "\n",
        "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "            embed_images = self.embedding_layer(int_encoded_img)\n",
        "            pixels = tf.math.multiply(embed_images, mask)\n",
        "\n",
        "            noisy_images = signal_rates * embed_images + noise_rates * noises\n",
        "            noisy_images = tf.math.multiply(noisy_images, tf.math.abs(mask - 1))\n",
        "\n",
        "            # train the network to separate noisy images to their components\n",
        "            pred_images, pred_noise = self.denoise(\n",
        "                noisy_images, noise_rates, signal_rates, training=True, mask=mask, pixels=pixels\n",
        "            )\n",
        "\n",
        "            image_loss = self.loss(images, pred_images)  # training loss\n",
        "\n",
        "        n_loss = keras.losses.mean_absolute_error(tf.math.multiply(noises, tf.math.abs(mask - 1)), tf.math.multiply(pred_noise, tf.math.abs(mask - 1)))\n",
        "        gradients_model = tape1.gradient(image_loss , self.network.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients_model, self.network.trainable_weights))\n",
        "\n",
        "        gradients_embeddings = tape2.gradient(image_loss, self.embedding_layer.trainable_weights)\n",
        "        self.emb_optimiser.apply_gradients(zip(gradients_embeddings, self.embedding_layer.trainable_weights))\n",
        "\n",
        "        self.noise_loss_tracker.update_state(n_loss)\n",
        "        self.image_loss_tracker.update_state(image_loss)\n",
        "\n",
        "        # track the exponential moving averages of weights\n",
        "        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
        "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
        "\n",
        "        # KID is not measured during the training phase for computational efficiency\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, images):\n",
        "        noises = tf.random.normal(shape=(self.batch_size, self.image_size[0], self.image_size[1], self.img_embed_size))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = tf.random.uniform(\n",
        "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "\n",
        "        mask_uncondi = tf.zeros((self.batch_size // 2, images.shape[1], images.shape[2], 1))\n",
        "        mask_condi = tf.map_fn(make_mask, images[self.batch_size // 2:])\n",
        "        mask = tf.concat([mask_uncondi, mask_condi], axis=0)\n",
        "\n",
        "        int_encoded_img = tf.argmax(images, axis=-1)\n",
        "\n",
        "        embed_images = self.embedding_layer(int_encoded_img)\n",
        "\n",
        "        #std = marginal_prob_std(diffusion_times, sigma=sigma)\n",
        "        pixels = tf.math.multiply(embed_images, mask)\n",
        "\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        noisy_images = signal_rates * embed_images + noise_rates * noises\n",
        "        noisy_images = tf.math.multiply(noisy_images, tf.math.abs(mask - 1))\n",
        "        #noisy_images = embed_images + noises * tf.reshape(std, (-1, 1, 1, 1))\n",
        "\n",
        "        # use the network to separate noisy images to their components\n",
        "        pred_images, pred_noise = self.denoise(\n",
        "            noisy_images, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels\n",
        "        )\n",
        "\n",
        "        image_loss = self.loss(images, pred_images)\n",
        "        n_loss = keras.losses.mean_absolute_error(tf.math.multiply(noises, tf.math.abs(mask - 1)), tf.math.multiply(pred_noise, tf.math.abs(mask - 1)))\n",
        "        self.noise_loss_tracker.update_state(n_loss)\n",
        "        self.image_loss_tracker.update_state(image_loss)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqYrG2VPD0Y7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "LHcHZit9D0Y8"
      },
      "outputs": [],
      "source": [
        "# create and compile the model\n",
        "model = DiffusionModel(image_size, widths, block_depth, img_embed_size=img_embed_size, categories_nb=categories_nb, large_model=True)\n",
        "\n",
        "learning_rate = 1e-4\n",
        "t_epochs_nb=50\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    loss= tf.keras.losses.CategoricalCrossentropy(),\n",
        ")\n",
        "\n",
        "# run training and plot generated images periodically"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.load_trained_models import load_msnwgen_2d_gs_horizontal\n",
        "import numpy as np\n",
        "\n",
        "LOAD_MODEL = True\n",
        "\n",
        "if LOAD_MODEL:\n",
        "    !tar -xvf ddim_model_weights_horizontal_good.tar.gz\n",
        "    checkpoint_file=\"ddim_model_weights_horizontal/cp-ddim_model_horizontal.ckpt\"\n",
        "    model.load_weights(checkpoint_file)\n"
      ],
      "metadata": {
        "id": "iJXcsURSXb3H",
        "outputId": "29084839-6ac2-4b32-8e3e-a6fdb200590c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./ddim_model_weights_horizontal/\n",
            "./ddim_model_weights_horizontal/cp-ddim_model_horizontal.ckpt.index\n",
            "./ddim_model_weights_horizontal/checkpoint\n",
            "./ddim_model_weights_horizontal/cp-ddim_model_horizontal.ckpt.data-00000-of-00001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, batch_size=batch_size, epochs=t_epochs_nb, validation_data=(x_test,))"
      ],
      "metadata": {
        "id": "RDSktS3wIy4C",
        "outputId": "7148926f-8396-4efd-ff84-90b38e62fa1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0646 - n_loss: 0.0029 - val_i_loss: 0.0422 - val_n_loss: 0.0020\n",
            "Epoch 2/50\n",
            "92/92 [==============================] - 117s 1s/step - i_loss: 0.0680 - n_loss: 0.0031 - val_i_loss: 0.0546 - val_n_loss: 0.0025\n",
            "Epoch 3/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0656 - n_loss: 0.0029 - val_i_loss: 0.0489 - val_n_loss: 0.0021\n",
            "Epoch 4/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0603 - n_loss: 0.0028 - val_i_loss: 0.0459 - val_n_loss: 0.0023\n",
            "Epoch 5/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0639 - n_loss: 0.0029 - val_i_loss: 0.0527 - val_n_loss: 0.0021\n",
            "Epoch 6/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0594 - n_loss: 0.0027 - val_i_loss: 0.0433 - val_n_loss: 0.0021\n",
            "Epoch 7/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0645 - n_loss: 0.0029 - val_i_loss: 0.0435 - val_n_loss: 0.0023\n",
            "Epoch 8/50\n",
            "92/92 [==============================] - 117s 1s/step - i_loss: 0.0582 - n_loss: 0.0028 - val_i_loss: 0.0568 - val_n_loss: 0.0024\n",
            "Epoch 9/50\n",
            "92/92 [==============================] - 117s 1s/step - i_loss: 0.0614 - n_loss: 0.0029 - val_i_loss: 0.0450 - val_n_loss: 0.0024\n",
            "Epoch 10/50\n",
            "34/92 [==========>...................] - ETA: 1:10 - i_loss: 0.0579 - n_loss: 0.0026"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-fea734d692f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_epochs_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "SAVE_AND_TAR_RESULTS_WEIGHTS = True\n",
        "\n",
        "if SAVE_AND_TAR_RESULTS_WEIGHTS:\n",
        "  ddim_model_checkpoint_path = \"ddim_model_weights_horizontal/cp-ddim_model_horizontal.ckpt\"\n",
        "  ddim_model_checkpoint_dir = os.path.dirname(ddim_model_checkpoint_path)\n",
        "\n",
        "  model.save_weights(ddim_model_checkpoint_path)\n",
        "\n",
        "  !tar -czvf ddim_model_weights_horizontal_good.tar.gz ./ddim_model_weights_horizontal"
      ],
      "metadata": {
        "id": "iprlb_tQmY6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.network.summary()"
      ],
      "metadata": {
        "id": "Jcm7Vh6v9R0s",
        "outputId": "ea823842-cec6-4b62-933e-d6e5e82b6015",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"residual_unet\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, None, None,  0           []                               \n",
            "                                 64)]                                                             \n",
            "                                                                                                  \n",
            " input_15 (InputLayer)          [(None, None, None,  0           []                               \n",
            "                                 1)]                                                              \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)          [(None, None, None,  0           []                               \n",
            "                                 64)]                                                             \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, None, None,   0           ['input_13[0][0]',               \n",
            "                                129)                              'input_15[0][0]',               \n",
            "                                                                  'input_16[0][0]']               \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 1, 1, 1)]    0           []                               \n",
            "                                                                                                  \n",
            " conv2d_123 (Conv2D)            (None, None, None,   8320        ['concatenate_21[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 64)     0           ['input_14[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_102 (Group  (None, None, None,   128        ['conv2d_123[0][0]']             \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " dense_48 (Dense)               (None, 1, 1, 64)     4160        ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " tf.nn.silu_84 (TFOpLambda)     (None, None, None,   0           ['group_normalization_102[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " dense_49 (Dense)               (None, 1, 1, 64)     4160        ['dense_48[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_124 (Conv2D)            (None, None, None,   36928       ['tf.nn.silu_84[0][0]']          \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " dense_50 (Dense)               (None, 1, 1, 64)     4160        ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_102 (Add)                  (None, None, None,   0           ['conv2d_124[0][0]',             \n",
            "                                64)                               'dense_50[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_103 (Group  (None, None, None,   128        ['add_102[0][0]']                \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " tf.nn.silu_85 (TFOpLambda)     (None, None, None,   0           ['group_normalization_103[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_125 (Conv2D)            (None, None, None,   36928       ['tf.nn.silu_85[0][0]']          \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " add_103 (Add)                  (None, None, None,   0           ['conv2d_123[0][0]',             \n",
            "                                64)                               'conv2d_125[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_104 (Group  (None, None, None,   128        ['add_103[0][0]']                \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " tf.nn.silu_86 (TFOpLambda)     (None, None, None,   0           ['group_normalization_104[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_126 (Conv2D)            (None, None, None,   36928       ['tf.nn.silu_86[0][0]']          \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " dense_51 (Dense)               (None, 1, 1, 64)     4160        ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_104 (Add)                  (None, None, None,   0           ['conv2d_126[0][0]',             \n",
            "                                64)                               'dense_51[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_105 (Group  (None, None, None,   128        ['add_104[0][0]']                \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " tf.nn.silu_87 (TFOpLambda)     (None, None, None,   0           ['group_normalization_105[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_127 (Conv2D)            (None, None, None,   36928       ['tf.nn.silu_87[0][0]']          \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " add_105 (Add)                  (None, None, None,   0           ['add_103[0][0]',                \n",
            "                                64)                               'conv2d_127[0][0]']             \n",
            "                                                                                                  \n",
            " average_pooling2d_9 (AveragePo  (None, None, None,   0          ['add_105[0][0]']                \n",
            " oling2D)                       64)                                                               \n",
            "                                                                                                  \n",
            " group_normalization_106 (Group  (None, None, None,   128        ['average_pooling2d_9[0][0]']    \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " tf.nn.silu_88 (TFOpLambda)     (None, None, None,   0           ['group_normalization_106[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_129 (Conv2D)            (None, None, None,   73856       ['tf.nn.silu_88[0][0]']          \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " dense_52 (Dense)               (None, 1, 1, 128)    8320        ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_106 (Add)                  (None, None, None,   0           ['conv2d_129[0][0]',             \n",
            "                                128)                              'dense_52[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_107 (Group  (None, None, None,   256        ['add_106[0][0]']                \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_89 (TFOpLambda)     (None, None, None,   0           ['group_normalization_107[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_128 (Conv2D)            (None, None, None,   8320        ['average_pooling2d_9[0][0]']    \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_130 (Conv2D)            (None, None, None,   147584      ['tf.nn.silu_89[0][0]']          \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " add_107 (Add)                  (None, None, None,   0           ['conv2d_128[0][0]',             \n",
            "                                128)                              'conv2d_130[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_108 (Group  (None, None, None,   256        ['add_107[0][0]']                \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_90 (TFOpLambda)     (None, None, None,   0           ['group_normalization_108[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_131 (Conv2D)            (None, None, None,   147584      ['tf.nn.silu_90[0][0]']          \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " dense_53 (Dense)               (None, 1, 1, 128)    8320        ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_108 (Add)                  (None, None, None,   0           ['conv2d_131[0][0]',             \n",
            "                                128)                              'dense_53[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_109 (Group  (None, None, None,   256        ['add_108[0][0]']                \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_91 (TFOpLambda)     (None, None, None,   0           ['group_normalization_109[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_132 (Conv2D)            (None, None, None,   147584      ['tf.nn.silu_91[0][0]']          \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " add_109 (Add)                  (None, None, None,   0           ['add_107[0][0]',                \n",
            "                                128)                              'conv2d_132[0][0]']             \n",
            "                                                                                                  \n",
            " average_pooling2d_10 (AverageP  (None, None, None,   0          ['add_109[0][0]']                \n",
            " ooling2D)                      128)                                                              \n",
            "                                                                                                  \n",
            " group_normalization_110 (Group  (None, None, None,   256        ['average_pooling2d_10[0][0]']   \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_92 (TFOpLambda)     (None, None, None,   0           ['group_normalization_110[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_134 (Conv2D)            (None, None, None,   295168      ['tf.nn.silu_92[0][0]']          \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " dense_54 (Dense)               (None, 1, 1, 256)    16640       ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_110 (Add)                  (None, None, None,   0           ['conv2d_134[0][0]',             \n",
            "                                256)                              'dense_54[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_111 (Group  (None, None, None,   512        ['add_110[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_93 (TFOpLambda)     (None, None, None,   0           ['group_normalization_111[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_133 (Conv2D)            (None, None, None,   33024       ['average_pooling2d_10[0][0]']   \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_135 (Conv2D)            (None, None, None,   590080      ['tf.nn.silu_93[0][0]']          \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " add_111 (Add)                  (None, None, None,   0           ['conv2d_133[0][0]',             \n",
            "                                256)                              'conv2d_135[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_112 (Group  (None, None, None,   0          ['add_111[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention_18 (Multi  (None, None, None,   1051904    ['group_normalization_112[0][0]',\n",
            " HeadAttention)                 256)                              'group_normalization_112[0][0]']\n",
            "                                                                                                  \n",
            " add_112 (Add)                  (None, None, None,   0           ['add_111[0][0]',                \n",
            "                                256)                              'multi_head_attention_18[0][0]']\n",
            "                                                                                                  \n",
            " group_normalization_113 (Group  (None, None, None,   512        ['add_112[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_94 (TFOpLambda)     (None, None, None,   0           ['group_normalization_113[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_136 (Conv2D)            (None, None, None,   590080      ['tf.nn.silu_94[0][0]']          \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " dense_55 (Dense)               (None, 1, 1, 256)    16640       ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_113 (Add)                  (None, None, None,   0           ['conv2d_136[0][0]',             \n",
            "                                256)                              'dense_55[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_114 (Group  (None, None, None,   512        ['add_113[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_95 (TFOpLambda)     (None, None, None,   0           ['group_normalization_114[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_137 (Conv2D)            (None, None, None,   590080      ['tf.nn.silu_95[0][0]']          \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " add_114 (Add)                  (None, None, None,   0           ['add_112[0][0]',                \n",
            "                                256)                              'conv2d_137[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_115 (Group  (None, None, None,   0          ['add_114[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention_19 (Multi  (None, None, None,   1051904    ['group_normalization_115[0][0]',\n",
            " HeadAttention)                 256)                              'group_normalization_115[0][0]']\n",
            "                                                                                                  \n",
            " add_115 (Add)                  (None, None, None,   0           ['add_114[0][0]',                \n",
            "                                256)                              'multi_head_attention_19[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_11 (AverageP  (None, None, None,   0          ['add_115[0][0]']                \n",
            " ooling2D)                      256)                                                              \n",
            "                                                                                                  \n",
            " group_normalization_116 (Group  (None, None, None,   512        ['average_pooling2d_11[0][0]']   \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_96 (TFOpLambda)     (None, None, None,   0           ['group_normalization_116[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_139 (Conv2D)            (None, None, None,   1180160     ['tf.nn.silu_96[0][0]']          \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " dense_56 (Dense)               (None, 1, 1, 512)    33280       ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_116 (Add)                  (None, None, None,   0           ['conv2d_139[0][0]',             \n",
            "                                512)                              'dense_56[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_117 (Group  (None, None, None,   1024       ['add_116[0][0]']                \n",
            " Normalization)                 512)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_97 (TFOpLambda)     (None, None, None,   0           ['group_normalization_117[0][0]']\n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " conv2d_138 (Conv2D)            (None, None, None,   131584      ['average_pooling2d_11[0][0]']   \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " conv2d_140 (Conv2D)            (None, None, None,   2359808     ['tf.nn.silu_97[0][0]']          \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " add_117 (Add)                  (None, None, None,   0           ['conv2d_138[0][0]',             \n",
            "                                512)                              'conv2d_140[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_118 (Group  (None, None, None,   0          ['add_117[0][0]']                \n",
            " Normalization)                 512)                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention_20 (Multi  (None, None, None,   4200960    ['group_normalization_118[0][0]',\n",
            " HeadAttention)                 512)                              'group_normalization_118[0][0]']\n",
            "                                                                                                  \n",
            " add_118 (Add)                  (None, None, None,   0           ['add_117[0][0]',                \n",
            "                                512)                              'multi_head_attention_20[0][0]']\n",
            "                                                                                                  \n",
            " group_normalization_119 (Group  (None, None, None,   1024       ['add_118[0][0]']                \n",
            " Normalization)                 512)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_98 (TFOpLambda)     (None, None, None,   0           ['group_normalization_119[0][0]']\n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " conv2d_141 (Conv2D)            (None, None, None,   2359808     ['tf.nn.silu_98[0][0]']          \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " dense_57 (Dense)               (None, 1, 1, 512)    33280       ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_119 (Add)                  (None, None, None,   0           ['conv2d_141[0][0]',             \n",
            "                                512)                              'dense_57[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_120 (Group  (None, None, None,   1024       ['add_119[0][0]']                \n",
            " Normalization)                 512)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_99 (TFOpLambda)     (None, None, None,   0           ['group_normalization_120[0][0]']\n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " conv2d_142 (Conv2D)            (None, None, None,   2359808     ['tf.nn.silu_99[0][0]']          \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " add_120 (Add)                  (None, None, None,   0           ['add_118[0][0]',                \n",
            "                                512)                              'conv2d_142[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_121 (Group  (None, None, None,   0          ['add_120[0][0]']                \n",
            " Normalization)                 512)                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention_21 (Multi  (None, None, None,   4200960    ['group_normalization_121[0][0]',\n",
            " HeadAttention)                 512)                              'group_normalization_121[0][0]']\n",
            "                                                                                                  \n",
            " add_121 (Add)                  (None, None, None,   0           ['add_120[0][0]',                \n",
            "                                512)                              'multi_head_attention_21[0][0]']\n",
            "                                                                                                  \n",
            " up_sampling2d_9 (UpSampling2D)  (None, None, None,   0          ['add_121[0][0]']                \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, None, None,   0           ['up_sampling2d_9[0][0]',        \n",
            "                                768)                              'add_115[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization_122 (Group  (None, None, None,   1536       ['concatenate_22[0][0]']         \n",
            " Normalization)                 768)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_100 (TFOpLambda)    (None, None, None,   0           ['group_normalization_122[0][0]']\n",
            "                                768)                                                              \n",
            "                                                                                                  \n",
            " conv2d_144 (Conv2D)            (None, None, None,   1769728     ['tf.nn.silu_100[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " dense_58 (Dense)               (None, 1, 1, 256)    16640       ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_122 (Add)                  (None, None, None,   0           ['conv2d_144[0][0]',             \n",
            "                                256)                              'dense_58[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_123 (Group  (None, None, None,   512        ['add_122[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_101 (TFOpLambda)    (None, None, None,   0           ['group_normalization_123[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_143 (Conv2D)            (None, None, None,   196864      ['concatenate_22[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_145 (Conv2D)            (None, None, None,   590080      ['tf.nn.silu_101[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " add_123 (Add)                  (None, None, None,   0           ['conv2d_143[0][0]',             \n",
            "                                256)                              'conv2d_145[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_124 (Group  (None, None, None,   0          ['add_123[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention_22 (Multi  (None, None, None,   1051904    ['group_normalization_124[0][0]',\n",
            " HeadAttention)                 256)                              'group_normalization_124[0][0]']\n",
            "                                                                                                  \n",
            " add_124 (Add)                  (None, None, None,   0           ['add_123[0][0]',                \n",
            "                                256)                              'multi_head_attention_22[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_23 (Concatenate)   (None, None, None,   0           ['add_124[0][0]',                \n",
            "                                512)                              'add_112[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization_125 (Group  (None, None, None,   1024       ['concatenate_23[0][0]']         \n",
            " Normalization)                 512)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_102 (TFOpLambda)    (None, None, None,   0           ['group_normalization_125[0][0]']\n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " conv2d_147 (Conv2D)            (None, None, None,   1179904     ['tf.nn.silu_102[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " dense_59 (Dense)               (None, 1, 1, 256)    16640       ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_125 (Add)                  (None, None, None,   0           ['conv2d_147[0][0]',             \n",
            "                                256)                              'dense_59[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_126 (Group  (None, None, None,   512        ['add_125[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_103 (TFOpLambda)    (None, None, None,   0           ['group_normalization_126[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_146 (Conv2D)            (None, None, None,   131328      ['concatenate_23[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_148 (Conv2D)            (None, None, None,   590080      ['tf.nn.silu_103[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " add_126 (Add)                  (None, None, None,   0           ['conv2d_146[0][0]',             \n",
            "                                256)                              'conv2d_148[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_127 (Group  (None, None, None,   0          ['add_126[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention_23 (Multi  (None, None, None,   1051904    ['group_normalization_127[0][0]',\n",
            " HeadAttention)                 256)                              'group_normalization_127[0][0]']\n",
            "                                                                                                  \n",
            " add_127 (Add)                  (None, None, None,   0           ['add_126[0][0]',                \n",
            "                                256)                              'multi_head_attention_23[0][0]']\n",
            "                                                                                                  \n",
            " up_sampling2d_10 (UpSampling2D  (None, None, None,   0          ['add_127[0][0]']                \n",
            " )                              256)                                                              \n",
            "                                                                                                  \n",
            " concatenate_24 (Concatenate)   (None, None, None,   0           ['up_sampling2d_10[0][0]',       \n",
            "                                384)                              'add_109[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization_128 (Group  (None, None, None,   768        ['concatenate_24[0][0]']         \n",
            " Normalization)                 384)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_104 (TFOpLambda)    (None, None, None,   0           ['group_normalization_128[0][0]']\n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_150 (Conv2D)            (None, None, None,   442496      ['tf.nn.silu_104[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " dense_60 (Dense)               (None, 1, 1, 128)    8320        ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_128 (Add)                  (None, None, None,   0           ['conv2d_150[0][0]',             \n",
            "                                128)                              'dense_60[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_129 (Group  (None, None, None,   256        ['add_128[0][0]']                \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_105 (TFOpLambda)    (None, None, None,   0           ['group_normalization_129[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_149 (Conv2D)            (None, None, None,   49280       ['concatenate_24[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_151 (Conv2D)            (None, None, None,   147584      ['tf.nn.silu_105[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " add_129 (Add)                  (None, None, None,   0           ['conv2d_149[0][0]',             \n",
            "                                128)                              'conv2d_151[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_25 (Concatenate)   (None, None, None,   0           ['add_129[0][0]',                \n",
            "                                256)                              'add_107[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization_130 (Group  (None, None, None,   512        ['concatenate_25[0][0]']         \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_106 (TFOpLambda)    (None, None, None,   0           ['group_normalization_130[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_153 (Conv2D)            (None, None, None,   295040      ['tf.nn.silu_106[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " dense_61 (Dense)               (None, 1, 1, 128)    8320        ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_130 (Add)                  (None, None, None,   0           ['conv2d_153[0][0]',             \n",
            "                                128)                              'dense_61[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_131 (Group  (None, None, None,   256        ['add_130[0][0]']                \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_107 (TFOpLambda)    (None, None, None,   0           ['group_normalization_131[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_152 (Conv2D)            (None, None, None,   32896       ['concatenate_25[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_154 (Conv2D)            (None, None, None,   147584      ['tf.nn.silu_107[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " add_131 (Add)                  (None, None, None,   0           ['conv2d_152[0][0]',             \n",
            "                                128)                              'conv2d_154[0][0]']             \n",
            "                                                                                                  \n",
            " up_sampling2d_11 (UpSampling2D  (None, None, None,   0          ['add_131[0][0]']                \n",
            " )                              128)                                                              \n",
            "                                                                                                  \n",
            " concatenate_26 (Concatenate)   (None, None, None,   0           ['up_sampling2d_11[0][0]',       \n",
            "                                192)                              'add_105[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization_132 (Group  (None, None, None,   384        ['concatenate_26[0][0]']         \n",
            " Normalization)                 192)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_108 (TFOpLambda)    (None, None, None,   0           ['group_normalization_132[0][0]']\n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_156 (Conv2D)            (None, None, None,   110656      ['tf.nn.silu_108[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " dense_62 (Dense)               (None, 1, 1, 64)     4160        ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_132 (Add)                  (None, None, None,   0           ['conv2d_156[0][0]',             \n",
            "                                64)                               'dense_62[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_133 (Group  (None, None, None,   128        ['add_132[0][0]']                \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " tf.nn.silu_109 (TFOpLambda)    (None, None, None,   0           ['group_normalization_133[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_155 (Conv2D)            (None, None, None,   12352       ['concatenate_26[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_157 (Conv2D)            (None, None, None,   36928       ['tf.nn.silu_109[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " add_133 (Add)                  (None, None, None,   0           ['conv2d_155[0][0]',             \n",
            "                                64)                               'conv2d_157[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_27 (Concatenate)   (None, None, None,   0           ['add_133[0][0]',                \n",
            "                                128)                              'add_103[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization_134 (Group  (None, None, None,   256        ['concatenate_27[0][0]']         \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_110 (TFOpLambda)    (None, None, None,   0           ['group_normalization_134[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_159 (Conv2D)            (None, None, None,   73792       ['tf.nn.silu_110[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " dense_63 (Dense)               (None, 1, 1, 64)     4160        ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " add_134 (Add)                  (None, None, None,   0           ['conv2d_159[0][0]',             \n",
            "                                64)                               'dense_63[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_135 (Group  (None, None, None,   128        ['add_134[0][0]']                \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " tf.nn.silu_111 (TFOpLambda)    (None, None, None,   0           ['group_normalization_135[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_158 (Conv2D)            (None, None, None,   8256        ['concatenate_27[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_160 (Conv2D)            (None, None, None,   36928       ['tf.nn.silu_111[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " add_135 (Add)                  (None, None, None,   0           ['conv2d_158[0][0]',             \n",
            "                                64)                               'conv2d_160[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2DTran  (None, None, None,   260        ['add_135[0][0]']                \n",
            " spose)                         4)                                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,836,548\n",
            "Trainable params: 29,836,548\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_axis = np.arange(t_epochs_nb)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x_axis, history.history[\"i_loss\"], label=\"Training image CE loss\")\n",
        "plt.plot(x_axis, history.history[\"val_i_loss\"], label=\"Testing image CE loss\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efA4YUlX723d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fky6ewS3D0Y-"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm"
      ],
      "metadata": {
        "id": "PoJyZzMZqAIZ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def second_order_correction(\n",
        "    model,\n",
        "    diffusion_times,\n",
        "    step_size,\n",
        "    noisy_images,\n",
        "    signal_rates,\n",
        "    noise_rates,\n",
        "    pred_images,\n",
        "    pred_noises,\n",
        "    second_order_alpha,\n",
        "    mask,\n",
        "    pixels,\n",
        "):\n",
        "    # generic second-order Runge-Kutta method\n",
        "    # https://en.wikipedia.org/wiki/List_of_Runge%E2%80%93Kutta_methods#Generic_second-order_method\n",
        "    # based on https://arxiv.org/abs/2206.00364\n",
        "    #batch_size=noisy_images.shape[0]\n",
        "    #mask = tf.zeros((batch_size, image_size[0], image_size[1], 1))\n",
        "    #pixels = tf.zeros((batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "\n",
        "    # use first estimate to sample alpha steps away)\n",
        "    alpha_signal_rates, alpha_noise_rates = model.diffusion_schedule(\n",
        "        diffusion_times - second_order_alpha * step_size\n",
        "    )\n",
        "    alpha_noisy_images = (\n",
        "        alpha_signal_rates * pred_images + alpha_noise_rates * pred_noises\n",
        "    )\n",
        "    x_input = tf.math.multiply(alpha_noisy_images, tf.math.abs(mask - 1))\n",
        "    pred_x0, alpha_pred_noises = model.denoise(alpha_noisy_images, alpha_noise_rates, alpha_signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "    int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "    embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "    # linearly combine the two noise estimates\n",
        "    pred_noises = (1.0 - 1.0 / (2.0 * second_order_alpha)) * pred_noises + 1.0 / (\n",
        "        2.0 * second_order_alpha\n",
        "        ) * alpha_pred_noises\n",
        "\n",
        "    pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
        "    return pred_images, pred_noises"
      ],
      "metadata": {
        "id": "SEJciKNQzCGs"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_beta(curr_alphas, prev_alphas):\n",
        "\n",
        "    betas = 1 - (prev_alphas / curr_alphas)\n",
        "    return betas"
      ],
      "metadata": {
        "id": "wVWp820FKVRV"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import xla\n",
        "def ddpm_sampler(model, img_embed_size, image_size, batch_size=10, num_steps=500, eps=1e-3, mask=None, pixels=None, fixed_noise=None):\n",
        "    second_order_alpha = 1.1\n",
        "    # T and schedule\n",
        "    t_max = 1.0\n",
        "    t = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32)\n",
        "    noise_rates, signal_rates = model.diffusion_schedule(t)\n",
        "\n",
        "    if mask is None:\n",
        "        mask = tf.zeros((batch_size, image_size[0], image_size[1], 1), dtype=tf.float32)\n",
        "        pixels = tf.zeros((batch_size, image_size[0], image_size[1], img_embed_size), dtype=tf.float32)\n",
        "    else:\n",
        "\n",
        "        pixels = tf.argmax(pixels, axis=-1)\n",
        "        pixels = model.embedding_layer(pixels)\n",
        "        pixels = tf.math.multiply(pixels, mask)\n",
        "\n",
        "        mask = tf.repeat(mask, batch_size, axis=0)\n",
        "        mask = tf.cast(mask,  dtype=tf.float32)\n",
        "        pixels = tf.repeat(pixels, batch_size, axis=0)\n",
        "\n",
        "    # Sample noise\n",
        "    if fixed_noise is None:\n",
        "        noises = tf.random.normal(shape=(batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "    else:\n",
        "        noises = fixed_noise\n",
        "    init_x = noises\n",
        "\n",
        "    # Keep track of the chain\n",
        "    samples_list = []\n",
        "    #samples_list.append( tf.keras.backend.constant(keras.utils.to_categorical(uniform_init_x)))\n",
        "\n",
        "    # Steps and other algorithmic variables\n",
        "    time_steps = tf.linspace(1., eps, num_steps)\n",
        "    step_size = time_steps[0] - time_steps[1]\n",
        "    x = init_x\n",
        "    prev_alphas = signal_rates**2\n",
        "\n",
        "    # INFERENCE REVERSE LOOP\n",
        "    for time_step in tqdm.tqdm(time_steps):\n",
        "        #print(time_step)\n",
        "        batch_time_step = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32) * time_step\n",
        "        #print(batch_time_step)\n",
        "        noise_rates, signal_rates = model.diffusion_schedule(batch_time_step)\n",
        "        #print(noise_rates, signal_rates)\n",
        "        cur_alphas = signal_rates**2\n",
        "        betas = compute_beta(cur_alphas, prev_alphas)\n",
        "\n",
        "        # PREDICT IMAGE\n",
        "        x_input = tf.math.multiply(x, tf.math.abs(mask - 1))\n",
        "        pred_x0, pred_noise = model.denoise(x_input, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "        int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "        embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "        # optional second order sampling\n",
        "        if second_order_alpha is not None:\n",
        "            embed_pred_x0, pred_noises = second_order_correction(\n",
        "                model,\n",
        "                batch_time_step,\n",
        "                step_size,\n",
        "                x,\n",
        "                signal_rates,\n",
        "                noise_rates,\n",
        "                embed_pred_x0,\n",
        "                pred_noise,\n",
        "                second_order_alpha,\n",
        "                mask,\n",
        "                pixels,)\n",
        "\n",
        "        mean_x0 = tf.math.sqrt(cur_alphas) * betas / (1 - prev_alphas) * embed_pred_x0\n",
        "        mean_x = tf.math.sqrt(1 - betas) * (1 - cur_alphas) / (1 - prev_alphas) * x\n",
        "        x = mean_x + mean_x0 + tf.reshape(tf.math.sqrt(betas), (-1, 1, 1, 1)) * tf.random.normal(x.shape)\n",
        "\n",
        "        samples_list.append(pred_x0)\n",
        "        prev_alphas = cur_alphas\n",
        "\n",
        "    return pred_x0, samples_list"
      ],
      "metadata": {
        "id": "Krk0-qfIKNk1"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_snr(signal_rate, noise_rate):\n",
        "    return tf.math.log(signal_rate / noise_rate)\n",
        "\n",
        "def get_t_from_snr(snr):\n",
        "    max_beta = 20.\n",
        "    min_beta = 0.1\n",
        "    return 2 * tf.math.log(tf.math.exp(-2*snr)+1) / (tf.math.sqrt(min_beta**2 + 2\n",
        "                                                                 * (max_beta - min_beta)\n",
        "                                                                 * tf.math.log(tf.math.exp(-2*snr)+1)) + min_beta)"
      ],
      "metadata": {
        "id": "WKvodkeuyJFb"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_snr_list(nb_steps, model, eps):\n",
        "\n",
        "    start_noise_rates, start_signal_rates = model.diffusion_schedule(np.array([eps, ]))\n",
        "    start_snr = compute_snr(start_signal_rates, start_noise_rates)\n",
        "    end_noise_rates, end_signal_rates = model.diffusion_schedule(np.array([1., ]))\n",
        "    end_snr = compute_snr(end_signal_rates, end_noise_rates)\n",
        "    list_i = np.arange(0, nb_steps, 1.0, dtype=int)\n",
        "    all_snr = tf.cast(list_i / nb_steps * (start_snr - end_snr) + end_snr, dtype=tf.float32)\n",
        "    all_t = tf.cast(get_t_from_snr(all_snr), dtype=tf.float32)\n",
        "    return all_snr, all_t"
      ],
      "metadata": {
        "id": "MYg3ZH4rpjqp"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import xla\n",
        "def ddpm_solver(model, img_embed_size, image_size, batch_size=100, num_steps=50, eps=1e-3, mask=None, pixels=None, fixed_noise=None):\n",
        "    # T and schedule\n",
        "    t = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32)\n",
        "    noise_rates, signal_rates = model.diffusion_schedule(t)\n",
        "\n",
        "    if mask is None:\n",
        "        mask = tf.zeros((batch_size, image_size[0], image_size[1], 1), dtype=tf.float32)\n",
        "        pixels = tf.zeros((batch_size, image_size[0], image_size[1], img_embed_size), dtype=tf.float32)\n",
        "    else:\n",
        "\n",
        "        pixels = tf.argmax(pixels, axis=-1)\n",
        "        pixels = model.embedding_layer(pixels)\n",
        "        pixels = tf.math.multiply(pixels, mask)\n",
        "\n",
        "        mask = tf.repeat(mask, batch_size, axis=0)\n",
        "        mask = tf.cast(mask,  dtype=tf.float32)\n",
        "        pixels = tf.repeat(pixels, batch_size, axis=0)\n",
        "\n",
        "    # Sample noise\n",
        "    if fixed_noise is None:\n",
        "        noises = tf.random.normal(shape=(batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "    else:\n",
        "        noises = fixed_noise\n",
        "\n",
        "    # Pure white noise\n",
        "    init_x = noises\n",
        "\n",
        "    # Keep track of the Markov chain\n",
        "    samples_list = []\n",
        "\n",
        "    # Steps and other algorithmic variables\n",
        "    x = init_x\n",
        "\n",
        "    all_snr, time_steps = get_snr_list(num_steps, model, eps)\n",
        "    #step_size = time_steps[0] - time_steps[1]\n",
        "\n",
        "    prev_noise_rates, prev_signal_rates = noise_rates, signal_rates\n",
        "\n",
        "    # INFERENCE REVERSE LOOP\n",
        "    for i, time_step in enumerate(tqdm.tqdm(time_steps)):\n",
        "        prev_snr = all_snr[i]\n",
        "        batch_time_step = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32) * time_step\n",
        "        noise_rates, signal_rates = model.diffusion_schedule(batch_time_step)\n",
        "\n",
        "        # PREDICT IMAGE\n",
        "        x_input = tf.math.multiply(x, tf.math.abs(mask - 1))\n",
        "        _, pred_noises = model.denoise(x_input, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "\n",
        "        #snr = compute_snr(signal_rates, noise_rates)\n",
        "        if i < len(time_steps)-1:\n",
        "            snr = all_snr[i+1]\n",
        "\n",
        "            s_t = get_t_from_snr((snr + prev_snr) / 2)\n",
        "            s_t = tf.repeat(tf.reshape(s_t, (1, 1, 1, 1)), batch_size, axis=0)\n",
        "\n",
        "            s_noise_rates, s_signal_rates = model.diffusion_schedule(s_t)\n",
        "\n",
        "\n",
        "            u = s_signal_rates / prev_signal_rates * x - s_noise_rates * (tf.math.exp((snr - prev_snr) / 2) - 1) * pred_noises\n",
        "\n",
        "            # PREDICT IMAGE 2\n",
        "            x_input = tf.math.multiply(u, tf.math.abs(mask - 1))\n",
        "            pred_x0, pred_noise = model.denoise(x_input, s_noise_rates, s_signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "            int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "            embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "            pred_noises = (u - s_signal_rates * embed_pred_x0) / s_noise_rates\n",
        "            pred_noises = tf.math.multiply(pred_noises, tf.math.abs(mask - 1))\n",
        "\n",
        "            x = signal_rates / prev_signal_rates * x - noise_rates * (tf.math.exp((snr - prev_snr)) - 1) * pred_noises\n",
        "\n",
        "        samples_list.append(pred_x0)\n",
        "        #prev_alphas = cur_alphas\n",
        "        prev_snr = snr\n",
        "        prev_noise_rates, prev_signal_rates = noise_rates, signal_rates\n",
        "    pred_x0, pred_noise = model.denoise(x, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "    return pred_x0, samples_list"
      ],
      "metadata": {
        "id": "hcWvFddmeN_W"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import xla\n",
        "def ddpm_solver_3rd_order(model, img_embed_size, image_size, batch_size=100, num_steps=50, eps=1e-3, mask=None, pixels=None, fixed_noise=None):\n",
        "    # T and schedule\n",
        "    r1 = 1/3\n",
        "    r2 = 2/3\n",
        "\n",
        "    if mask is None:\n",
        "        mask = tf.zeros((batch_size, image_size[0], image_size[1], 1), dtype=tf.float32)\n",
        "        pixels = tf.zeros((batch_size, image_size[0], image_size[1], img_embed_size), dtype=tf.float32)\n",
        "    else:\n",
        "\n",
        "        pixels = tf.argmax(pixels, axis=-1)\n",
        "        pixels = model.embedding_layer(pixels)\n",
        "        pixels = tf.math.multiply(pixels, mask)\n",
        "\n",
        "        mask = tf.repeat(mask, batch_size, axis=0)\n",
        "        mask = tf.cast(mask,  dtype=tf.float32)\n",
        "        pixels = tf.repeat(pixels, batch_size, axis=0)\n",
        "\n",
        "    # Sample noise\n",
        "    if fixed_noise is None:\n",
        "        noises = tf.random.normal(shape=(batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "    else:\n",
        "        noises = fixed_noise\n",
        "\n",
        "    # Keep track of the Markov chain\n",
        "    samples_list = []\n",
        "\n",
        "    # Steps and other algorithmic variables\n",
        "    #time_steps = tf.linspace(1., eps, num_steps)\n",
        "    #step_size = time_steps[0] - time_steps[1]\n",
        "    x = noises\n",
        "    #prev_alphas = signal_rates**2\n",
        "    #prev_snr = compute_snr(signal_rates, noise_rates)\n",
        "    all_snr, time_steps = get_snr_list(num_steps, model, eps)\n",
        "\n",
        "    noise_rates, signal_rates = model.diffusion_schedule(time_steps[0])\n",
        "    prev_noise_rates, prev_signal_rates = noise_rates, signal_rates\n",
        "\n",
        "    # INFERENCE REVERSE LOOP\n",
        "    for i, time_step in enumerate(tqdm.tqdm(time_steps)):\n",
        "\n",
        "        prev_snr = all_snr[i]\n",
        "        batch_time_step = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32) * time_step\n",
        "        noise_rates, signal_rates = model.diffusion_schedule(batch_time_step)\n",
        "\n",
        "        # PREDICT IMAGE (x_input is to take into account observations)\n",
        "        x_input = tf.math.multiply(x, tf.math.abs(mask - 1))\n",
        "        pred_x0, pred_noises = model.denoise(x_input, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "        int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "        embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "        #snr = compute_snr(signal_rates, noise_rates)\n",
        "        if i < len(time_steps)-1:\n",
        "            snr = all_snr[i+1]\n",
        "            hi = snr - prev_snr\n",
        "\n",
        "            prev_s_t = get_t_from_snr(prev_snr + r1 * hi)\n",
        "            prev_s_t = tf.repeat(tf.reshape(prev_s_t, (1, 1, 1, 1)), batch_size, axis=0)\n",
        "\n",
        "            prev_s_noise_rates, prev_s_signal_rates = model.diffusion_schedule(prev_s_t)\n",
        "\n",
        "            prev_u = prev_s_signal_rates / prev_signal_rates * x - prev_s_noise_rates * (tf.math.exp(r1 * hi) - 1) * pred_noises\n",
        "            #input_prev_u = tf.math.multiply(prev_u, tf.math.abs(mask - 1))\n",
        "            _, u_pred_noise = model.denoise(prev_u, prev_s_noise_rates, prev_s_signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "\n",
        "            prev_d = u_pred_noise - pred_noises\n",
        "\n",
        "            s_t = get_t_from_snr(prev_snr + r2 * hi)\n",
        "            s_t = tf.repeat(tf.reshape(s_t, (1, 1, 1, 1)), batch_size, axis=0)\n",
        "            s_noise_rates, s_signal_rates = model.diffusion_schedule(s_t)\n",
        "\n",
        "            u = s_signal_rates / prev_signal_rates * x - s_noise_rates * (tf.math.exp(r2 * hi) - 1) * pred_noises - s_noise_rates * r2 / r1 * ((tf.math.exp(r2 * hi) - 1) / r2*hi - 1) * prev_d\n",
        "            #input_u = tf.math.multiply(u, tf.math.abs(mask - 1))\n",
        "            pred_x0, u_pred_noise = model.denoise(u, s_noise_rates, s_signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "\n",
        "            d = u_pred_noise - pred_noises\n",
        "\n",
        "            x = signal_rates / prev_signal_rates * x - noise_rates * (tf.math.exp((snr - prev_snr)) - 1) * pred_noises - noise_rates / r2 * ((tf.math.exp(hi) - 1) / hi - 1) * d\n",
        "\n",
        "        samples_list.append(pred_x0)\n",
        "        #prev_alphas = cur_alphas\n",
        "        prev_snr = snr\n",
        "        prev_noise_rates, prev_signal_rates = noise_rates, signal_rates\n",
        "    pred_x0, pred_noise = model.denoise(x, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "    return pred_x0, samples_list"
      ],
      "metadata": {
        "id": "fZlMJ5cRFsqr"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_embed_size"
      ],
      "metadata": {
        "id": "Hh8ijmL5vvle",
        "outputId": "c8bc141d-692b-4548-ee26-b5d09ecbce65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_batch_size"
      ],
      "metadata": {
        "id": "tFdUH0yFvxJG",
        "outputId": "2d591fda-6b7b-4386-96d8-5af980975f09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# noises = tf.random.normal(shape=(sample_batch_size, 64, 128, img_embed_size))"
      ],
      "metadata": {
        "id": "6MU6X2C55X6S"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = ddpm_solver_3rd_order #@param ['ddpm_sampler', 'ddpm_solver', 'ddpm_solver_3rd_order'] {'type': 'raw'}\n",
        "\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                (64, 128),\n",
        "                                sample_batch_size,\n",
        "                                num_steps=60,\n",
        "                                mask=None,\n",
        "                                pixels=None,\n",
        "                                fixed_noise=noises)"
      ],
      "metadata": {
        "id": "T0uowLytML_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc79c140-bfd8-416e-dc29-87c3dfc5fe20"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|ââââââââââ| 60/60 [01:05<00:00,  1.09s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(sample_batch_size):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(np.argmax(samples[i].numpy(), axis=-1).reshape((64, 128)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "O4GzBJX-MUmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data.load_data import ConditionalDataGenerator\n",
        "\n",
        "# Load Data\n",
        "slice_size = (64, 128, 4)\n",
        "dataloader = ConditionalDataGenerator(x_test, 1, slice_size, wells=10, mode=3)\n",
        "pixels, mask, ground_truth = dataloader.__getitem__(0)\n",
        "print(pixels.shape, mask.shape, ground_truth.shape)"
      ],
      "metadata": {
        "id": "LAAAFTajBiUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9825b3-7c8a-4b4e-aee0-8d87f66bdf8d"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 64, 128, 4) (1, 64, 128, 1) (1, 64, 128, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_noisesraise Exception(\"DEBUG\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "fJZxsZYbtlnu",
        "outputId": "2e29bf32-a5c0-4aa1-cc3d-24e240e9fbd1"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-163-59578291e494>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pred_noisesraise Exception(\"DEBUG\")\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = ddpm_sampler #@param ['ddpm_sampler', 'pc_sampler'] {'type': 'raw'}\n",
        "\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                (64, 128),\n",
        "                                sample_batch_size,\n",
        "                                mask=mask,\n",
        "                                pixels=pixels)"
      ],
      "metadata": {
        "id": "iGnksfxx3JRB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(sample_batch_size):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(np.argmax(samples[i].numpy(), axis=-1).reshape((64, 128)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QjbvDyWoK7fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from utils.visualisation import *\n",
        "from data.load_data import get_3d_flumy_data, load_data, ConditionalDataGenerator\n",
        "from models.load_trained_models import load_msgen_horizontal, wgan_horizontal,\\\n",
        "    load_msnwgen_2d_gs_horizontal, load_wgan_gs_horizontal, load_mswgen_sn_3d_horizontal\n",
        "from utils.utils import generate_noise, correct_percentage\n"
      ],
      "metadata": {
        "id": "pP5_ix6TCLdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_simulations = 3\n",
        "cmap, norm = get_color_map(number_of_categories=4)\n",
        "\n",
        "print_conditioned_results(ground_truth, samples, mask, nb_simulations, cmap, norm)"
      ],
      "metadata": {
        "id": "0nla2sslCFTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.axis('off')\n",
        "\n",
        "plt.imshow(np.argmax(ground_truth, axis=-1).reshape((64, 128)),\n",
        "            interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "URj4sIq1BlYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = ddpm_sampler #@param ['ddpm_sampler', 'pc_sampler'] {'type': 'raw'}\n",
        "\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                (128, 256),\n",
        "                                sample_batch_size,)"
      ],
      "metadata": {
        "id": "gzMLizIXPUAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(sample_batch_size):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(np.argmax(samples[i].numpy(), axis=-1).reshape((128, 256)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "p88yuQgCPUA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GIF"
      ],
      "metadata": {
        "id": "8X9v42ujwnt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "images = []\n",
        "for i, batch in enumerate(samples_list):\n",
        "    figure = plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Timestep {0:.3f}\".format(1 - (i / 350)))\n",
        "    plt.imshow(np.argmax(batch[0].numpy(), axis=-1).reshape((64, 128)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.savefig('foo.png', bbox_inches='tight')\n",
        "    images.append(imageio.imread('foo.png'))\n",
        "    plt.show() #close(figure)\n",
        "imageio.mimsave('/movie.gif', images)"
      ],
      "metadata": {
        "id": "m9JzX8pVMf7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Models"
      ],
      "metadata": {
        "id": "GCPKYflD2fyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "SAVE_AND_TAR_RESULTS_WEIGHTS = True\n",
        "\n",
        "if SAVE_AND_TAR_RESULTS_WEIGHTS:\n",
        "  diffusion_checkpoint_path = \"diffusion_weights_horiz/cp-diffusion2d_net_horiz.ckpt\"\n",
        "  diffusion_checkpoint_dir = os.path.dirname(diffusion_checkpoint_path)\n",
        "\n",
        "  model.ema_network.save_weights(diffusion_checkpoint_path)\n",
        "\n",
        "  !tar -czvf diffusion_weights_horiz.tar.gz ./diffusion_weights_horiz\n",
        "\n",
        "  diffusion_checkpoint_path = \"diffusion_weights_horiz/cp-diffusion2d_embed_horiz.ckpt\"\n",
        "  diffusion_checkpoint_dir = os.path.dirname(diffusion_checkpoint_path)\n",
        "\n",
        "  model.embedding_layer.save_weights(diffusion_checkpoint_path)\n",
        "\n",
        "  !tar -czvf diffusion_weights_horiz.tar.gz ./diffusion_weights_horiz"
      ],
      "metadata": {
        "id": "gi9sVjP0QHdb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ddim",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}