{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pumafi/dl_spatial_gen_geol_facies/blob/main/stationary_ddim_cdcd_conditioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "TA0koXfT2e7k",
        "outputId": "ba28e8bc-3b25-4f31-b5d3-56d5c6d19cad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 22 13:13:16 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## IDEAS \n",
        "1.   Normalization in embedding ?\n",
        "2.   Formula when I replace beta by t\n",
        "3.   Use keras inference for potential changes\n",
        "\n",
        "## What to try next?\n",
        "\n",
        "If you would like to dive in deeper to the topic, a recommend checking out\n",
        "[this repository](https://github.com/beresandras/clear-diffusion-keras) that I created in\n",
        "preparation for this code example, which implements a wider range of features in a\n",
        "similar style, such as:\n",
        "\n",
        "* stochastic sampling\n",
        "* second-order sampling based on the\n",
        "[differential equation view of DDIMs (Equation 13)](https://arxiv.org/abs/2010.02502)\n",
        "* more diffusion schedules\n",
        "* more network output types: predicting image or\n",
        "[velocity (Appendix D)](https://arxiv.org/abs/2202.00512) instead of noise\n",
        "* more datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "rge41L-HIY-i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqkfNOJcD0Ym"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install deel.lip\n",
        "!python -m pip install -i https://test.pypi.org/simple/ gstlearn"
      ],
      "metadata": {
        "id": "u03Nq4eK2ems",
        "outputId": "dd397517-5fed-45ce-cde9-b330a985789e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deel.lip\n",
            "  Downloading deel_lip-1.4.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deel.lip) (1.22.4)\n",
            "Requirement already satisfied: tensorflow~=2.2 in /usr/local/lib/python3.10/dist-packages (from deel.lip) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.2->deel.lip) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.2->deel.lip) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.2->deel.lip) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (3.2.2)\n",
            "Installing collected packages: deel.lip\n",
            "Successfully installed deel.lip-1.4.0\n",
            "Looking in indexes: https://test.pypi.org/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gstlearn\n",
            "  Downloading https://test-files.pythonhosted.org/packages/4b/33/9b8e2546cfe286525409a1d17279b325a7a1d2968eba07357a0e30976d29/gstlearn-0.2.1-cp310-cp310-manylinux1_x86_64.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gstlearn) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gstlearn) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gstlearn) (1.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from gstlearn) (5.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gstlearn) (1.5.3)\n",
            "INFO: pip is looking at multiple versions of gstlearn to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://test-files.pythonhosted.org/packages/ea/39/5475c8fac8f0b9897ef6be34b92d40dae6d209dbfe2a7db0fb7a9386fadc/gstlearn-0.1.38-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://test-files.pythonhosted.org/packages/11/e9/eee76162b77c60d4c57aea94c230259605295eb3ad7e76b4b3c67773ff0f/gstlearn-0.1.37-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gstlearn\n",
            "Successfully installed gstlearn-0.1.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RUNNING_IN_COLAB = True\n",
        "\n",
        "if RUNNING_IN_COLAB:\n",
        "    # Uses a private Auth Token, giving read and write access to repo\n",
        "    # TO DELETE IF REPO GOES PUBLIC\n",
        "    REPO_URL = 'https://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@github.com/Pumafi/flumy-wgan-mines'\n",
        "    BRANCH   = 'main'\n",
        "    REPO_DIR = 'flumy-wgan-mines'\n",
        "\n",
        "    from pathlib import Path\n",
        "\n",
        "    %cd /content\n",
        "\n",
        "    if Path(REPO_DIR).is_dir():\n",
        "      !rm -rf {REPO_DIR}\n",
        "\n",
        "    # Download the repository\n",
        "    if not Path(REPO_DIR).is_dir():\n",
        "        !git clone --branch {BRANCH} --depth=1 -- {REPO_URL} {REPO_DIR}\n",
        "    \n",
        "    %cd {REPO_DIR}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqPH8VxsGGrb",
        "outputId": "e53d1c41-67f3-41f3-a5f4-2a68075234a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'flumy-wgan-mines'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 146 (delta 31), reused 74 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 140.19 MiB | 11.28 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n",
            "Updating files: 100% (121/121), done.\n",
            "/content/flumy-wgan-mines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3E8qnnCGH5K",
        "outputId": "4fcc466e-5567-4672-e998-74dcb4e34386"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2VQcRg8KD0Yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a91c67-a0a9-476b-b84a-298f7b5acd1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from data.load_data import load_data\n",
        "from utils.visualisation import get_color_map\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "1eF1rmySGCzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful constants\n",
        "image_size = (64, 128)\n",
        "cmap, norm = get_color_map(number_of_categories=4)\n",
        "facies_names = np.array([\"Sand, Channel lag\", \"Sand, Point bar\", \"Silts, Levee\", \"Shale, Overbank\"])\n",
        "x = load_data(image_size[0], image_size[1], \"./data/horizontal/dataFlumyHoriz.csv\")\n",
        "x_train = x[:2760]\n",
        "x_test = x[2760:]"
      ],
      "metadata": {
        "id": "4yPyDmhUGCTa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7-ElzPrD0Yq"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FBoSc7iCD0Ys"
      },
      "outputs": [],
      "source": [
        "# sampling\n",
        "min_signal_rate = 0.02\n",
        "max_signal_rate = 0.95\n",
        "\n",
        "# architecture\n",
        "widths = [32, 64, 128, 256]\n",
        "block_depth = 2\n",
        "\n",
        "# Data values embedding\n",
        "img_embed_size = 64\n",
        "categories_nb = 4\n",
        "\n",
        "# optimization\n",
        "batch_size = 30\n",
        "ema = 0.999\n",
        "learning_rate = 1e-4\n",
        "embeding_net_lr = 1e-3\n",
        "weight_decay = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diffusion Schedules"
      ],
      "metadata": {
        "id": "acL9rhHAdCCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "class DiffusionSchedule(ABC):\n",
        "    def __init__(self, start_log_snr, end_log_snr):\n",
        "        assert (\n",
        "            start_log_snr > end_log_snr\n",
        "        ), \"The starting SNR has to be higher than the final SNR.\"\n",
        "\n",
        "        self.start_snr = tf.exp(start_log_snr)\n",
        "        self.end_snr = tf.exp(end_log_snr)\n",
        "\n",
        "        self.start_noise_power = 1.0 / (1.0 + self.start_snr)\n",
        "        self.end_noise_power = 1.0 / (1.0 + self.end_snr)\n",
        "\n",
        "    def __call__(self, diffusion_times):\n",
        "        noise_powers = self.get_noise_powers(diffusion_times)\n",
        "\n",
        "        # the signal and noise power will always sum to one\n",
        "        signal_powers = 1.0 - noise_powers\n",
        "\n",
        "        # the rates are the square roots of the powers\n",
        "        # variance**0.5 -> standard deviation\n",
        "        signal_rates = signal_powers**0.5\n",
        "        noise_rates = noise_powers**0.5\n",
        "\n",
        "        return noise_rates, signal_rates\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_noise_powers(self, diffusion_times):\n",
        "        pass\n",
        "\n",
        "\n",
        "class SignalStepLinearSchedule(DiffusionSchedule):\n",
        "    # the ratio between next-step and current signal powers decreases approximately linearly to 1\n",
        "    # similar to the \"linear schedule\" of DDPM https://arxiv.org/abs/2006.11239\n",
        "    def get_noise_powers(self, diffusion_times):\n",
        "        return 1.0 - (1.0 - self.start_noise_power) * (\n",
        "            (1.0 - self.end_noise_power) / (1.0 - self.start_noise_power)\n",
        "        ) ** (diffusion_times**2)"
      ],
      "metadata": {
        "id": "Z5U9ClBmdDxI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "QxyZaS_UJ_YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianFourierProjection(tf.keras.layers.Layer):\n",
        "    \"\"\"Gaussian random features for encoding time steps.\"\"\"  \n",
        "    def __init__(self, embed_dim, scale=30.):\n",
        "        super().__init__()\n",
        "        # Randomly sample weights during initialization. These weights are fixed \n",
        "        # during optimization and are not trainable.\n",
        "        self.W = self.add_weight(shape=(embed_dim // 2,),\n",
        "                                 trainable=False,\n",
        "                                 initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.), name=\"GFP\") * tf.constant(scale, dtype=tf.float32)\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        x_proj = x * self.W * tf.constant(2., dtype=tf.float32) * tf.constant(np.pi, dtype=tf.float32)\n",
        "        y = tf.concat([tf.math.sin(x_proj), tf.cos(x_proj)], axis=-1)\n",
        "        return y # Probleme vient pas de là :()\n",
        "\n",
        "class CustomLinear(tf.keras.layers.Layer):\n",
        "    \"\"\"Rhaaah.\"\"\"  \n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.W = tf.random.uniform((input_dim, output_dim), minval=-tf.math.sqrt(1/input_dim), maxval=tf.math.sqrt(1/input_dim))\n",
        "        self.b = tf.random.uniform((1, output_dim, ), minval=-tf.math.sqrt(1/input_dim), maxval=tf.math.sqrt(1/input_dim))\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        y = tf.tensordot(x, self.W, 1) + self.b\n",
        "        y = tf.keras.activations.gelu(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "class EmbedLayerNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, img_embed_size):\n",
        "        super().__init__()\n",
        "        self.beta = self.add_weight(shape=(1, 1, 1, 1),\n",
        "                                    initializer=tf.keras.initializers.Zeros(),\n",
        "                                    dtype=tf.float32,\n",
        "                                    name=\"beta_embed_layer\",\n",
        "                                    trainable=True)\n",
        "        self.gamma = self.add_weight(shape=(1, 1, 1, 1),\n",
        "                                     initializer=tf.keras.initializers.Ones(),\n",
        "                                     dtype=tf.float32,\n",
        "                                     name=\"gamma_embed_layer\",\n",
        "                                     trainable=True)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "\n",
        "        mean, variance = tf.nn.moments(inputs, [1, 2], keepdims=True)\n",
        "        outputs = tf.nn.batch_normalization(\n",
        "            inputs,\n",
        "            mean,\n",
        "            variance,\n",
        "            offset=self.beta,\n",
        "            scale=self.gamma,\n",
        "            variance_epsilon=1e-8,\n",
        "        )\n",
        "\n",
        "        return outputs #* self.gamma + self.beta\n",
        "\n",
        "@tf.function\n",
        "def embedding_normalization(logits):\n",
        "    # normalement vont avoir taille (batch_size, 64, 128, embedding_size)\n",
        "    # axis=-1 is embedding normalement\n",
        "    return (logits / tf.norm(logits, axis=-1, keepdims=True)) * tf.constant(np.sqrt(logits.shape[-1]), dtype=tf.float32)\n",
        "\n",
        "class NormalizedEmbedding(tf.keras.Model):\n",
        "    \"\"\"\"\"\"  \n",
        "    def __init__(self, categories_nb, img_embed_size):\n",
        "        super().__init__()\n",
        "        self.embed_layer = tf.keras.layers.Embedding(categories_nb, img_embed_size)\n",
        "        self.embed_layer2 = layers.Conv2D(img_embed_size, kernel_size=1, padding=\"same\", activation=keras.activations.swish)\n",
        "        self.embed_layer3 = layers.Conv2D(img_embed_size, kernel_size=1, padding=\"same\", activation=keras.activations.swish)\n",
        "        self.embed_layer4 = layers.Conv2D(img_embed_size, kernel_size=1, activation=None)\n",
        "        self.layer_norm = EmbedLayerNormalization(img_embed_size=16)\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        y = self.embed_layer(x)\n",
        "        y = self.embed_layer2(y)\n",
        "        y = self.embed_layer3(y)\n",
        "        y = self.embed_layer4(y)\n",
        "        y = embedding_normalization(y)\n",
        "        y = self.layer_norm(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "Ej4nARwoGmme"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network(\n",
        "    image_size,\n",
        "    noise_embedding_max_frequency,\n",
        "    noise_embedding_dims,\n",
        "    image_embedding_dims,\n",
        "    block_depth,\n",
        "    widths,\n",
        "    attentions,\n",
        "    patch_size,\n",
        "    embed_size\n",
        "):\n",
        "    def EmbeddingLayer(embedding_max_frequency, embedding_dims):\n",
        "        def sinusoidal_embedding(x):\n",
        "            embedding_min_frequency = 1.0\n",
        "            frequencies = tf.exp(\n",
        "                tf.linspace(\n",
        "                    tf.math.log(embedding_min_frequency),\n",
        "                    tf.math.log(embedding_max_frequency),\n",
        "                    embedding_dims // 2,\n",
        "                )\n",
        "            )\n",
        "            angular_speeds = 2.0 * math.pi * frequencies\n",
        "            embeddings = tf.concat(\n",
        "                [\n",
        "                    tf.sin(angular_speeds * x),\n",
        "                    tf.cos(angular_speeds * x),\n",
        "                ],\n",
        "                axis=3,\n",
        "            )\n",
        "            return embeddings\n",
        "\n",
        "        def forward(x):\n",
        "            x = layers.Lambda(sinusoidal_embedding)(x)\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def ResidualBlock(width, attention):\n",
        "        def forward(x):\n",
        "            x, n = x\n",
        "            input_width = x.shape[3]\n",
        "            if input_width == width:\n",
        "                residual = x\n",
        "            else:\n",
        "                residual = layers.Conv2D(width, kernel_size=1)(x)\n",
        "\n",
        "            n = layers.Dense(width)(n)\n",
        "\n",
        "            x = tfa.layers.GroupNormalization(groups=8)(x)\n",
        "            x = keras.activations.swish(x)\n",
        "            x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "\n",
        "            x = layers.Add()([x, n])\n",
        "\n",
        "            x = tfa.layers.GroupNormalization(groups=8)(x)\n",
        "            x = keras.activations.swish(x)\n",
        "            x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "\n",
        "            x = layers.Add()([residual, x])\n",
        "\n",
        "            if attention:\n",
        "                residual = x\n",
        "                x = tfa.layers.GroupNormalization(groups=8, center=False, scale=False)(\n",
        "                    x\n",
        "                )\n",
        "                x = layers.MultiHeadAttention(\n",
        "                    num_heads=4, key_dim=width, attention_axes=(1, 2)\n",
        "                )(x, x)\n",
        "\n",
        "                x = layers.Add()([residual, x])\n",
        "\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def DownBlock(block_depth, width, attention):\n",
        "        def forward(x):\n",
        "            x, n, skips = x\n",
        "            for _ in range(block_depth):\n",
        "                x = ResidualBlock(width, attention)([x, n])\n",
        "                skips.append(x)\n",
        "            x = layers.AveragePooling2D(pool_size=2)(x)\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def UpBlock(block_depth, width, attention):\n",
        "        def forward(x):\n",
        "            x, n, skips = x\n",
        "            x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
        "            for _ in range(block_depth):\n",
        "                x = layers.Concatenate()([x, skips.pop()])\n",
        "                x = ResidualBlock(width, attention)([x, n])\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    images = keras.Input(shape=(None, None, embed_size))\n",
        "    noise_powers = keras.Input(shape=(1, 1, 1))\n",
        "    mask = keras.Input(shape=(None, None, 1))\n",
        "    conditioning_pixels = keras.Input(shape=(None, None, embed_size))\n",
        "\n",
        "    x = tf.keras.layers.Concatenate(axis=-1)([images, mask, conditioning_pixels])\n",
        "\n",
        "    x = layers.Conv2D(image_embedding_dims, kernel_size=patch_size, strides=patch_size)(\n",
        "        x\n",
        "    )\n",
        "\n",
        "    # NOISE EMBEDDING\n",
        "    n = EmbeddingLayer(noise_embedding_max_frequency, noise_embedding_dims)(\n",
        "        noise_powers\n",
        "    )\n",
        "    n = layers.Dense(noise_embedding_dims, activation=keras.activations.swish)(n)\n",
        "    n = layers.Dense(noise_embedding_dims, activation=keras.activations.swish)(n)\n",
        "\n",
        "    skips = []\n",
        "    for width, attention in zip(widths[:-1], attentions[:-1]):\n",
        "        x = DownBlock(block_depth, width, attention)([x, n, skips])\n",
        "\n",
        "    for _ in range(block_depth):\n",
        "        x = ResidualBlock(widths[-1], attentions[-1])([x, n])\n",
        "\n",
        "    for width, attention in zip(widths[-2::-1], attentions[-2::-1]):\n",
        "        x = UpBlock(block_depth, width, attention)([x, n, skips])\n",
        "\n",
        "    x = layers.Conv2DTranspose(\n",
        "        4, kernel_size=patch_size, strides=patch_size, kernel_initializer=\"zeros\", activation=\"softmax\"\n",
        "    )(x)\n",
        "\n",
        "    return keras.Model([images, noise_powers, mask, conditioning_pixels], x, name=\"residual_unet\")"
      ],
      "metadata": {
        "id": "IgeH9voVvsN5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sampling\n",
        "def make_mask(image_to_condition):\n",
        "    nb_conditioning_points = np.random.randint(low=1.0, high=image_to_condition.shape[0] * image_to_condition.shape[1])\n",
        "    random_x_coordinates = np.random.choice(image_to_condition.shape[0], nb_conditioning_points)\n",
        "    random_y_coordinates = np.random.choice(image_to_condition.shape[1], nb_conditioning_points)\n",
        "    mask = np.zeros((image_to_condition.shape[0], image_to_condition.shape[1], 1))\n",
        "    mask[random_x_coordinates, random_y_coordinates, :] = 1\n",
        "    return mask"
      ],
      "metadata": {
        "id": "XEMZlA-xdLcT"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "lZ37576YD0Y5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DiffusionModel(keras.Model):\n",
        "    def __init__(self, image_size, widths, block_depth, img_embed_size,\n",
        "                 categories_nb, embedding_lr=1e-3, batch_size=30,\n",
        "                 large_model=False):\n",
        "        super().__init__()\n",
        "\n",
        "        noise_embedding_max_frequency = 1000.0\n",
        "        noise_embedding_dims = 64\n",
        "        image_embedding_dims = 64\n",
        "        block_depth = 2\n",
        "\n",
        "        if large_model:\n",
        "            widths = [64, 128, 256, 512]\n",
        "            attentions = [False, False, True, True]\n",
        "        else:\n",
        "            widths = [64, 96, 128, 256]\n",
        "            attentions = [False, False, False, False]\n",
        "            \n",
        "        patch_size = 1\n",
        "\n",
        "        self.diffusion_schedule = SignalStepLinearSchedule(start_log_snr=3.0, end_log_snr=-10.0,)\n",
        "        #self.diffusion_schedule = CosineSchedule(start_log_snr=3.0, end_log_snr=-10.0,)\n",
        "        self.network = get_network(image_size, noise_embedding_max_frequency,\n",
        "                                   noise_embedding_dims, image_embedding_dims,\n",
        "                                   block_depth, widths, attentions, patch_size,\n",
        "                                   img_embed_size)\n",
        "        \n",
        "        self.ema_network = keras.models.clone_model(self.network)\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Embedding\n",
        "        self.img_embed_size = img_embed_size\n",
        "        self.embedding_layer = NormalizedEmbedding(categories_nb, img_embed_size)\n",
        "        self.emb_optimiser = tf.keras.optimizers.legacy.Adam(learning_rate=embedding_lr)\n",
        "\n",
        "    def compile(self, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.image_loss_tracker]\n",
        "\n",
        "    def denoise(self, noisy_images, noise_rates, signal_rates, training, mask=None, pixels=None):\n",
        "        # the exponential moving average weights are used at evaluation\n",
        "\n",
        "        if mask is None or pixels is None:\n",
        "            mask = tf.zeros(noisy_images.shape)\n",
        "            pixels = tf.zeros(noisy_images.shape)\n",
        "\n",
        "        if training:\n",
        "            network = self.network\n",
        "        else:\n",
        "            network = self.ema_network\n",
        "\n",
        "        # predict noise component and calculate the image component using it\n",
        "        pred_images = network([noisy_images, noise_rates**2, mask, pixels], training=training)\n",
        "\n",
        "        \n",
        "        int_encoded_img = tf.argmax(pred_images, axis=-1)\n",
        "        embed_pred_images = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "        pred_noises = (noisy_images - signal_rates * embed_pred_images) / noise_rates\n",
        "\n",
        "        return pred_images, pred_noises\n",
        "\n",
        "    def train_step(self, images):\n",
        "        # normalize images to have standard deviation of 1, like the noises\n",
        "        noises = tf.random.normal(shape=(self.batch_size, self.image_size[0], self.image_size[1], self.img_embed_size))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = tf.random.uniform(\n",
        "            shape=(self.batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        mask_uncondi = tf.zeros((self.batch_size // 2, images.shape[1], images.shape[2], 1))\n",
        "        mask_condi = tf.map_fn(make_mask, images[self.batch_size // 2:])\n",
        "        mask = tf.concat([mask_uncondi, mask_condi], axis=0)\n",
        "\n",
        "\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        # mix the images with noises accordingly\n",
        "        int_encoded_img = tf.argmax(images, axis=-1)\n",
        "\n",
        "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "            embed_images = self.embedding_layer(int_encoded_img)\n",
        "            pixels = tf.math.multiply(embed_images, mask)\n",
        "\n",
        "            noisy_images = signal_rates * embed_images + noise_rates * noises\n",
        "            noisy_images = tf.math.multiply(noisy_images, tf.math.abs(mask - 1))\n",
        "\n",
        "            # train the network to separate noisy images to their components\n",
        "            pred_images, pred_noise = self.denoise(\n",
        "                noisy_images, noise_rates, signal_rates, training=True, mask=mask, pixels=pixels\n",
        "            )\n",
        "\n",
        "            image_loss = self.loss(images, pred_images)  # training loss\n",
        "            \n",
        "\n",
        "        gradients_model = tape1.gradient(image_loss, self.network.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients_model, self.network.trainable_weights))\n",
        "\n",
        "        gradients_embeddings = tape2.gradient(image_loss, self.embedding_layer.trainable_weights)\n",
        "        self.emb_optimiser.apply_gradients(zip(gradients_embeddings, self.embedding_layer.trainable_weights))\n",
        "\n",
        "        self.image_loss_tracker.update_state(image_loss)\n",
        "\n",
        "        # track the exponential moving averages of weights\n",
        "        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
        "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
        "\n",
        "        # KID is not measured during the training phase for computational efficiency\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, images):\n",
        "        noises = tf.random.normal(shape=(self.batch_size, self.image_size[0], self.image_size[1], self.img_embed_size))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = tf.random.uniform(\n",
        "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "\n",
        "        mask_uncondi = tf.zeros((self.batch_size // 2, images.shape[1], images.shape[2], 1))\n",
        "        mask_condi = tf.map_fn(make_mask, images[self.batch_size // 2:])\n",
        "        mask = tf.concat([mask_uncondi, mask_condi], axis=0)\n",
        "\n",
        "        int_encoded_img = tf.argmax(images, axis=-1)\n",
        "\n",
        "        embed_images = self.embedding_layer(int_encoded_img)\n",
        "\n",
        "        #std = marginal_prob_std(diffusion_times, sigma=sigma)\n",
        "        pixels = tf.math.multiply(embed_images, mask)\n",
        "\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        noisy_images = signal_rates * embed_images + noise_rates * noises\n",
        "        noisy_images = tf.math.multiply(noisy_images, tf.math.abs(mask - 1))\n",
        "        #noisy_images = embed_images + noises * tf.reshape(std, (-1, 1, 1, 1))\n",
        "\n",
        "        # use the network to separate noisy images to their components\n",
        "        pred_images, pred_noise = self.denoise(\n",
        "            noisy_images, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels\n",
        "        )\n",
        "\n",
        "        image_loss = self.loss(images, pred_images)\n",
        "\n",
        "        self.image_loss_tracker.update_state(image_loss)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqYrG2VPD0Y7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "LHcHZit9D0Y8"
      },
      "outputs": [],
      "source": [
        "# create and compile the model\n",
        "model = DiffusionModel(image_size, widths, block_depth, img_embed_size=img_embed_size, categories_nb=categories_nb, large_model=True)\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    loss= tf.keras.losses.CategoricalCrossentropy(),\n",
        ")\n",
        "\n",
        "# run training and plot generated images periodically"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, batch_size=batch_size, epochs=50, validation_data=(x_test,))"
      ],
      "metadata": {
        "id": "RDSktS3wIy4C",
        "outputId": "5e6cc9f6-0dd0-43aa-e90c-32b9453cf755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "92/92 [==============================] - 174s 1s/step - i_loss: 0.5668 - val_i_loss: 1.3853\n",
            "Epoch 2/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.2084 - val_i_loss: 1.3844\n",
            "Epoch 3/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.1417 - val_i_loss: 1.3808\n",
            "Epoch 4/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.1255 - val_i_loss: 1.3720\n",
            "Epoch 5/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.1148 - val_i_loss: 1.3528\n",
            "Epoch 6/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.1152 - val_i_loss: 1.3199\n",
            "Epoch 7/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.1011 - val_i_loss: 1.2689\n",
            "Epoch 8/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0994 - val_i_loss: 1.1949\n",
            "Epoch 9/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0911 - val_i_loss: 1.1209\n",
            "Epoch 10/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0881 - val_i_loss: 1.0132\n",
            "Epoch 11/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0833 - val_i_loss: 0.9279\n",
            "Epoch 12/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0811 - val_i_loss: 0.8017\n",
            "Epoch 13/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0838 - val_i_loss: 0.6761\n",
            "Epoch 14/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0751 - val_i_loss: 0.6198\n",
            "Epoch 15/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0708 - val_i_loss: 0.5354\n",
            "Epoch 16/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0782 - val_i_loss: 0.4467\n",
            "Epoch 17/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0793 - val_i_loss: 0.3360\n",
            "Epoch 18/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0692 - val_i_loss: 0.3013\n",
            "Epoch 19/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0711 - val_i_loss: 0.2507\n",
            "Epoch 20/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0639 - val_i_loss: 0.2147\n",
            "Epoch 21/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0672 - val_i_loss: 0.2062\n",
            "Epoch 22/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0638 - val_i_loss: 0.1568\n",
            "Epoch 23/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0615 - val_i_loss: 0.1247\n",
            "Epoch 24/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0630 - val_i_loss: 0.1232\n",
            "Epoch 25/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0581 - val_i_loss: 0.1223\n",
            "Epoch 26/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0632 - val_i_loss: 0.1038\n",
            "Epoch 27/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0536 - val_i_loss: 0.0824\n",
            "Epoch 28/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0611 - val_i_loss: 0.0931\n",
            "Epoch 29/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.0568 - val_i_loss: 0.0778\n",
            "Epoch 30/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0572 - val_i_loss: 0.0754\n",
            "Epoch 31/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0557 - val_i_loss: 0.1005\n",
            "Epoch 32/50\n",
            "92/92 [==============================] - 113s 1s/step - i_loss: 0.0506 - val_i_loss: 0.0842\n",
            "Epoch 33/50\n",
            "86/92 [===========================>..] - ETA: 7s - i_loss: 0.0566"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-c1bca070aa2f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \"\"\"\n\u001b[1;32m   1159\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.network.summary()"
      ],
      "metadata": {
        "id": "Jcm7Vh6v9R0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_axis = np.arange(100)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x_axis, history.history[\"i_loss\"], label=\"Training image CE loss\")\n",
        "plt.plot(x_axis, history.history[\"val_i_loss\"], label=\"Testing image CE loss\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efA4YUlX723d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fky6ewS3D0Y-"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def second_order_correction(\n",
        "    model,\n",
        "    diffusion_times,\n",
        "    step_size,\n",
        "    noisy_images,\n",
        "    signal_rates,\n",
        "    noise_rates,\n",
        "    pred_images,\n",
        "    pred_noises,\n",
        "    second_order_alpha,\n",
        "    mask,\n",
        "    pixels,\n",
        "):\n",
        "    # generic second-order Runge-Kutta method\n",
        "    # https://en.wikipedia.org/wiki/List_of_Runge%E2%80%93Kutta_methods#Generic_second-order_method\n",
        "    # based on https://arxiv.org/abs/2206.00364\n",
        "    #batch_size=noisy_images.shape[0]\n",
        "    #mask = tf.zeros((batch_size, image_size[0], image_size[1], 1))\n",
        "    #pixels = tf.zeros((batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "\n",
        "    # use first estimate to sample alpha steps away\n",
        "    alpha_signal_rates, alpha_noise_rates = model.diffusion_schedule(\n",
        "        diffusion_times - second_order_alpha * step_size\n",
        "    )\n",
        "    alpha_noisy_images = (\n",
        "        alpha_signal_rates * pred_images + alpha_noise_rates * pred_noises\n",
        "    )\n",
        "    pred_x0, alpha_pred_noises = model.denoise(noisy_images, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "    int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "    embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "    # linearly combine the two noise estimates\n",
        "    pred_noises = (1.0 - 1.0 / (2.0 * second_order_alpha)) * pred_noises + 1.0 / (\n",
        "        2.0 * second_order_alpha\n",
        "        ) * alpha_pred_noises\n",
        "\n",
        "    pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
        "    return pred_images, pred_noises"
      ],
      "metadata": {
        "id": "SEJciKNQzCGs"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "def compute_beta(curr_alphas, prev_alphas):\n",
        "\n",
        "    betas = 1 - (prev_alphas / curr_alphas)\n",
        "    return betas"
      ],
      "metadata": {
        "id": "wVWp820FKVRV"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import xla\n",
        "def ddpm_sampler(model, img_embed_size, image_size, batch_size=10, num_steps=350, eps=1e-3, mask=None, pixels=None):\n",
        "    second_order_alpha = 1.1\n",
        "    # T and schedule\n",
        "    t = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32)\n",
        "    noise_rates, signal_rates = model.diffusion_schedule(t)\n",
        "    print(batch_size)\n",
        "\n",
        "    if mask is None:\n",
        "        mask = tf.zeros((batch_size, image_size[0], image_size[1], 1))\n",
        "        pixels = tf.zeros((batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "\n",
        "    # Sample noise\n",
        "    uniform_init_x = tf.random.uniform((batch_size, image_size[0], image_size[1]), 0, 4, dtype=tf.dtypes.int32)\n",
        "    noises = tf.random.normal(shape=(batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "    init_x = signal_rates * model.embedding_layer(uniform_init_x) + noise_rates * noises\n",
        "\n",
        "    # Keep track of the chain\n",
        "    samples_list = []\n",
        "    samples_list.append( tf.keras.backend.constant(keras.utils.to_categorical(uniform_init_x)))\n",
        "\n",
        "    # Steps and other algorithmic variables\n",
        "    time_steps = tf.linspace(1., eps, num_steps)\n",
        "    step_size = time_steps[0] - time_steps[1]\n",
        "    x = init_x\n",
        "    prev_alphas = signal_rates**2\n",
        "\n",
        "    # INFERENCE REVERSE LOOP\n",
        "    for time_step in tqdm.tqdm(time_steps):\n",
        "        batch_time_step = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32) * time_step\n",
        "        noise_rates, signal_rates = model.diffusion_schedule(batch_time_step)\n",
        "        cur_alphas = signal_rates**2\n",
        "        betas = compute_beta(cur_alphas, prev_alphas)\n",
        "\n",
        "        # PREDICT IMAGE\n",
        "        pred_x0, pred_noise = model.denoise(x, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "        int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "        embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "        # optional second order sampling\n",
        "        if second_order_alpha is not None:\n",
        "            embed_pred_x0, pred_noises = second_order_correction(\n",
        "                model,\n",
        "                batch_time_step,\n",
        "                step_size,\n",
        "                x,\n",
        "                signal_rates,\n",
        "                noise_rates,\n",
        "                embed_pred_x0,\n",
        "                pred_noise,\n",
        "                second_order_alpha,\n",
        "                mask,\n",
        "                pixels,)\n",
        "\n",
        "        mean_x0 = tf.math.sqrt(cur_alphas) * betas / (1 - prev_alphas) * embed_pred_x0\n",
        "        mean_x = tf.math.sqrt(1 - betas) * (1 - cur_alphas) / (1 - prev_alphas) * x\n",
        "        x = mean_x + mean_x0 + tf.reshape(tf.math.sqrt(betas), (-1, 1, 1, 1)) * tf.random.normal(x.shape)\n",
        "\n",
        "        samples_list.append(pred_x0)\n",
        "        prev_alphas = cur_alphas\n",
        "\n",
        "    return pred_x0, samples_list"
      ],
      "metadata": {
        "id": "Krk0-qfIKNk1"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "slice_size = (64, 128, 4)\n",
        "dataloader = ConditionalDataGenerator(x_train, 1, slice_size, wells=32, mode=3)\n",
        "pixels, mask, ground_truth = dataloader.__getitem__(0)\n",
        "print(pixels.shape, mask.shape, ground_truth.shape)"
      ],
      "metadata": {
        "id": "LAAAFTajBiUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = ddpm_sampler #@param ['ddpm_sampler', 'pc_sampler'] {'type': 'raw'}\n",
        "\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                (64, 128),\n",
        "                                sample_batch_size,\n",
        "                                mask=mask,\n",
        "                                pixels=pixels)"
      ],
      "metadata": {
        "id": "T0uowLytML_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae6b460-c2e8-45f6-97ca-7174baad0bda"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "(10, 64, 128, 1)\n",
            "(10, 64, 128, 64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 350/350 [03:02<00:00,  1.92it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(sample_batch_size):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(np.argmax(samples[i].numpy(), axis=-1).reshape((64, 128)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "O4GzBJX-MUmk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3c4d2df-6e41-4d7e-d589-e8d40eefa59d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAGVCAYAAABjBWf4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAARiElEQVR4nO3c7W3jSoIFUHvxgpg4xspiAWfReGE0OowHZ2Fgs5A6i4lD++Mt0L0Dl4alW6wP8pyfBE2VimRRF4Tv6/1+v78AAAAE/mv0AAAAgPUJFgAAQEywAAAAYoIFAAAQEywAAICYYAEAAMQECwAAICZYAAAAMcECAACI/bF1x+v1suc4aOxj0Od+q9x/1Dh7+Ofts2r/n2/vO42EnmrvAZ5z+Z/b6CF0df3vt9FDgKGO/HuhhdKzp3beSse5XK6b/t4bCwAAICZYAAAAMcECAACICRYAAEBMsAAAAGKbW6FKbpXNN2+Dmm9qG0Rma+BYpQ2hNM6/SvNfmOfSdbV3c1Jtk9MztD9xJHu3M5XW4to1ujTO0nFa7d/K6s+wIys99zTEPbbK75q9lX531P5eLv4e7/ybwxsLAAAgJlgAAAAxwQIAAIgJFgAAQEywAAAAYq/3+/2+ac8fr00+8M9CU8Xq7Qml/8av/a/+1VsSas/j6t+XforNYq2O/49/fbl9VJMdx7Z3i1StVu1YnNfqz/NWrZCzPTNa3cOXy3XTft5YAAAAMcECAACICRYAAEBMsAAAAGKCBQAAENvcCnW9XvYey9JWb0NopdQKVZqf2v35Ze8GrlFNbXu35WizoadmjSyVxxnVOuX+4t+VrsVSS2jJ6u2hq9MKBQAAdCNYAAAAMcECAACICRYAAEBMsAAAAGLLtkLdbp9fbv/59t55JJyBNop5jWq/qbV6W06rdiP6mO2+cJ3A2rRCAQAA3QgWAABATLAAAABiggUAABATLAAAgNiyrVAfowdAE6W2JQ0065mtheZs3Btz2nstG3X8o3Ifwde0QgEAAN0IFgAAQEywAAAAYoIFAAAQEywAAIDYsq1QJdqinlNqZ2ql1CzyZ2UDx97jpJ9WbTOj2nVqx197nLO108x2PZzN2dqfRnKNsiKtUAAAQDeCBQAAEBMsAACAmGABAADEBAsAACB2uFaoWqu3SI1qSdJkM97eLUa1tB4d2yqtQa6rOc22Xo2093ce1ZQ3yirjnE31PakVCgAA6EWwAAAAYoIFAAAQEywAAICYYAEAAMRO3wpVa+8WqVEtT8DaRrXo7N0GdtTvVXK7fVbt//Pt/cvtpWeJBp3HjtxGVbJKSxVjaYUCAAC6ESwAAICYYAEAAMQECwAAICZYAAAAMa1QlVq1Qml/6mNkA8qoz9b6cgyrt9Oscr3VzvOo7+W+Hmv1+/EIRj07a+3dslWr2Xi+b4oL3lgAAAA5wQIAAIgJFgAAQEywAAAAYoIFAAAQW7YVqvTf7H8OasgotTxp8nhslUaWZxz5u1HPWsAWrpM+ZpxnzVN91J5j5+X/aIUCAAB6ESwAAICYYAEAAMQECwAAICZYAAAAselboT6GfGq9UivU2czWtDHbeIBj0gLHLFq1GJWu0dlaklYZ5/K0QgEAAL0IFgAAQEywAAAAYoIFAAAQEywAAIDY9K1QtWpbpLQ5tTVbC9Pe7RjA19x7wCNam54z7PfU5bppP28sAACAmGABAADEBAsAACAmWAAAADHBAgAAiB2uFQr20LK94qgtN1qAxio14mm+e85sDXdwFqu0RZXWgtrxl44z25quFQoAAOhGsAAAAGKCBQAAEBMsAACAmGABAADEtELBJLTQAAAz0goFAAB0I1gAAAAxwQIAAIgJFgAAQEywAAAAYn+MHgDUOHJz0hG+AwBwXt5YAAAAMcECAACICRYAAEBMsAAAAGKCBQAAENMKxVI0JwEvL8duiGMtrkX4xRsLAAAgJlgAAAAxwQIAAIgJFgAAQEywAAAAYlqhgCWUmldKWjWytGp8+Shs/1Y5HmAu2p/aslauzRsLAAAgJlgAAAAxwQIAAIgJFgAAQEywAAAAYq/3+/2+Zcfr9bL3WGBajxqJNIK01aqFaRW32+eX23++vX+5XTMKAL1dLtdN+3ljAQAAxAQLAAAgJlgAAAAxwQIAAIgJFgAAQEwrFEyutiWpVavSoyasFo7a8gTwjFZrfYk195ya/SbQCgUAAPQiWAAAADHBAgAAiAkWAABATLAAAABiWqE4tFatGXs3JPWgWQQAeHl5ebndPr/c/vb2/uV2rVAAAEA3ggUAABATLAAAgJhgAQAAxAQLAAAg9sfoAbRWarjRZDPWKq1Kq4zzEdc6R2JN72O2eZ5tPEdmrs+p1P6U8sYCAACICRYAAEBMsAAAAGKCBQAAEBMsAACA2Ov9fr9v2fF6vew9FiZ0hJakFka2Y4w6BxpB6EkzDVvUXieuq/bM6TldLtdN+3ljAQAAxAQLAAAgJlgAAAAxwQIAAIgJFgAAQEwr1CJmawaarS1q7zaKlt9377F+VO7/rXL/ozaCHPV7ARxBqzW69nnuGfA3rVAAAEA3ggUAABATLAAAgJhgAQAAxAQLAAAgphVqMrO1Le1N28LzbrfP0UP4f97e3kcPAWAXj57NnmP0NKrBUCsUAADQjWABAADEBAsAACAmWAAAADHBAgAAiGmFGmT19ictGO3N1vJUov0JAM5FKxQAANCNYAEAAMQECwAAICZYAAAAMcECAACI/TF6AEexestTifanfkptS6Vrq9W52fv4AKPUPptnXPes0azEGwsAACAmWAAAADHBAgAAiAkWAABATLAAAABir/f7/b5lx+v1svdYprJ6y5O2CM5iVGOKphYAVlX7DLtcrpuO640FAAAQEywAAICYYAEAAMQECwAAICZYAAAAsdO3Qml/gnPRIgXMqNUaYa35m3l4TvF38fdNccEbCwAAICdYAAAAMcECAACICRYAAEBMsAAAAGKHa4VapeVJKwEA/662yabV/iWeVeNpN+pj73tvdZfLddN+3lgAAAAxwQIAAIgJFgAAQEywAAAAYoIFAAAQ29wK9fLjterArf4rXssTAAAM/F38fVtc8MYCAACICRYAAEBMsAAAAGKCBQAAEBMsAACA2G6tUKvbu9WqdPza/XmOeV7PqHNW28DhGnrMvcceXFfwtdvt88vtb2/vVce5XK6b9vPGAgAAiAkWAABATLAAAABiggUAABATLAAAgNjhWqFGNUBopADOyNrH2bkHOAOtUAAAQDeCBQAAEBMsAACAmGABAADEBAsAACAmWAAAALHNdbPX62XvsQAL+yhs/9bo+Cod+xg1z85vH6V5LinNf+35cn7Zyho0J3WzAABAN4IFAAAQEywAAICYYAEAAMQECwAAINa9Fcp/3bOF6+R5e8/d3u1PHNtR7+2jfq9a5uH4apvFZuNafI5WKAAAoBvBAgAAiAkWAABATLAAAABiggUAABDr3goFAGdX26yjyeb4jtqo1apFqnYeRn3ubEpNjn9VXm9aoQAAgG4ECwAAICZYAAAAMcECAACICRYAAEBscyvUy4/XJh/Y6r/6V/8v/b2ZN45mthad2nts74YS9/xzzNtYo+a/1f040t5rzd6Oeo8ddU3RCgUAAHQjWAAAADHBAgAAiAkWAABATLAAAABicSvU2VqeZmuwmG3eWjXljPpes40HZjFbKxfnpF3tl9nan2acI9rRCgUAAHQjWAAAADHBAgAAiAkWAABATLAAAABim1uhrtfLl9s1hQD8slKrzFes6WOZ/z5G3qd7N1sd1eprayvD2km1QgEAAL0IFgAAQEywAAAAYoIFAAAQEywAAIBY3AoFnItmjmM46nk86vcCGEkrFAAA0I1gAQAAxAQLAAAgJlgAAAAxwQIAAIh1b4XS2PG30jyUnG1+gDVY00m4fmANWqEAAIBuBAsAACAmWAAAADHBAgAAiAkWAABAbLdWqNqmh1WaIWrbnGrN9n0BWhrViDfbM6bVeGb7XrAq99JjWqEAAIBuBAsAACAmWAAAADHBAgAAiAkWAABALG6FOup/0Wt/AvjPRj0DRrVLreKoz2ZgDK1QAABAN4IFAAAQEywAAICYYAEAAMQECwAAILa5Ferlx+uXm2sbJlo1VWi8gDVoWANG0R4GbWiFAgAAuhEsAACAmGABAADEBAsAACAmWAAAALHNrVDX62XvsTRx1Laoj8L2b4Xtt9tn1fF/vr1X7V+rNM4jqL3mjnqNzsY8A/xiTSShFQoAAOhGsAAAAGKCBQAAEBMsAACAmGABAADEDtcKVVJqSdq7DYnHVmmLKrVpvLy0a9RYpbFjlXGylkf3WI3Vr0P3F/BI7VpZu3YUj/99U1zwxgIAAMgJFgAAQEywAAAAYoIFAAAQEywAAIDYsq1QH6MHwJdKLU+l8/WXBpTmtOvQkxajx8zPY7PNz2zjob1Wz8jT0QoFAAD0IlgAAAAxwQIAAIgJFgAAQEywAAAAYptboV5+vH65ebamhNpGh9p2qVKLUcmfg+an1M4EwHFoMYI2jtoWVbsWFOdBKxQAANCLYAEAAMQECwAAICZYAAAAMcECAACIbW6Ful4ve4+FB1Zv/mjVtrDK9315Wf+ccWy196Trtq29G2icr+M4alvRUY2693a/TrRCAQAAvQgWAABATLAAAABiggUAABATLAAAgNhurVCtGnE068ypVaPMqGYa1xVbaWThd9aIv1lDf7FGnNPZrvXL5bppP28sAACAmGABAADEBAsAACAmWAAAADHBAgAAiO3WClWrtmFCI0Ufo+Z5tpYN11U/s517gCNo1c5Ye/xaft89Nur3slYoAACgG8ECAACICRYAAEBMsAAAAGKCBQAAENutFWrv/+qfrTlm9baCUfNZ3UpQOc7Vz8szPgrbv3UdxXizrRF7O+raCuzLc5gttEIBAADdCBYAAEBMsAAAAGKCBQAAEBMsAACA2G6tULPRdPLY3o0ye7dC7N1S8cz1s3oTxtmawvb+vqNam1a/DlvxDOB3M94Xo56f8LviWvl9U1zwxgIAAMgJFgAAQEywAAAAYoIFAAAQEywAAIDYaVqhSjSFPHa2NopnWjlaXUOzzbWGkufM1ozmfD2n1Xyu0q4G8Mjlct20nzcWAABATLAAAABiggUAABATLAAAgJhgAQAAxE7fClUyW1tUqeFj7yaYVZpmRo5zlWuFtaxy7/Gc2nXDeQdG0goFAAB0I1gAAAAxwQIAAIgJFgAAQEywAAAAYlqhCjR20Nts7VIlq1zrreZzle+7utvts2r/t7f3nUYCPDKqse6jsP3brp+6v1HzWfu5WqEAAIBuBAsAACAmWAAAADHBAgAAiAkWAABATCsUsIS9m9pGNXMcVW3LUyvaovjdUZuEoDetUAAAQDeCBQAAEBMsAACAmGABAADEBAsAACCmFeqg9m640aBzXrXtTK3Mdm3t3VIFrM1zso9Wz6SznZfqefu+KS54YwEAAOQECwAAICZYAAAAMcECAACICRYAAEAsboXy3/j8bpUWjFXG2dKoNqdaezeX7f258LszrjVH9Gg9cS45g8vlumk/bywAAICYYAEAAMQECwAAICZYAAAAMcECAACIxa1QtTS1nFNtM8qotjENLvNybgBgDK1QAABAN4IFAAAQEywAAICYYAEAAMQECwAAINa9FaqWJphjGNUG1upzW7VUPfqMVtwzAG09egYcdW31LOF3WqEAAIBuBAsAACAmWAAAADHBAgAAiAkWAABAbPpWqFqrtBjUjvOo32vU8Vu2PJW0GtPq1wTAbG63z2bHent7/3J7q+eMNZ0ZaIUCAAC6ESwAAICYYAEAAMQECwAAICZYAAAAsd1aoWZrQ1ilQWeVcY5Se12NnLcezVMtzHZttWrHKpnt+67C2jSnUffL6i12ta1QPwvNTy8vLy//bNgwVfvZX/lW2P5R+bml43BOWqEAAIBuBAsAACAmWAAAADHBAgAAiAkWAABAbLdWqFqrN0yMGs9s88B/5pwBPDbbOvmoUanUClXb5tTqOCVankhohQIAALoRLAAAgJhgAQAAxAQLAAAgJlgAAACxza1QAAAAJd5YAAAAMcECAACICRYAAEBMsAAAAGKCBQAAEBMsAACAmGABAADEBAsAACAmWAAAALH/BatDbZ+fjbyxAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAGVCAYAAABjBWf4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAQcUlEQVR4nO3c240byQEFUMnYIAw4C5NZGFAWgsIQNgxBWQjYLDjOwoCzoD8G8HrXUxRrbj27z/mkWmR1dXU1L4i5H+/3+/0DAABA4C+zBwAAAOxPsAAAAGKCBQAAEBMsAACAmGABAADEBAsAACAmWAAAADHBAgAAiAkWAABA7JdnD7zdrm++fv3t5e3j/3F534gOpnZ+SseXrDbPteMvWe28+Lnvkz732yZ7UO38lM7rS+V5/f3lx5uvXy6fKkfETuzF5zVrLz6bz4XXW30v7v39saT4/tfbU//fLxYAAEBMsAAAAGKCBQAAEBMsAACAmGABAADEPt7v9/szB5ZaoXif3q0BrdS2D5yxQaT3NSiZNdezrn2p6aTUzAEraNXscsa9lfepbYUqNcf986DNcZ4Z76MVCgAAGEawAAAAYoIFAAAQEywAAICYYAEAAMS0QjWyS5PHLuM8o9omj1k0ahyDveCx3s195pnULs+MWTyr2tIKBQAADCNYAAAAMcECAACICRYAAEBMsAAAAGLDW6FqGzJKx5f0buzQ8EEvuzR8aNp4zB7BM3o/285mxfuutKd/K4z1y2LXuLTXz3pWefbMpRUKAAAYRrAAAABiggUAABATLAAAgJhgAQAAxIa3QsEKVmwQaaV3Y4dmDoCfO/JzhvPRCgUAAAwjWAAAADHBAgAAiAkWAABATLAAAABiWqHg5I7aXHLU84Ieet8v7kdS1tCrWfOgFQoAABhGsAAAAGKCBQAAEBMsAACAmGABAADEtEKdjFYFRrPmAH5X2hNbqd1ba8fT6v09A/aiFQoAABhGsAAAAGKCBQAAEBMsAACAmGABAADEtEJtrlXbwvfKz/3WuUWC89Ig8pj5gX7cXz/Xu0Wq9nNdmzG0QgEAAMMIFgAAQEywAAAAYoIFAAAQEywAAICYViim0vLAs2Y1kdSypvlf1kNbu+wD72GtsDKtUAAAwDCCBQAAEBMsAACAmGABAADEBAsAACCmFQpOThNJW0edz95tPEdu+2nhqOuK+Y66to56XrNohQIAAIYRLAAAgJhgAQAAxAQLAAAgJlgAAACxZVqh/PX+mna/Lq3G/6ixZpe5aGX3NdGKeZjL/I9hnvmzWWvCWpxLKxQAADCMYAEAAMQECwAAICZYAAAAMcECAACILdMKBcDxaXaZy/y3Vzunqx0Pz9AKBQAADCNYAAAAMcECAACICRYAAEBMsAAAAGJxK5T2AVbQex2W3r/lZ8Az7LkAjKYVCgAAGEawAAAAYoIFAAAQEywAAICYYAEAAMTiVihgb0dtGao9r6POQ2/mjbN71Br4FvcGO9IKBQAADCNYAAAAMcECAACICRYAAEBMsAAAAGLDW6FWaxBZbTxnY/6hL/cY7M09zAq0QgEAAMMIFgAAQEywAAAAYoIFAAAQEywAAIDY8FYogBFKTSolGlba0mQDjGCvGUMrFAAAMIxgAQAAxAQLAAAgJlgAAAAxwQIAAIhphQLgvzSswDHV3turHc9cWqEAAIBhBAsAACAmWAAAADHBAgAAiAkWAABATCsUh6Z14vhK17iWNcH/6r132Jte7TIPLy8/iv92uXzq+hmt3h8SWqEAAIBhBAsAACAmWAAAADHBAgAAiAkWAABA7DStULs0TwDnUttqZc/iDFZ7Zj+6T1e7J2fN3WrXjMdqr5dWKAAAYBjBAgAAiAkWAABATLAAAABiggUAABA7TSsU5/S90ft8bvQ+rKu6IUPzCgB/8vLy483XL5dPg0fSllYoAABgGMECAACICRYAAEBMsAAAAGKCBQAAENMKBWxNS9Jj5uecXPd1uTbsSCsUAAAwjGABAADEBAsAACAmWAAAADHBAgAAiGmFgsWVGkRKNIsAAC1phQIAAIYRLAAAgJhgAQAAxAQLAAAgJlgAAACxX2YPgD8qNQCVmn5qG4NqzWoY6n1eLfWeo1bvv9Oc7myX61U7ztq9ibm0yQEz+MUCAACICRYAAEBMsAAAAGKCBQAAEBMsAACA2Mf7/X5/5sDb7dp7LF1pxAGOQHsPR6JtDPZwvd6eOs4vFgAAQEywAAAAYoIFAAAQEywAAICYYAEAAMR+6fXGvZsetDzB3rS+wP87W0vSUc/rjFZbu6uN5yz8YgEAAMQECwAAICZYAAAAMcECAACICRYAAEDs4/1+vz9z4O127T2WKmdrherdYnC2+TyC0pqYdS01bbACTTCvavcB8/PqbPMwkzW6l+v19tRxfrEAAABiggUAABATLAAAgJhgAQAAxAQLAAAg9nQr1IdfP3Yeyh7O1kqgLYrRznaPwUj29PZqG/qOuse1Ot9Z86al6jGtUAAAwDCCBQAAEBMsAACAmGABAADEBAsAACC2bSvU2f4a/2w0l8zXqsmj9+fOcrbGFx6zZ7G6o+5Nu9x7u8+/VigAAGAYwQIAAIgJFgAAQEywAAAAYoIFAAAQe7oV6na79h4LDLNLi8QjuzdM8FjvNdpq/RzhXqJs93Y46/P4Smtl1h562DX39bkSWb9YAAAAMcECAACICRYAAEBMsAAAAGKCBQAAENMKVVD6q35NPLC3Vvf2YZs/OKXTNdwAdbRCAQAAowgWAABATLAAAABiggUAABATLAAAgJhWKNhUbbvRrHYXbUtAK5oZf3fUvXL3a9z7utQ+41vN5/V6e+o4v1gAAAAxwQIAAIgJFgAAQEywAAAAYoIFAAAQ0wp1ULWtBLu3MNBeq4aJVg0ZvT+3WXPGJm1drfTeO3o3nfBqVpMNsAetUAAAwDCCBQAAEBMsAACAmGABAADEBAsAACCmFYp30dTS3lHntFVL0i6tSrtfr1k02QH8XO/vCsW9+OtTccEvFgAAQE6wAAAAYoIFAAAQEywAAICYYAEAAMQECwAAIHb6utmjVnwCAPtr9T3F9x0S1+vtqeP8YgEAAMQECwAAICZYAAAAMcECAACICRYAAEDs9K1QHFupBaPWo9aM1Zo2VhsPQCtH2N+OcA41vlce/7ny+LPN5yxaoQAAgGEECwAAICZYAAAAMcECAACICRYAAEBMKxTARK2ay3rTsAJz1LYe1R5f29q0mtoWKd5HKxQAADCMYAEAAMQECwAAICZYAAAAMcECAACIaYUCADiIUstTqT2p1CL1ZfMmuG+V7Vi7693u9U0rFAAAMIpgAQAAxAQLAAAgJlgAAAAxwQIAAIhphWIrpfaKkqO2P/BzpbViTZyT9QB91bZR8ap3m1MrWqEAAIBhBAsAACAmWAAAADHBAgAAiAkWAABATCtUwVEbRI56XiVnO99HzMWr2nnoPW+uy5pcl7nM/35cs7l6t0tphQIAAIYRLAAAgJhgAQAAxAQLAAAgJlgAAACx4a1Qu7cG7D5+SK3WqrSas50vr0rXvaT3erAO3+fl5Uf1/7lcPjX57KNes13Oa5dxznLVCgUAAIwiWAAAADHBAgAAiAkWAABATLAAAABiw1uh4OxmNU+s1lpzNrs3juw+/t2Zf2AmrVAAAMAwggUAABATLAAAgJhgAQAAxAQLAAAgtnwrlCYMVtdqjWqLeuWeB4C1aIUCAACGESwAAICYYAEAAMQECwAAICZYAAAAseVboZjr5eXHm69fLp8Gj2QMjUS/MxcApDxLHttlfrRCAQAAwwgWAABATLAAAABiggUAABATLAAAgNgyrVClv4ovWe2v5XdRanlq5cu//1Z1/BGu4y6NDryq3WtKXF9Y34r784pjWklpfr799V9vvn7G7x0zaIUCAACGESwAAICYYAEAAMQECwAAICZYAAAAsWVaoTgn7Ri/Mxc8wzoBWqrdU1rtQb1bKlsptU7Vzk/JLnu3VigAAGAYwQIAAIgJFgAAQEywAAAAYoIFAAAQ0woF8IAWJoB19W6Xqm2FOiqtUAAAwDCCBQAAEBMsAACAmGABAADEBAsAACCmFaqg1ART62ytASWadc7rqNd+1nnVNqBcLp86jQRgXbs8e7YZp1YoAABgFMECAACICRYAAEBMsAAAAGKCBQAAENMKNckuLQCrqW3rqp1P12U/vdfE7mpbpEr+WWiX+tzk3QFeeQ6vSSsUAAAwjGABAADEBAsAACAmWAAAADHBAgAAiGmFOhltCwBwPp7/JLRCAQAAwwgWAABATLAAAABiggUAABATLAAAgJhWKIAFtWpw0QQDfbnHGKm03rr7+lRc8IsFAACQEywAAICYYAEAAMQECwAAICZYAAAAMa1QLEnLBgBAHy8vP958/XL59Obr1+vtqff1iwUAABATLAAAgJhgAQAAxAQLAAAgJlgAAAAxrVAAAECRVigAAGAYwQIAAIgJFgAAQEywAAAAYoIFAAAQ+2X2AN7r+tvLm6/f/nEZPBIAgD+q/Z5SOr4V34/WdLTvs36xAAAAYoIFAAAQEywAAICYYAEAAMQECwAAIPbxfr/fnznwdrv2HgsAB3G0phMoqW1z+rLYPfB59gAWUXsdZ+1ls/bW6/X21HF+sQAAAGKCBQAAEBMsAACAmGABAADEBAsAACCmFYqpNMccn2vMCqxDEt9nD2CCUltU7VxonVpT7Z6oFQoAABhGsAAAAGKCBQAAEBMsAACAmGABAADElm+F0uSxl9L1OrKjrsXaa3nUeQByZ9xPztgk1YIWqVe975nS+izNv1YoAABgGMECAACICRYAAEBMsAAAAGKCBQAAEFu+FaqWFqlXZ2xngmecbS9gDM+ex1o9k0rz2Wr+NTm9nzanNdXeG8V79etTccEvFgAAQE6wAAAAYoIFAAAQEywAAICYYAEAAMRO0wpVMquxo1WDhfYnaEN7T1takh5bbX5WGw/vN6vZapdWKGv9Ma1QAADAdIIFAAAQEywAAICYYAEAAMQECwAAIHa4VqiSVu1Jq7UG1LYbaJF6dcb5OeM5z7DaHrGa1Zr7Zj0ben/uUZ95jNPq+8Vqa2i1PahW72d28fpeb0/9f79YAAAAMcECAACICRYAAEBMsAAAAGKCBQAAEDtNK1StVu0GR23cmdWSsFObQ++GjLOtrd3Pd1a70az5POp1bGW1tqvdm29WG/9Ozvas2v28pn3/0goFAACMIlgAAAAxwQIAAIgJFgAAQEywAAAAYk+3Qn349eObL5+tiWG1doNWznYda73nuu8yp987v//nzu+/+z15traoVna5v+BoWu0RvduZdmmvbKX7s0QrFAAAMIpgAQAAxAQLAAAgJlgAAAAxwQIAAIg93Qp1u117j6WJ1RpNVmteKY2n1AzUu9GH/bRqkdplba22p/SmbekYereBWSewh2bPsK/Plcj6xQIAAIgJFgAAQEywAAAAYoIFAAAQEywAAIDY4Vqhemv11/W17Uy1/v7y483XL5dPjT7hbbVNJJzX7mtltbao2nnrvZft4qjrcNb4W7VRtVrPu1zHDx/mzcUR5o7+rtfbU8f5xQIAAIgJFgAAQEywAAAAYoIFAAAQEywAAICYVqiDqm2X+lx5vNYJzm5WK5R75tVR95Te57X7+x/BrDny3CahFQoAABhGsAAAAGKCBQAAEBMsAACAmGABAADEnm6FAgAAKPGLBQAAEBMsAACAmGABAADEBAsAACAmWAAAADHBAgAAiAkWAABATLAAAABiggUAABD7D+g9QevkNavvAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = ddpm_sampler #@param ['ddpm_sampler', 'pc_sampler'] {'type': 'raw'}\n",
        "\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                (128, 256),\n",
        "                                sample_batch_size,)"
      ],
      "metadata": {
        "id": "gzMLizIXPUAy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "83d5aa4f-724c-4a6a-bd86-df689655207a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "(10, 128, 256, 1)\n",
            "(10, 128, 256, 64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/350 [00:04<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-a297e5e2a35d>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m## Generate samples using the specified sampler.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m samples, samples_list = sampler(model,\n\u001b[0m\u001b[1;32m     13\u001b[0m                                 \u001b[0mimg_embed_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                 \u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-e10479327904>\u001b[0m in \u001b[0;36mddpm_sampler\u001b[0;34m(model, img_embed_size, image_size, batch_size, num_steps, eps)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# optional second order sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msecond_order_alpha\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             embed_pred_x0, pred_noises = second_order_correction(\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mbatch_time_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-d81f185899fc>\u001b[0m in \u001b[0;36msecond_order_correction\u001b[0;34m(model, diffusion_times, step_size, noisy_images, signal_rates, noise_rates, pred_images, pred_noises, second_order_alpha)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0malpha_signal_rates\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred_images\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha_noise_rates\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred_noises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     )\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mpred_x0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_pred_noises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mint_encoded_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_x0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0membed_pred_x0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_encoded_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-99eab0836bb7>\u001b[0m in \u001b[0;36mdenoise\u001b[0;34m(self, noisy_images, noise_rates, signal_rates, training, mask, pixels)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# predict noise component and calculate the image component using it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mpred_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoisy_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_rates\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(tensors, axis)\u001b[0m\n\u001b[1;32m   3579\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3580\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3581\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'concatenate_92' (type Concatenate).\n\n{{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:GPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [10,128,256,64] vs. shape[1] = [10,64,128,1] [Op:ConcatV2] name: concat\n\nCall arguments received by layer 'concatenate_92' (type Concatenate):\n  • inputs=['tf.Tensor(shape=(10, 128, 256, 64), dtype=float32)', 'tf.Tensor(shape=(10, 64, 128, 1), dtype=float32)', 'tf.Tensor(shape=(10, 64, 128, 64), dtype=float32)']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(sample_batch_size):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(np.argmax(samples[i].numpy(), axis=-1).reshape((128, 256)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "p88yuQgCPUA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GIF"
      ],
      "metadata": {
        "id": "8X9v42ujwnt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "images = []\n",
        "for i, batch in enumerate(samples_list):\n",
        "    figure = plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Timestep {0:.3f}\".format(1 - (i / 350)))\n",
        "    plt.imshow(np.argmax(batch[0].numpy(), axis=-1).reshape((64, 128)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.savefig('foo.png', bbox_inches='tight')\n",
        "    images.append(imageio.imread('foo.png'))\n",
        "    plt.show() #close(figure)\n",
        "imageio.mimsave('/movie.gif', images)"
      ],
      "metadata": {
        "id": "m9JzX8pVMf7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Models"
      ],
      "metadata": {
        "id": "GCPKYflD2fyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "SAVE_AND_TAR_RESULTS_WEIGHTS = True\n",
        "\n",
        "if SAVE_AND_TAR_RESULTS_WEIGHTS:\n",
        "  diffusion_checkpoint_path = \"diffusion_weights_horiz/cp-diffusion2d_net_horiz.ckpt\"\n",
        "  diffusion_checkpoint_dir = os.path.dirname(diffusion_checkpoint_path)\n",
        "\n",
        "  model.ema_network.save_weights(diffusion_checkpoint_path)\n",
        "\n",
        "  !tar -czvf diffusion_weights_horiz.tar.gz ./diffusion_weights_horiz\n",
        "\n",
        "  diffusion_checkpoint_path = \"diffusion_weights_horiz/cp-diffusion2d_embed_horiz.ckpt\"\n",
        "  diffusion_checkpoint_dir = os.path.dirname(diffusion_checkpoint_path)\n",
        "\n",
        "  model.embedding_layer.save_weights(diffusion_checkpoint_path)\n",
        "\n",
        "  !tar -czvf diffusion_weights_horiz.tar.gz ./diffusion_weights_horiz"
      ],
      "metadata": {
        "id": "gi9sVjP0QHdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditioning"
      ],
      "metadata": {
        "id": "BaRNaI2d2jLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from data.load_data import ConditionalDataGenerator"
      ],
      "metadata": {
        "id": "NaalGhKP8eJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "slice_size = (64, 128, 4)\n",
        "dataloader = ConditionalDataGenerator(x_train, 1, slice_size, wells=32, mode=3)\n",
        "pixels, mask, ground_truth = dataloader.__getitem__(0)\n",
        "print(pixels.shape, mask.shape, ground_truth.shape)"
      ],
      "metadata": {
        "id": "ih3wx-i58Yar",
        "outputId": "7307bc66-5504-45f8-d8ad-c7b8358b4d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-fec2465ac74b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mslice_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConditionalDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwells\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ConditionalDataGenerator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_second_order_correction(\n",
        "    model,\n",
        "    diffusion_times,\n",
        "    step_size,\n",
        "    noisy_images,\n",
        "    signal_rates,\n",
        "    noise_rates,\n",
        "    pred_images,\n",
        "    pred_noises,\n",
        "    second_order_alpha,\n",
        "    mask,\n",
        "    pixels,\n",
        "):\n",
        "    # generic second-order Runge-Kutta method\n",
        "    # https://en.wikipedia.org/wiki/List_of_Runge%E2%80%93Kutta_methods#Generic_second-order_method\n",
        "    # based on https://arxiv.org/abs/2206.00364\n",
        "\n",
        "    # use first estimate to sample alpha steps away\n",
        "    alpha_signal_rates, alpha_noise_rates = model.diffusion_schedule(\n",
        "        diffusion_times - second_order_alpha * step_size\n",
        "    )\n",
        "    alpha_noisy_images = (\n",
        "        alpha_signal_rates * pred_images + alpha_noise_rates * pred_noises\n",
        "    )\n",
        "    pred_x0, alpha_pred_noises = model.denoise(noisy_images, noise_rates, signal_rates, training=False)\n",
        "    pred_x0 = pred_x0.numpy()\n",
        "    _, real_x_idx, real_y_idx, _ = np.nonzero(mask)\n",
        "    real_x_idx, real_y_idx = list(map(list, zip(*list(set(zip(real_x_idx, real_y_idx))))))\n",
        "\n",
        "    #pred_x0[:, real_x_idx, real_y_idx, :] = pixels[:, real_x_idx, real_y_idx, :]\n",
        "\n",
        "\n",
        "    int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "    embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "    # linearly combine the two noise estimates\n",
        "    pred_noises = (1.0 - 1.0 / (2.0 * second_order_alpha)) * pred_noises + 1.0 / (\n",
        "        2.0 * second_order_alpha\n",
        "        ) * alpha_pred_noises\n",
        "\n",
        "    pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
        "    return pred_images, pred_noises"
      ],
      "metadata": {
        "id": "v2oCx8E-4ZzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "def compute_beta(curr_alphas, prev_alphas):\n",
        "\n",
        "    betas = 1 - (prev_alphas / curr_alphas)\n",
        "    return betas"
      ],
      "metadata": {
        "id": "fuXMyVZU4ZzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import xla\n",
        "def conditional_ddpm_sampler(model, img_embed_size, image_size, mask, \n",
        "                 pixels, batch_size=10, num_steps=350, eps=1e-3):\n",
        "    second_order_alpha = 1.1\n",
        "    # T and schedule\n",
        "    t = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32)\n",
        "    noise_rates, signal_rates = model.diffusion_schedule(t)\n",
        "\n",
        "    _, real_x_idx, real_y_idx, _ = np.nonzero(mask)\n",
        "    real_x_idx, real_y_idx = list(map(list, zip(*list(set(zip(real_x_idx, real_y_idx))))))\n",
        "\n",
        "    # Sample noise\n",
        "    uniform_init_x = tf.random.uniform((batch_size, image_size[0], image_size[1]), 0, 4, dtype=tf.dtypes.int32).numpy()\n",
        "    uniform_init_x[:, real_x_idx, real_y_idx] = np.argmax(pixels, axis=-1)[:, real_x_idx, real_y_idx]\n",
        "\n",
        "    noises = tf.random.normal(shape=(batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "\n",
        "    embedded_init_x = model.embedding_layer(uniform_init_x)\n",
        "    init_x = signal_rates * embedded_init_x  + noise_rates * noises\n",
        "\n",
        "    # Keep track of the chain\n",
        "    samples_list = []\n",
        "    samples_list.append( tf.keras.backend.constant(keras.utils.to_categorical(uniform_init_x)))\n",
        "\n",
        "    # Steps and other algorithmic variables\n",
        "    time_steps = tf.linspace(1., eps, num_steps)\n",
        "    step_size = time_steps[0] - time_steps[1]\n",
        "    x = init_x\n",
        "    prev_alphas = signal_rates**2\n",
        "\n",
        "    # INFERENCE REVERSE LOOP\n",
        "    for time_step in tqdm.tqdm(time_steps):\n",
        "        batch_time_step = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32) * time_step\n",
        "        noise_rates, signal_rates = model.diffusion_schedule(batch_time_step)\n",
        "        cur_alphas = signal_rates**2\n",
        "        betas = compute_beta(cur_alphas, prev_alphas)\n",
        "\n",
        "        # PREDICT IMAGE\n",
        "        pred_x0, pred_noise = model.denoise(x, noise_rates, signal_rates, training=False)\n",
        "        pred_x0 = pred_x0.numpy()\n",
        "\n",
        "        #real_proba_condi = np.squeeze(np.multiply(mask, mask))\n",
        "        #pred_x0 = pred_x0[:, real_x_idx, real_y_idx, :].assign(pixels[:, real_x_idx, real_y_idx, :])\n",
        "\n",
        "        int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "        embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "        # optional second order sampling\n",
        "        if second_order_alpha is not None:\n",
        "            embed_pred_x0, pred_noises = conditional_second_order_correction(\n",
        "                model,\n",
        "                batch_time_step,\n",
        "                step_size,\n",
        "                x,\n",
        "                signal_rates,\n",
        "                noise_rates,\n",
        "                embed_pred_x0,\n",
        "                pred_noise,\n",
        "                second_order_alpha,\n",
        "                mask,\n",
        "                pixels)\n",
        "\n",
        "        mean_x0 = tf.math.sqrt(cur_alphas) * betas / (1 - prev_alphas) * embed_pred_x0\n",
        "        mean_x = tf.math.sqrt(1 - betas) * (1 - cur_alphas) / (1 - prev_alphas) * x\n",
        "        x = mean_x + mean_x0 + tf.reshape(tf.math.sqrt(betas), (-1, 1, 1, 1)) * tf.random.normal(x.shape)\n",
        "\n",
        "        samples_list.append(pred_x0)\n",
        "        prev_alphas = cur_alphas\n",
        "\n",
        "    return pred_x0, samples_list"
      ],
      "metadata": {
        "id": "6cVAhljx4ZzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = conditional_ddpm_sampler\n",
        "\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                (64, 128),\n",
        "                                mask,\n",
        "                                pixels,\n",
        "                                sample_batch_size,)"
      ],
      "metadata": {
        "id": "e_7XS9Kd4ZzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from utils.visualisation import *\n",
        "from data.load_data import get_3d_flumy_data, load_data, ConditionalDataGenerator\n",
        "from models.load_trained_models import load_msgen_horizontal, wgan_horizontal,\\\n",
        "    load_msnwgen_2d_gs_horizontal, load_wgan_gs_horizontal, load_mswgen_sn_3d_horizontal\n",
        "from utils.utils import generate_noise, correct_percentage\n"
      ],
      "metadata": {
        "id": "pP5_ix6TCLdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_simulations = 3\n",
        "cmap, norm = get_color_map(number_of_categories=4)\n",
        "\n",
        "print_conditioned_results(ground_truth, samples, mask, nb_simulations, cmap, norm)"
      ],
      "metadata": {
        "id": "0nla2sslCFTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples.shape"
      ],
      "metadata": {
        "id": "mysb9YlUA-Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.axis('off')\n",
        "\n",
        "plt.imshow(np.argmax(samples[0], axis=-1).reshape((64, 128)),\n",
        "            interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WcfukFdDA4Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.axis('off')\n",
        "\n",
        "plt.imshow(np.argmax(ground_truth, axis=-1).reshape((64, 128)),\n",
        "            interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "URj4sIq1BlYH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ddim",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}