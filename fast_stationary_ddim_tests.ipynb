{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pumafi/dl_spatial_gen_geol_facies/blob/main/fast_stationary_ddim_tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "TA0koXfT2e7k",
        "outputId": "2475076e-b259-4d4d-ca82-249d82ba2020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun 12 13:04:16 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## IDEAS \n",
        "1.   Normalization in embedding ?\n",
        "2.   Formula when I replace beta by t\n",
        "3.   Use keras inference for potential changes\n",
        "\n",
        "## What to try next?\n",
        "\n",
        "If you would like to dive in deeper to the topic, a recommend checking out\n",
        "[this repository](https://github.com/beresandras/clear-diffusion-keras) that I created in\n",
        "preparation for this code example, which implements a wider range of features in a\n",
        "similar style, such as:\n",
        "\n",
        "* stochastic sampling\n",
        "* second-order sampling based on the\n",
        "[differential equation view of DDIMs (Equation 13)](https://arxiv.org/abs/2010.02502)\n",
        "* more diffusion schedules\n",
        "* more network output types: predicting image or\n",
        "[velocity (Appendix D)](https://arxiv.org/abs/2202.00512) instead of noise\n",
        "* more datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "rge41L-HIY-i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqkfNOJcD0Ym"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install deel.lip\n",
        "!python -m pip install -i https://test.pypi.org/simple/ gstlearn"
      ],
      "metadata": {
        "id": "u03Nq4eK2ems",
        "outputId": "6721aa3d-ac8c-41af-f3c6-25ae4662cfc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deel.lip\n",
            "  Downloading deel_lip-1.4.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deel.lip) (1.22.4)\n",
            "Requirement already satisfied: tensorflow~=2.2 in /usr/local/lib/python3.10/dist-packages (from deel.lip) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.2->deel.lip) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.2->deel.lip) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.2->deel.lip) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (3.2.2)\n",
            "Installing collected packages: deel.lip\n",
            "Successfully installed deel.lip-1.4.0\n",
            "Looking in indexes: https://test.pypi.org/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gstlearn\n",
            "  Downloading https://test-files.pythonhosted.org/packages/4b/33/9b8e2546cfe286525409a1d17279b325a7a1d2968eba07357a0e30976d29/gstlearn-0.2.1-cp310-cp310-manylinux1_x86_64.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gstlearn) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gstlearn) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gstlearn) (1.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from gstlearn) (5.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gstlearn) (1.5.3)\n",
            "INFO: pip is looking at multiple versions of gstlearn to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://test-files.pythonhosted.org/packages/ea/39/5475c8fac8f0b9897ef6be34b92d40dae6d209dbfe2a7db0fb7a9386fadc/gstlearn-0.1.38-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://test-files.pythonhosted.org/packages/11/e9/eee76162b77c60d4c57aea94c230259605295eb3ad7e76b4b3c67773ff0f/gstlearn-0.1.37-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gstlearn\n",
            "Successfully installed gstlearn-0.1.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RUNNING_IN_COLAB = True\n",
        "\n",
        "if RUNNING_IN_COLAB:\n",
        "    # Uses a private Auth Token, giving read and write access to repo\n",
        "    # TO DELETE IF REPO GOES PUBLIC\n",
        "    REPO_URL = 'https://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@github.com/Pumafi/flumy-wgan-mines'\n",
        "    BRANCH   = 'main'\n",
        "    REPO_DIR = 'flumy-wgan-mines'\n",
        "\n",
        "    from pathlib import Path\n",
        "\n",
        "    %cd /content\n",
        "\n",
        "    if Path(REPO_DIR).is_dir():\n",
        "      !rm -rf {REPO_DIR}\n",
        "\n",
        "    # Download the repository\n",
        "    if not Path(REPO_DIR).is_dir():\n",
        "        !git clone --branch {BRANCH} --depth=1 -- {REPO_URL} {REPO_DIR}\n",
        "    \n",
        "    %cd {REPO_DIR}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqPH8VxsGGrb",
        "outputId": "3f421c6d-ad7e-49eb-d1b0-04c8efdd0c26"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'flumy-wgan-mines'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 146 (delta 31), reused 74 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 140.19 MiB | 10.37 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n",
            "Updating files: 100% (121/121), done.\n",
            "/content/flumy-wgan-mines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3E8qnnCGH5K",
        "outputId": "63d1967f-e21b-4eff-832a-f3e9ee9fdfc4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2VQcRg8KD0Yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "983f4268-8352-4e71-d12d-dba7d95073be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from data.load_data import load_data\n",
        "from utils.visualisation import get_color_map\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "1eF1rmySGCzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful constants\n",
        "image_size = (64, 128)\n",
        "cmap, norm = get_color_map(number_of_categories=4)\n",
        "facies_names = np.array([\"Sand, Channel lag\", \"Sand, Point bar\", \"Silts, Levee\", \"Shale, Overbank\"])\n",
        "x = load_data(image_size[0], image_size[1], \"./data/horizontal/dataFlumyHoriz.csv\")\n",
        "x_train = x[:2760]\n",
        "x_test = x[2760:]"
      ],
      "metadata": {
        "id": "4yPyDmhUGCTa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7-ElzPrD0Yq"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FBoSc7iCD0Ys"
      },
      "outputs": [],
      "source": [
        "# sampling\n",
        "min_signal_rate = 0.02\n",
        "max_signal_rate = 0.95\n",
        "\n",
        "# architecture\n",
        "widths = [32, 64, 128, 256]\n",
        "block_depth = 2\n",
        "\n",
        "# Data values embedding\n",
        "img_embed_size = 64\n",
        "categories_nb = 4\n",
        "\n",
        "# optimization\n",
        "batch_size = 30\n",
        "ema = 0.999\n",
        "learning_rate = 1e-4\n",
        "embeding_net_lr = 1e-3\n",
        "weight_decay = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diffusion Schedules"
      ],
      "metadata": {
        "id": "acL9rhHAdCCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "class DiffusionSchedule(ABC):\n",
        "    def __init__(self, start_log_snr, end_log_snr):\n",
        "        assert (\n",
        "            start_log_snr > end_log_snr\n",
        "        ), \"The starting SNR has to be higher than the final SNR.\"\n",
        "\n",
        "        self.end_beta = 20\n",
        "        self.start_beta = 0.1\n",
        "\n",
        "        self.start_snr = tf.exp(start_log_snr)\n",
        "        self.end_snr = tf.exp(end_log_snr)\n",
        "\n",
        "        #self.start_noise_power = 1.0 / (1.0 + self.start_snr)\n",
        "        #self.end_noise_power = 1.0 / (1.0 + self.end_snr)\n",
        "\n",
        "    def __call__(self, diffusion_times):\n",
        "        signal_powers = self.get_noise_powers(diffusion_times)\n",
        "        signal_rates = tf.math.exp(signal_powers)\n",
        "\n",
        "        noise_rates = (1 - signal_rates**2)**0.5\n",
        "\n",
        "\n",
        "        # the signal and noise power will always sum to one\n",
        "        #signal_powers = 1.0 - noise_powers\n",
        "\n",
        "        # the rates are the square roots of the powers\n",
        "        # variance**0.5 -> standard deviation\n",
        "        #signal_rates = signal_powers**0.5\n",
        "        #noise_rates = noise_powers**0.5\n",
        "\n",
        "\n",
        "        return noise_rates, signal_rates\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_noise_powers(self, diffusion_times):\n",
        "        pass\n",
        "\n",
        "\n",
        "class SignalStepLinearSchedule(DiffusionSchedule):\n",
        "    # the ratio between next-step and current signal powers decreases approximately linearly to 1\n",
        "    # similar to the \"linear schedule\" of DDPM https://arxiv.org/abs/2006.11239\n",
        "    def get_noise_powers(self, diffusion_times):\n",
        "        \n",
        "        return -(self.end_beta - self.start_beta) / 4 * diffusion_times**2 - self.start_beta / 2 * diffusion_times\n",
        "\n",
        "        #return 1.0 - (1.0 - self.start_noise_power) * (\n",
        "        #    (1.0 - self.end_noise_power) / (1.0 - self.start_noise_power)\n",
        "        #) ** (diffusion_times**2)"
      ],
      "metadata": {
        "id": "Z5U9ClBmdDxI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_snr(signal_rate, noise_rate):\n",
        "    return tf.math.log(signal_rate / noise_rate)\n",
        "\n",
        "def get_t_from_snr(snr):\n",
        "    max_beta = 20.\n",
        "    min_beta = 0.1\n",
        "    return 2 * tf.math.log(tf.math.exp(-2*snr)+1) / (tf.math.sqrt(min_beta**2 + 2\n",
        "                                                                 * (max_beta - min_beta)\n",
        "                                                                 * tf.math.log(tf.math.exp(-2*snr)+1)) + min_beta)"
      ],
      "metadata": {
        "id": "WKvodkeuyJFb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diff_schedule = SignalStepLinearSchedule(start_log_snr=3.0, end_log_snr=-10.0,)\n",
        "t = np.array([0.486,])\n",
        "noise_rates, signal_rates = diff_schedule(t)\n",
        "snr = compute_snr(signal_rates, noise_rates)\n",
        "t_prime = get_t_from_snr(snr)\n",
        "print(t_prime)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qItvy_HRVox_",
        "outputId": "e93a1f4b-a1eb-49d1-8149-85d0dc7116ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0.486], shape=(1,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "QxyZaS_UJ_YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianFourierProjection(tf.keras.layers.Layer):\n",
        "    \"\"\"Gaussian random features for encoding time steps.\"\"\"  \n",
        "    def __init__(self, embed_dim, scale=30.):\n",
        "        super().__init__()\n",
        "        # Randomly sample weights during initialization. These weights are fixed \n",
        "        # during optimization and are not trainable.\n",
        "        self.W = self.add_weight(shape=(embed_dim // 2,),\n",
        "                                 trainable=False,\n",
        "                                 initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.), name=\"GFP\") * tf.constant(scale, dtype=tf.float32)\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        x_proj = x * self.W * tf.constant(2., dtype=tf.float32) * tf.constant(np.pi, dtype=tf.float32)\n",
        "        y = tf.concat([tf.math.sin(x_proj), tf.cos(x_proj)], axis=-1)\n",
        "        return y # Probleme vient pas de là :()\n",
        "\n",
        "class CustomLinear(tf.keras.layers.Layer):\n",
        "    \"\"\"Rhaaah.\"\"\"  \n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.W = tf.random.uniform((input_dim, output_dim), minval=-tf.math.sqrt(1/input_dim), maxval=tf.math.sqrt(1/input_dim))\n",
        "        self.b = tf.random.uniform((1, output_dim, ), minval=-tf.math.sqrt(1/input_dim), maxval=tf.math.sqrt(1/input_dim))\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        y = tf.tensordot(x, self.W, 1) + self.b\n",
        "        y = tf.keras.activations.gelu(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "class EmbedLayerNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, img_embed_size):\n",
        "        super().__init__()\n",
        "        self.beta = self.add_weight(shape=(1, 1, 1, 1),\n",
        "                                    initializer=tf.keras.initializers.Zeros(),\n",
        "                                    dtype=tf.float32,\n",
        "                                    name=\"beta_embed_layer\",\n",
        "                                    trainable=True)\n",
        "        self.gamma = self.add_weight(shape=(1, 1, 1, 1),\n",
        "                                     initializer=tf.keras.initializers.Ones(),\n",
        "                                     dtype=tf.float32,\n",
        "                                     name=\"gamma_embed_layer\",\n",
        "                                     trainable=True)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "\n",
        "        mean, variance = tf.nn.moments(inputs, -1, keepdims=True)\n",
        "        outputs = tf.nn.batch_normalization(\n",
        "            inputs,\n",
        "            mean,\n",
        "            variance,\n",
        "            offset=self.beta,\n",
        "            scale=self.gamma,\n",
        "            variance_epsilon=1e-8,\n",
        "        )\n",
        "\n",
        "        return outputs #* self.gamma + self.beta\n",
        "\n",
        "@tf.function\n",
        "def embedding_normalization(logits):\n",
        "    # normalement vont avoir taille (batch_size, 64, 128, embedding_size)\n",
        "    # axis=-1 is embedding normalement\n",
        "    return (logits / tf.norm(logits, axis=-1, keepdims=True)) * tf.constant(np.sqrt(logits.shape[-1]), dtype=tf.float32)\n",
        "\n",
        "class NormalizedEmbedding(tf.keras.Model):\n",
        "    \"\"\"\"\"\"  \n",
        "    def __init__(self, categories_nb, img_embed_size):\n",
        "        super().__init__()\n",
        "        self.embed_layer = tf.keras.layers.Embedding(categories_nb, img_embed_size)\n",
        "        self.embed_layer2 = layers.Conv2D(img_embed_size, kernel_size=1, padding=\"same\", activation=keras.activations.swish)\n",
        "        self.embed_layer3 = layers.Conv2D(img_embed_size, kernel_size=1, padding=\"same\", activation=keras.activations.swish)\n",
        "        self.embed_layer4 = layers.Conv2D(img_embed_size, kernel_size=1, activation=None)\n",
        "        self.layer_norm = EmbedLayerNormalization(img_embed_size=16)\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        # I tested the embedding it's perfectly pixel by pixel\n",
        "        y = self.embed_layer(x)\n",
        "        y = self.embed_layer2(y)\n",
        "        y = self.embed_layer3(y)\n",
        "        y = self.embed_layer4(y)\n",
        "        #y = embedding_normalization(y)\n",
        "        y = self.layer_norm(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "Ej4nARwoGmme"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network(\n",
        "    image_size,\n",
        "    noise_embedding_max_frequency,\n",
        "    noise_embedding_dims,\n",
        "    image_embedding_dims,\n",
        "    block_depth,\n",
        "    widths,\n",
        "    attentions,\n",
        "    patch_size,\n",
        "    embed_size\n",
        "):\n",
        "    def EmbeddingLayer(embedding_max_frequency, embedding_dims):\n",
        "        def sinusoidal_embedding(x):\n",
        "            embedding_min_frequency = 1.0\n",
        "            frequencies = tf.exp(\n",
        "                tf.linspace(\n",
        "                    tf.math.log(embedding_min_frequency),\n",
        "                    tf.math.log(embedding_max_frequency),\n",
        "                    embedding_dims // 2,\n",
        "                )\n",
        "            )\n",
        "            angular_speeds = 2.0 * math.pi * frequencies\n",
        "            embeddings = tf.concat(\n",
        "                [\n",
        "                    tf.sin(angular_speeds * x),\n",
        "                    tf.cos(angular_speeds * x),\n",
        "                ],\n",
        "                axis=-1,\n",
        "            )\n",
        "            return embeddings\n",
        "\n",
        "        def forward(x):\n",
        "            x = layers.Lambda(sinusoidal_embedding)(x)\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def ResidualBlock(width, attention):\n",
        "        def forward(x):\n",
        "            x, n = x\n",
        "            input_width = x.shape[3]\n",
        "            if input_width == width:\n",
        "                residual = x\n",
        "            else:\n",
        "                residual = layers.Conv2D(width, kernel_size=1)(x)\n",
        "\n",
        "            n = layers.Dense(width)(n)\n",
        "\n",
        "            x = tfa.layers.GroupNormalization(groups=8)(x)\n",
        "            x = keras.activations.swish(x)\n",
        "            x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "\n",
        "            x = layers.Add()([x, n])\n",
        "\n",
        "            x = tfa.layers.GroupNormalization(groups=8)(x)\n",
        "            x = keras.activations.swish(x)\n",
        "            x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "\n",
        "            x = layers.Add()([residual, x])\n",
        "\n",
        "            if attention:\n",
        "                residual = x\n",
        "                x = tfa.layers.GroupNormalization(groups=8, center=False, scale=False)(\n",
        "                    x\n",
        "                )\n",
        "                x = layers.MultiHeadAttention(\n",
        "                    num_heads=4, key_dim=width, attention_axes=(1, 2)\n",
        "                )(x, x)\n",
        "\n",
        "                x = layers.Add()([residual, x])\n",
        "\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def DownBlock(block_depth, width, attention):\n",
        "        def forward(x):\n",
        "            x, n, skips = x\n",
        "            for _ in range(block_depth):\n",
        "                x = ResidualBlock(width, attention)([x, n])\n",
        "                skips.append(x)\n",
        "            x = layers.AveragePooling2D(pool_size=2)(x)\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def UpBlock(block_depth, width, attention):\n",
        "        def forward(x):\n",
        "            x, n, skips = x\n",
        "            x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
        "            for _ in range(block_depth):\n",
        "                x = layers.Concatenate()([x, skips.pop()])\n",
        "                x = ResidualBlock(width, attention)([x, n])\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    images = keras.Input(shape=(None, None, embed_size))\n",
        "    noise_powers = keras.Input(shape=(1, 1, 1))\n",
        "    mask = keras.Input(shape=(None, None, 1))\n",
        "    conditioning_pixels = keras.Input(shape=(None, None, embed_size))\n",
        "\n",
        "    x = tf.keras.layers.Concatenate(axis=-1)([images, mask, conditioning_pixels])\n",
        "\n",
        "    x = layers.Conv2D(image_embedding_dims, kernel_size=patch_size, strides=patch_size)(\n",
        "        x\n",
        "    )\n",
        "\n",
        "    # NOISE EMBEDDING\n",
        "    #print(noise_powers)\n",
        "    n = EmbeddingLayer(noise_embedding_max_frequency, noise_embedding_dims)(\n",
        "        noise_powers\n",
        "    )\n",
        "    n = layers.Dense(noise_embedding_dims, activation=keras.activations.swish)(n)\n",
        "    n = layers.Dense(noise_embedding_dims, activation=keras.activations.swish)(n)\n",
        "\n",
        "    skips = []\n",
        "    for width, attention in zip(widths[:-1], attentions[:-1]):\n",
        "        x = DownBlock(block_depth, width, attention)([x, n, skips])\n",
        "\n",
        "    for _ in range(block_depth):\n",
        "        x = ResidualBlock(widths[-1], attentions[-1])([x, n])\n",
        "\n",
        "    for width, attention in zip(widths[-2::-1], attentions[-2::-1]):\n",
        "        x = UpBlock(block_depth, width, attention)([x, n, skips])\n",
        "\n",
        "    x = layers.Conv2DTranspose(\n",
        "        4, kernel_size=patch_size, strides=patch_size, kernel_initializer=\"zeros\", activation=\"softmax\"\n",
        "    )(x)\n",
        "\n",
        "    return keras.Model([images, noise_powers, mask, conditioning_pixels], x, name=\"residual_unet\")"
      ],
      "metadata": {
        "id": "IgeH9voVvsN5"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sampling\n",
        "def make_mask(image_to_condition):\n",
        "    nb_conditioning_points = np.random.randint(low=1.0, high=image_to_condition.shape[0] * image_to_condition.shape[1])\n",
        "    random_x_coordinates = np.random.choice(image_to_condition.shape[0], nb_conditioning_points)\n",
        "    random_y_coordinates = np.random.choice(image_to_condition.shape[1], nb_conditioning_points)\n",
        "    mask = np.zeros((image_to_condition.shape[0], image_to_condition.shape[1], 1))\n",
        "    mask[random_x_coordinates, random_y_coordinates, :] = 1\n",
        "    return mask"
      ],
      "metadata": {
        "id": "XEMZlA-xdLcT"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "lZ37576YD0Y5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DiffusionModel(keras.Model):\n",
        "    def __init__(self, image_size, widths, block_depth, img_embed_size,\n",
        "                 categories_nb, embedding_lr=1e-3, batch_size=30,\n",
        "                 large_model=False):\n",
        "        super().__init__()\n",
        "\n",
        "        noise_embedding_max_frequency = 1000.0\n",
        "        noise_embedding_dims = 64\n",
        "        image_embedding_dims = 64\n",
        "        block_depth = 2\n",
        "\n",
        "        if large_model:\n",
        "            widths = [64, 128, 256, 512]\n",
        "            attentions = [False, False, True, True]\n",
        "        else:\n",
        "            widths = [64, 96, 128, 256]\n",
        "            attentions = [False, False, False, False]\n",
        "            \n",
        "        patch_size = 1\n",
        "\n",
        "        self.diffusion_schedule = SignalStepLinearSchedule(start_log_snr=3.0, end_log_snr=-10.0,)\n",
        "        #self.diffusion_schedule = CosineSchedule(start_log_snr=3.0, end_log_snr=-10.0,)\n",
        "        self.network = get_network(image_size, noise_embedding_max_frequency,\n",
        "                                   noise_embedding_dims, image_embedding_dims,\n",
        "                                   block_depth, widths, attentions, patch_size,\n",
        "                                   img_embed_size)\n",
        "        \n",
        "        self.ema_network = keras.models.clone_model(self.network)\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Embedding\n",
        "        self.img_embed_size = img_embed_size\n",
        "        self.embedding_layer = NormalizedEmbedding(categories_nb, img_embed_size)\n",
        "        self.emb_optimiser = tf.keras.optimizers.legacy.Adam(learning_rate=embedding_lr)\n",
        "\n",
        "    def compile(self, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.image_loss_tracker]\n",
        "\n",
        "    def denoise(self, noisy_images, noise_rates, signal_rates, training, mask=None, pixels=None):\n",
        "        # the exponential moving average weights are used at evaluation\n",
        "\n",
        "        if mask is None or pixels is None:\n",
        "            mask = tf.zeros(noisy_images.shape)\n",
        "            pixels = tf.zeros(noisy_images.shape)\n",
        "\n",
        "        if training:\n",
        "            network = self.network\n",
        "        else:\n",
        "            network = self.ema_network\n",
        "\n",
        "        # predict noise component and calculate the image component using it\n",
        "        pred_images = network([noisy_images, noise_rates**2, mask, pixels], training=training)\n",
        "\n",
        "        \n",
        "        int_encoded_img = tf.argmax(pred_images, axis=-1)\n",
        "        embed_pred_images = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "        pred_noises = (noisy_images - signal_rates * embed_pred_images) / noise_rates\n",
        "\n",
        "        return pred_images, pred_noises\n",
        "\n",
        "    def train_step(self, images):\n",
        "        # normalize images to have standard deviation of 1, like the noises\n",
        "        noises = tf.random.normal(shape=(self.batch_size, self.image_size[0], self.image_size[1], self.img_embed_size))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = tf.random.uniform(\n",
        "            shape=(self.batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        mask_uncondi = tf.zeros((self.batch_size // 2, images.shape[1], images.shape[2], 1))\n",
        "        mask_condi = tf.map_fn(make_mask, images[self.batch_size // 2:])\n",
        "        mask = tf.concat([mask_uncondi, mask_condi], axis=0)\n",
        "\n",
        "\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        # mix the images with noises accordingly\n",
        "        int_encoded_img = tf.argmax(images, axis=-1)\n",
        "\n",
        "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "            embed_images = self.embedding_layer(int_encoded_img)\n",
        "            pixels = tf.math.multiply(embed_images, mask)\n",
        "\n",
        "            noisy_images = signal_rates * embed_images + noise_rates * noises\n",
        "            noisy_images = tf.math.multiply(noisy_images, tf.math.abs(mask - 1))\n",
        "\n",
        "            # train the network to separate noisy images to their components\n",
        "            pred_images, pred_noise = self.denoise(\n",
        "                noisy_images, noise_rates, signal_rates, training=True, mask=mask, pixels=pixels\n",
        "            )\n",
        "\n",
        "            image_loss = self.loss(images, pred_images)  # training loss\n",
        "            \n",
        "\n",
        "        gradients_model = tape1.gradient(image_loss, self.network.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients_model, self.network.trainable_weights))\n",
        "\n",
        "        gradients_embeddings = tape2.gradient(image_loss, self.embedding_layer.trainable_weights)\n",
        "        self.emb_optimiser.apply_gradients(zip(gradients_embeddings, self.embedding_layer.trainable_weights))\n",
        "\n",
        "        self.image_loss_tracker.update_state(image_loss)\n",
        "\n",
        "        # track the exponential moving averages of weights\n",
        "        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
        "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
        "\n",
        "        # KID is not measured during the training phase for computational efficiency\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, images):\n",
        "        noises = tf.random.normal(shape=(self.batch_size, self.image_size[0], self.image_size[1], self.img_embed_size))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = tf.random.uniform(\n",
        "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "\n",
        "        mask_uncondi = tf.zeros((self.batch_size // 2, images.shape[1], images.shape[2], 1))\n",
        "        mask_condi = tf.map_fn(make_mask, images[self.batch_size // 2:])\n",
        "        mask = tf.concat([mask_uncondi, mask_condi], axis=0)\n",
        "\n",
        "        int_encoded_img = tf.argmax(images, axis=-1)\n",
        "\n",
        "        embed_images = self.embedding_layer(int_encoded_img)\n",
        "\n",
        "        #std = marginal_prob_std(diffusion_times, sigma=sigma)\n",
        "        pixels = tf.math.multiply(embed_images, mask)\n",
        "\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        noisy_images = signal_rates * embed_images + noise_rates * noises\n",
        "        noisy_images = tf.math.multiply(noisy_images, tf.math.abs(mask - 1))\n",
        "        #noisy_images = embed_images + noises * tf.reshape(std, (-1, 1, 1, 1))\n",
        "\n",
        "        # use the network to separate noisy images to their components\n",
        "        pred_images, pred_noise = self.denoise(\n",
        "            noisy_images, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels\n",
        "        )\n",
        "\n",
        "        image_loss = self.loss(images, pred_images)\n",
        "\n",
        "        self.image_loss_tracker.update_state(image_loss)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqYrG2VPD0Y7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "LHcHZit9D0Y8"
      },
      "outputs": [],
      "source": [
        "# create and compile the model\n",
        "model = DiffusionModel(image_size, widths, block_depth, img_embed_size=img_embed_size, categories_nb=categories_nb, large_model=True)\n",
        "\n",
        "learning_rate = 1e-4\n",
        "t_epochs_nb=1\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    loss= tf.keras.losses.CategoricalCrossentropy(),\n",
        ")\n",
        "\n",
        "# run training and plot generated images periodically"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, batch_size=batch_size, epochs=t_epochs_nb, validation_data=(x_test,))"
      ],
      "metadata": {
        "id": "RDSktS3wIy4C",
        "outputId": "a73689d9-0b0f-4d73-daf1-c5aa19bf9ac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92/92 [==============================] - 180s 1s/step - i_loss: 0.5995 - val_i_loss: 1.3849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.network.summary()"
      ],
      "metadata": {
        "id": "Jcm7Vh6v9R0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10473cc2-77bc-45bf-981f-7da8521d8665"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"residual_unet\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_21 (InputLayer)          [(None, None, None,  0           []                               \n",
            "                                 64)]                                                             \n",
            "                                                                                                  \n",
            " input_23 (InputLayer)          [(None, None, None,  0           []                               \n",
            "                                 1)]                                                              \n",
            "                                                                                                  \n",
            " input_24 (InputLayer)          [(None, None, None,  0           []                               \n",
            "                                 64)]                                                             \n",
            "                                                                                                  \n",
            " concatenate_35 (Concatenate)   (None, None, None,   0           ['input_21[0][0]',               \n",
            "                                129)                              'input_23[0][0]',               \n",
            "                                                                  'input_24[0][0]']               \n",
            "                                                                                                  \n",
            " input_22 (InputLayer)          [(None, 1, 1, 1)]    0           []                               \n",
            "                                                                                                  \n",
            " conv2d_205 (Conv2D)            (None, None, None,   8320        ['concatenate_35[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 64)     0           ['input_22[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_170 (Group  (None, None, None,   128        ['conv2d_205[0][0]']             \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " dense_80 (Dense)               (None, 1, 1, 64)     4160        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " tf.nn.silu_140 (TFOpLambda)    (None, None, None,   0           ['group_normalization_170[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " dense_81 (Dense)               (None, 1, 1, 64)     4160        ['dense_80[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_206 (Conv2D)            (None, None, None,   36928       ['tf.nn.silu_140[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " dense_82 (Dense)               (None, 1, 1, 64)     4160        ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_170 (Add)                  (None, None, None,   0           ['conv2d_206[0][0]',             \n",
            "                                64)                               'dense_82[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_171 (Group  (None, None, None,   128        ['add_170[0][0]']                \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " tf.nn.silu_141 (TFOpLambda)    (None, None, None,   0           ['group_normalization_171[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_207 (Conv2D)            (None, None, None,   36928       ['tf.nn.silu_141[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " add_171 (Add)                  (None, None, None,   0           ['conv2d_205[0][0]',             \n",
            "                                64)                               'conv2d_207[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_172 (Group  (None, None, None,   128        ['add_171[0][0]']                \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " tf.nn.silu_142 (TFOpLambda)    (None, None, None,   0           ['group_normalization_172[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_208 (Conv2D)            (None, None, None,   36928       ['tf.nn.silu_142[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " dense_83 (Dense)               (None, 1, 1, 64)     4160        ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_172 (Add)                  (None, None, None,   0           ['conv2d_208[0][0]',             \n",
            "                                64)                               'dense_83[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_173 (Group  (None, None, None,   128        ['add_172[0][0]']                \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " tf.nn.silu_143 (TFOpLambda)    (None, None, None,   0           ['group_normalization_173[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_209 (Conv2D)            (None, None, None,   36928       ['tf.nn.silu_143[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " add_173 (Add)                  (None, None, None,   0           ['add_171[0][0]',                \n",
            "                                64)                               'conv2d_209[0][0]']             \n",
            "                                                                                                  \n",
            " average_pooling2d_15 (AverageP  (None, None, None,   0          ['add_173[0][0]']                \n",
            " ooling2D)                      64)                                                               \n",
            "                                                                                                  \n",
            " group_normalization_174 (Group  (None, None, None,   128        ['average_pooling2d_15[0][0]']   \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " tf.nn.silu_144 (TFOpLambda)    (None, None, None,   0           ['group_normalization_174[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_211 (Conv2D)            (None, None, None,   73856       ['tf.nn.silu_144[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " dense_84 (Dense)               (None, 1, 1, 128)    8320        ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_174 (Add)                  (None, None, None,   0           ['conv2d_211[0][0]',             \n",
            "                                128)                              'dense_84[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_175 (Group  (None, None, None,   256        ['add_174[0][0]']                \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_145 (TFOpLambda)    (None, None, None,   0           ['group_normalization_175[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_210 (Conv2D)            (None, None, None,   8320        ['average_pooling2d_15[0][0]']   \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_212 (Conv2D)            (None, None, None,   147584      ['tf.nn.silu_145[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " add_175 (Add)                  (None, None, None,   0           ['conv2d_210[0][0]',             \n",
            "                                128)                              'conv2d_212[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_176 (Group  (None, None, None,   256        ['add_175[0][0]']                \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_146 (TFOpLambda)    (None, None, None,   0           ['group_normalization_176[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_213 (Conv2D)            (None, None, None,   147584      ['tf.nn.silu_146[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " dense_85 (Dense)               (None, 1, 1, 128)    8320        ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_176 (Add)                  (None, None, None,   0           ['conv2d_213[0][0]',             \n",
            "                                128)                              'dense_85[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_177 (Group  (None, None, None,   256        ['add_176[0][0]']                \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_147 (TFOpLambda)    (None, None, None,   0           ['group_normalization_177[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_214 (Conv2D)            (None, None, None,   147584      ['tf.nn.silu_147[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " add_177 (Add)                  (None, None, None,   0           ['add_175[0][0]',                \n",
            "                                128)                              'conv2d_214[0][0]']             \n",
            "                                                                                                  \n",
            " average_pooling2d_16 (AverageP  (None, None, None,   0          ['add_177[0][0]']                \n",
            " ooling2D)                      128)                                                              \n",
            "                                                                                                  \n",
            " group_normalization_178 (Group  (None, None, None,   256        ['average_pooling2d_16[0][0]']   \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_148 (TFOpLambda)    (None, None, None,   0           ['group_normalization_178[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_216 (Conv2D)            (None, None, None,   295168      ['tf.nn.silu_148[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " dense_86 (Dense)               (None, 1, 1, 256)    16640       ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_178 (Add)                  (None, None, None,   0           ['conv2d_216[0][0]',             \n",
            "                                256)                              'dense_86[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_179 (Group  (None, None, None,   512        ['add_178[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_149 (TFOpLambda)    (None, None, None,   0           ['group_normalization_179[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_215 (Conv2D)            (None, None, None,   33024       ['average_pooling2d_16[0][0]']   \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_217 (Conv2D)            (None, None, None,   590080      ['tf.nn.silu_149[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " add_179 (Add)                  (None, None, None,   0           ['conv2d_215[0][0]',             \n",
            "                                256)                              'conv2d_217[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_180 (Group  (None, None, None,   0          ['add_179[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention_30 (Multi  (None, None, None,   1051904    ['group_normalization_180[0][0]',\n",
            " HeadAttention)                 256)                              'group_normalization_180[0][0]']\n",
            "                                                                                                  \n",
            " add_180 (Add)                  (None, None, None,   0           ['add_179[0][0]',                \n",
            "                                256)                              'multi_head_attention_30[0][0]']\n",
            "                                                                                                  \n",
            " group_normalization_181 (Group  (None, None, None,   512        ['add_180[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_150 (TFOpLambda)    (None, None, None,   0           ['group_normalization_181[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_218 (Conv2D)            (None, None, None,   590080      ['tf.nn.silu_150[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " dense_87 (Dense)               (None, 1, 1, 256)    16640       ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_181 (Add)                  (None, None, None,   0           ['conv2d_218[0][0]',             \n",
            "                                256)                              'dense_87[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_182 (Group  (None, None, None,   512        ['add_181[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_151 (TFOpLambda)    (None, None, None,   0           ['group_normalization_182[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_219 (Conv2D)            (None, None, None,   590080      ['tf.nn.silu_151[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " add_182 (Add)                  (None, None, None,   0           ['add_180[0][0]',                \n",
            "                                256)                              'conv2d_219[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_183 (Group  (None, None, None,   0          ['add_182[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention_31 (Multi  (None, None, None,   1051904    ['group_normalization_183[0][0]',\n",
            " HeadAttention)                 256)                              'group_normalization_183[0][0]']\n",
            "                                                                                                  \n",
            " add_183 (Add)                  (None, None, None,   0           ['add_182[0][0]',                \n",
            "                                256)                              'multi_head_attention_31[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_17 (AverageP  (None, None, None,   0          ['add_183[0][0]']                \n",
            " ooling2D)                      256)                                                              \n",
            "                                                                                                  \n",
            " group_normalization_184 (Group  (None, None, None,   512        ['average_pooling2d_17[0][0]']   \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_152 (TFOpLambda)    (None, None, None,   0           ['group_normalization_184[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_221 (Conv2D)            (None, None, None,   1180160     ['tf.nn.silu_152[0][0]']         \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " dense_88 (Dense)               (None, 1, 1, 512)    33280       ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_184 (Add)                  (None, None, None,   0           ['conv2d_221[0][0]',             \n",
            "                                512)                              'dense_88[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_185 (Group  (None, None, None,   1024       ['add_184[0][0]']                \n",
            " Normalization)                 512)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_153 (TFOpLambda)    (None, None, None,   0           ['group_normalization_185[0][0]']\n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " conv2d_220 (Conv2D)            (None, None, None,   131584      ['average_pooling2d_17[0][0]']   \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " conv2d_222 (Conv2D)            (None, None, None,   2359808     ['tf.nn.silu_153[0][0]']         \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " add_185 (Add)                  (None, None, None,   0           ['conv2d_220[0][0]',             \n",
            "                                512)                              'conv2d_222[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_186 (Group  (None, None, None,   0          ['add_185[0][0]']                \n",
            " Normalization)                 512)                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention_32 (Multi  (None, None, None,   4200960    ['group_normalization_186[0][0]',\n",
            " HeadAttention)                 512)                              'group_normalization_186[0][0]']\n",
            "                                                                                                  \n",
            " add_186 (Add)                  (None, None, None,   0           ['add_185[0][0]',                \n",
            "                                512)                              'multi_head_attention_32[0][0]']\n",
            "                                                                                                  \n",
            " group_normalization_187 (Group  (None, None, None,   1024       ['add_186[0][0]']                \n",
            " Normalization)                 512)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_154 (TFOpLambda)    (None, None, None,   0           ['group_normalization_187[0][0]']\n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " conv2d_223 (Conv2D)            (None, None, None,   2359808     ['tf.nn.silu_154[0][0]']         \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " dense_89 (Dense)               (None, 1, 1, 512)    33280       ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_187 (Add)                  (None, None, None,   0           ['conv2d_223[0][0]',             \n",
            "                                512)                              'dense_89[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_188 (Group  (None, None, None,   1024       ['add_187[0][0]']                \n",
            " Normalization)                 512)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_155 (TFOpLambda)    (None, None, None,   0           ['group_normalization_188[0][0]']\n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " conv2d_224 (Conv2D)            (None, None, None,   2359808     ['tf.nn.silu_155[0][0]']         \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " add_188 (Add)                  (None, None, None,   0           ['add_186[0][0]',                \n",
            "                                512)                              'conv2d_224[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_189 (Group  (None, None, None,   0          ['add_188[0][0]']                \n",
            " Normalization)                 512)                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention_33 (Multi  (None, None, None,   4200960    ['group_normalization_189[0][0]',\n",
            " HeadAttention)                 512)                              'group_normalization_189[0][0]']\n",
            "                                                                                                  \n",
            " add_189 (Add)                  (None, None, None,   0           ['add_188[0][0]',                \n",
            "                                512)                              'multi_head_attention_33[0][0]']\n",
            "                                                                                                  \n",
            " up_sampling2d_15 (UpSampling2D  (None, None, None,   0          ['add_189[0][0]']                \n",
            " )                              512)                                                              \n",
            "                                                                                                  \n",
            " concatenate_36 (Concatenate)   (None, None, None,   0           ['up_sampling2d_15[0][0]',       \n",
            "                                768)                              'add_183[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization_190 (Group  (None, None, None,   1536       ['concatenate_36[0][0]']         \n",
            " Normalization)                 768)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_156 (TFOpLambda)    (None, None, None,   0           ['group_normalization_190[0][0]']\n",
            "                                768)                                                              \n",
            "                                                                                                  \n",
            " conv2d_226 (Conv2D)            (None, None, None,   1769728     ['tf.nn.silu_156[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " dense_90 (Dense)               (None, 1, 1, 256)    16640       ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_190 (Add)                  (None, None, None,   0           ['conv2d_226[0][0]',             \n",
            "                                256)                              'dense_90[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_191 (Group  (None, None, None,   512        ['add_190[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_157 (TFOpLambda)    (None, None, None,   0           ['group_normalization_191[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_225 (Conv2D)            (None, None, None,   196864      ['concatenate_36[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_227 (Conv2D)            (None, None, None,   590080      ['tf.nn.silu_157[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " add_191 (Add)                  (None, None, None,   0           ['conv2d_225[0][0]',             \n",
            "                                256)                              'conv2d_227[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_192 (Group  (None, None, None,   0          ['add_191[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention_34 (Multi  (None, None, None,   1051904    ['group_normalization_192[0][0]',\n",
            " HeadAttention)                 256)                              'group_normalization_192[0][0]']\n",
            "                                                                                                  \n",
            " add_192 (Add)                  (None, None, None,   0           ['add_191[0][0]',                \n",
            "                                256)                              'multi_head_attention_34[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_37 (Concatenate)   (None, None, None,   0           ['add_192[0][0]',                \n",
            "                                512)                              'add_180[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization_193 (Group  (None, None, None,   1024       ['concatenate_37[0][0]']         \n",
            " Normalization)                 512)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_158 (TFOpLambda)    (None, None, None,   0           ['group_normalization_193[0][0]']\n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " conv2d_229 (Conv2D)            (None, None, None,   1179904     ['tf.nn.silu_158[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " dense_91 (Dense)               (None, 1, 1, 256)    16640       ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_193 (Add)                  (None, None, None,   0           ['conv2d_229[0][0]',             \n",
            "                                256)                              'dense_91[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_194 (Group  (None, None, None,   512        ['add_193[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_159 (TFOpLambda)    (None, None, None,   0           ['group_normalization_194[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_228 (Conv2D)            (None, None, None,   131328      ['concatenate_37[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_230 (Conv2D)            (None, None, None,   590080      ['tf.nn.silu_159[0][0]']         \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " add_194 (Add)                  (None, None, None,   0           ['conv2d_228[0][0]',             \n",
            "                                256)                              'conv2d_230[0][0]']             \n",
            "                                                                                                  \n",
            " group_normalization_195 (Group  (None, None, None,   0          ['add_194[0][0]']                \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention_35 (Multi  (None, None, None,   1051904    ['group_normalization_195[0][0]',\n",
            " HeadAttention)                 256)                              'group_normalization_195[0][0]']\n",
            "                                                                                                  \n",
            " add_195 (Add)                  (None, None, None,   0           ['add_194[0][0]',                \n",
            "                                256)                              'multi_head_attention_35[0][0]']\n",
            "                                                                                                  \n",
            " up_sampling2d_16 (UpSampling2D  (None, None, None,   0          ['add_195[0][0]']                \n",
            " )                              256)                                                              \n",
            "                                                                                                  \n",
            " concatenate_38 (Concatenate)   (None, None, None,   0           ['up_sampling2d_16[0][0]',       \n",
            "                                384)                              'add_177[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization_196 (Group  (None, None, None,   768        ['concatenate_38[0][0]']         \n",
            " Normalization)                 384)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_160 (TFOpLambda)    (None, None, None,   0           ['group_normalization_196[0][0]']\n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_232 (Conv2D)            (None, None, None,   442496      ['tf.nn.silu_160[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " dense_92 (Dense)               (None, 1, 1, 128)    8320        ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_196 (Add)                  (None, None, None,   0           ['conv2d_232[0][0]',             \n",
            "                                128)                              'dense_92[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_197 (Group  (None, None, None,   256        ['add_196[0][0]']                \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_161 (TFOpLambda)    (None, None, None,   0           ['group_normalization_197[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_231 (Conv2D)            (None, None, None,   49280       ['concatenate_38[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_233 (Conv2D)            (None, None, None,   147584      ['tf.nn.silu_161[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " add_197 (Add)                  (None, None, None,   0           ['conv2d_231[0][0]',             \n",
            "                                128)                              'conv2d_233[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_39 (Concatenate)   (None, None, None,   0           ['add_197[0][0]',                \n",
            "                                256)                              'add_175[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization_198 (Group  (None, None, None,   512        ['concatenate_39[0][0]']         \n",
            " Normalization)                 256)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_162 (TFOpLambda)    (None, None, None,   0           ['group_normalization_198[0][0]']\n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_235 (Conv2D)            (None, None, None,   295040      ['tf.nn.silu_162[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " dense_93 (Dense)               (None, 1, 1, 128)    8320        ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_198 (Add)                  (None, None, None,   0           ['conv2d_235[0][0]',             \n",
            "                                128)                              'dense_93[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_199 (Group  (None, None, None,   256        ['add_198[0][0]']                \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_163 (TFOpLambda)    (None, None, None,   0           ['group_normalization_199[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_234 (Conv2D)            (None, None, None,   32896       ['concatenate_39[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_236 (Conv2D)            (None, None, None,   147584      ['tf.nn.silu_163[0][0]']         \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " add_199 (Add)                  (None, None, None,   0           ['conv2d_234[0][0]',             \n",
            "                                128)                              'conv2d_236[0][0]']             \n",
            "                                                                                                  \n",
            " up_sampling2d_17 (UpSampling2D  (None, None, None,   0          ['add_199[0][0]']                \n",
            " )                              128)                                                              \n",
            "                                                                                                  \n",
            " concatenate_40 (Concatenate)   (None, None, None,   0           ['up_sampling2d_17[0][0]',       \n",
            "                                192)                              'add_173[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization_200 (Group  (None, None, None,   384        ['concatenate_40[0][0]']         \n",
            " Normalization)                 192)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_164 (TFOpLambda)    (None, None, None,   0           ['group_normalization_200[0][0]']\n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_238 (Conv2D)            (None, None, None,   110656      ['tf.nn.silu_164[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " dense_94 (Dense)               (None, 1, 1, 64)     4160        ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_200 (Add)                  (None, None, None,   0           ['conv2d_238[0][0]',             \n",
            "                                64)                               'dense_94[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_201 (Group  (None, None, None,   128        ['add_200[0][0]']                \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " tf.nn.silu_165 (TFOpLambda)    (None, None, None,   0           ['group_normalization_201[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_237 (Conv2D)            (None, None, None,   12352       ['concatenate_40[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_239 (Conv2D)            (None, None, None,   36928       ['tf.nn.silu_165[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " add_201 (Add)                  (None, None, None,   0           ['conv2d_237[0][0]',             \n",
            "                                64)                               'conv2d_239[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_41 (Concatenate)   (None, None, None,   0           ['add_201[0][0]',                \n",
            "                                128)                              'add_171[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization_202 (Group  (None, None, None,   256        ['concatenate_41[0][0]']         \n",
            " Normalization)                 128)                                                              \n",
            "                                                                                                  \n",
            " tf.nn.silu_166 (TFOpLambda)    (None, None, None,   0           ['group_normalization_202[0][0]']\n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_241 (Conv2D)            (None, None, None,   73792       ['tf.nn.silu_166[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " dense_95 (Dense)               (None, 1, 1, 64)     4160        ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " add_202 (Add)                  (None, None, None,   0           ['conv2d_241[0][0]',             \n",
            "                                64)                               'dense_95[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_203 (Group  (None, None, None,   128        ['add_202[0][0]']                \n",
            " Normalization)                 64)                                                               \n",
            "                                                                                                  \n",
            " tf.nn.silu_167 (TFOpLambda)    (None, None, None,   0           ['group_normalization_203[0][0]']\n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_240 (Conv2D)            (None, None, None,   8256        ['concatenate_41[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_242 (Conv2D)            (None, None, None,   36928       ['tf.nn.silu_167[0][0]']         \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " add_203 (Add)                  (None, None, None,   0           ['conv2d_240[0][0]',             \n",
            "                                64)                               'conv2d_242[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_transpose_5 (Conv2DTran  (None, None, None,   260        ['add_203[0][0]']                \n",
            " spose)                         4)                                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,836,548\n",
            "Trainable params: 29,836,548\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_axis = np.arange(t_epochs_nb)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x_axis, history.history[\"i_loss\"], label=\"Training image CE loss\")\n",
        "plt.plot(x_axis, history.history[\"val_i_loss\"], label=\"Testing image CE loss\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efA4YUlX723d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "93035be0-ca33-49b9-c8e0-1949b0baad99"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fky6ewS3D0Y-"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm"
      ],
      "metadata": {
        "id": "PoJyZzMZqAIZ"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def second_order_correction(\n",
        "    model,\n",
        "    diffusion_times,\n",
        "    step_size,\n",
        "    noisy_images,\n",
        "    signal_rates,\n",
        "    noise_rates,\n",
        "    pred_images,\n",
        "    pred_noises,\n",
        "    second_order_alpha,\n",
        "    mask,\n",
        "    pixels,\n",
        "):\n",
        "    # generic second-order Runge-Kutta method\n",
        "    # https://en.wikipedia.org/wiki/List_of_Runge%E2%80%93Kutta_methods#Generic_second-order_method\n",
        "    # based on https://arxiv.org/abs/2206.00364\n",
        "    #batch_size=noisy_images.shape[0]\n",
        "    #mask = tf.zeros((batch_size, image_size[0], image_size[1], 1))\n",
        "    #pixels = tf.zeros((batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "\n",
        "    # use first estimate to sample alpha steps away\n",
        "    alpha_signal_rates, alpha_noise_rates = model.diffusion_schedule(\n",
        "        diffusion_times - second_order_alpha * step_size\n",
        "    )\n",
        "    alpha_noisy_images = (\n",
        "        alpha_signal_rates * pred_images + alpha_noise_rates * pred_noises\n",
        "    )\n",
        "    x_input = tf.math.multiply(noisy_images, tf.math.abs(mask - 1))\n",
        "    pred_x0, alpha_pred_noises = model.denoise(x_input, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "    int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "    embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "    # linearly combine the two noise estimates\n",
        "    pred_noises = (1.0 - 1.0 / (2.0 * second_order_alpha)) * pred_noises + 1.0 / (\n",
        "        2.0 * second_order_alpha\n",
        "        ) * alpha_pred_noises\n",
        "\n",
        "    pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
        "    return pred_images, pred_noises"
      ],
      "metadata": {
        "id": "SEJciKNQzCGs"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_beta(curr_alphas, prev_alphas):\n",
        "\n",
        "    betas = 1 - (prev_alphas / curr_alphas)\n",
        "    return betas"
      ],
      "metadata": {
        "id": "wVWp820FKVRV"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import xla\n",
        "def ddpm_sampler(model, img_embed_size, image_size, batch_size=10, num_steps=10, eps=1e-3, mask=None, pixels=None):\n",
        "    second_order_alpha = 1.1\n",
        "    # T and schedule\n",
        "    t_max = 1.0\n",
        "    t = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32)\n",
        "    noise_rates, signal_rates = model.diffusion_schedule(t)\n",
        "\n",
        "    if mask is None:\n",
        "        mask = tf.zeros((batch_size, image_size[0], image_size[1], 1), dtype=tf.float32)\n",
        "        pixels = tf.zeros((batch_size, image_size[0], image_size[1], img_embed_size), dtype=tf.float32)\n",
        "    else:\n",
        "        \n",
        "        pixels = tf.argmax(pixels, axis=-1)\n",
        "        pixels = model.embedding_layer(pixels)\n",
        "        pixels = tf.math.multiply(pixels, mask)\n",
        "\n",
        "        mask = tf.repeat(mask, batch_size, axis=0)\n",
        "        mask = tf.cast(mask,  dtype=tf.float32)\n",
        "        pixels = tf.repeat(pixels, batch_size, axis=0)\n",
        "\n",
        "    # Sample noise\n",
        "    uniform_init_x = tf.random.uniform((batch_size, image_size[0], image_size[1]), 0, 4, dtype=tf.dtypes.int32)\n",
        "    noises = tf.random.normal(shape=(batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "    init_x = signal_rates * model.embedding_layer(uniform_init_x) + noise_rates * noises\n",
        "\n",
        "    # Keep track of the chain\n",
        "    samples_list = []\n",
        "    samples_list.append( tf.keras.backend.constant(keras.utils.to_categorical(uniform_init_x)))\n",
        "\n",
        "    # Steps and other algorithmic variables\n",
        "    time_steps = tf.linspace(1., eps, num_steps)\n",
        "    step_size = time_steps[0] - time_steps[1]\n",
        "    x = init_x\n",
        "    prev_alphas = signal_rates**2\n",
        "\n",
        "    # INFERENCE REVERSE LOOP\n",
        "    for time_step in tqdm.tqdm(time_steps):\n",
        "        #print(time_step)\n",
        "        batch_time_step = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32) * time_step\n",
        "        #print(batch_time_step)\n",
        "        noise_rates, signal_rates = model.diffusion_schedule(batch_time_step)\n",
        "        #print(noise_rates, signal_rates)\n",
        "        cur_alphas = signal_rates**2\n",
        "        betas = compute_beta(cur_alphas, prev_alphas)\n",
        "\n",
        "        # PREDICT IMAGE\n",
        "        x_input = tf.math.multiply(x, tf.math.abs(mask - 1))\n",
        "        print(\"X, NR, SR \", x_input.shape, noise_rates.shape, signal_rates.shape, \" \\n\")\n",
        "        pred_x0, pred_noise = model.denoise(x_input, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "        int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "        embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "        # optional second order sampling\n",
        "        if second_order_alpha is not None:\n",
        "            embed_pred_x0, pred_noises = second_order_correction(\n",
        "                model,\n",
        "                batch_time_step,\n",
        "                step_size,\n",
        "                x,\n",
        "                signal_rates,\n",
        "                noise_rates,\n",
        "                embed_pred_x0,\n",
        "                pred_noise,\n",
        "                second_order_alpha,\n",
        "                mask,\n",
        "                pixels,)\n",
        "\n",
        "        mean_x0 = tf.math.sqrt(cur_alphas) * betas / (1 - prev_alphas) * embed_pred_x0\n",
        "        mean_x = tf.math.sqrt(1 - betas) * (1 - cur_alphas) / (1 - prev_alphas) * x\n",
        "        x = mean_x + mean_x0 + tf.reshape(tf.math.sqrt(betas), (-1, 1, 1, 1)) * tf.random.normal(x.shape)\n",
        "\n",
        "        samples_list.append(pred_x0)\n",
        "        prev_alphas = cur_alphas\n",
        "\n",
        "    return pred_x0, samples_list"
      ],
      "metadata": {
        "id": "Krk0-qfIKNk1"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_snr_list(nb_steps, model, eps):\n",
        "\n",
        "    start_noise_rates, start_signal_rates = model.diffusion_schedule(np.array([eps, ]))\n",
        "    start_snr = compute_snr(start_signal_rates, start_noise_rates)\n",
        "    end_noise_rates, end_signal_rates = model.diffusion_schedule(np.array([1., ]))\n",
        "    end_snr = compute_snr(end_signal_rates, end_noise_rates)\n",
        "    list_i = np.arange(0, nb_steps, 1.0, dtype=int)\n",
        "    all_snr = tf.cast(list_i / nb_steps * (start_snr - end_snr) + end_snr, dtype=tf.float32)\n",
        "    all_t = tf.cast(get_t_from_snr(all_snr), dtype=tf.float32)\n",
        "\n",
        "    return all_snr, all_t"
      ],
      "metadata": {
        "id": "MYg3ZH4rpjqp"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import xla\n",
        "def ddpm_solver(model, img_embed_size, image_size, batch_size=10, num_steps=50, eps=1e-3, mask=None, pixels=None):\n",
        "    second_order_alpha = 1.1\n",
        "    # T and schedule\n",
        "    t = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32)\n",
        "    noise_rates, signal_rates = model.diffusion_schedule(t)\n",
        "\n",
        "    if mask is None:\n",
        "        mask = tf.zeros((batch_size, image_size[0], image_size[1], 1), dtype=tf.float32)\n",
        "        pixels = tf.zeros((batch_size, image_size[0], image_size[1], img_embed_size), dtype=tf.float32)\n",
        "    else:\n",
        "        \n",
        "        pixels = tf.argmax(pixels, axis=-1)\n",
        "        pixels = model.embedding_layer(pixels)\n",
        "        pixels = tf.math.multiply(pixels, mask)\n",
        "\n",
        "        mask = tf.repeat(mask, batch_size, axis=0)\n",
        "        mask = tf.cast(mask,  dtype=tf.float32)\n",
        "        pixels = tf.repeat(pixels, batch_size, axis=0)\n",
        "\n",
        "    # Sample noise\n",
        "    uniform_init_x = tf.random.uniform((batch_size, image_size[0], image_size[1]), 0, 4, dtype=tf.dtypes.int32)\n",
        "    # Pure white noise\n",
        "    noises = tf.random.normal(shape=(batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "    init_x = signal_rates * model.embedding_layer(uniform_init_x) + noise_rates * noises\n",
        "\n",
        "    # Keep track of the Markov chain\n",
        "    samples_list = []\n",
        "    samples_list.append(tf.keras.backend.constant(keras.utils.to_categorical(uniform_init_x)))\n",
        "\n",
        "    # Steps and other algorithmic variables\n",
        "    #time_steps = tf.linspace(1., eps, num_steps)\n",
        "    #step_size = time_steps[0] - time_steps[1]\n",
        "    x = init_x\n",
        "    #prev_alphas = signal_rates**2\n",
        "    #prev_snr = compute_snr(signal_rates, noise_rates)\n",
        "    all_snr, time_steps = get_snr_list(num_steps, model, eps)\n",
        "    \n",
        "    prev_noise_rates, prev_signal_rates = noise_rates, signal_rates\n",
        "\n",
        "    # INFERENCE REVERSE LOOP\n",
        "    for i, time_step in enumerate(tqdm.tqdm(time_steps)):\n",
        "        prev_snr = all_snr[i]\n",
        "        batch_time_step = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32) * time_step\n",
        "        noise_rates, signal_rates = model.diffusion_schedule(batch_time_step)\n",
        "\n",
        "        # PREDICT IMAGE\n",
        "        x_input = tf.math.multiply(x, tf.math.abs(mask - 1))\n",
        "        pred_x0, pred_noise = model.denoise(x_input, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "        int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "        embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "        predicted_noise = x_input - embed_pred_x0\n",
        "\n",
        "        #snr = compute_snr(signal_rates, noise_rates)\n",
        "        if i < len(time_steps)-1:\n",
        "            snr = all_snr[i+1]\n",
        "\n",
        "            s_t = get_t_from_snr((snr + prev_snr) / 2)\n",
        "            s_t = tf.repeat(tf.reshape(s_t, (1, 1, 1, 1)), batch_size, axis=0)\n",
        "\n",
        "            s_noise_rates, s_signal_rates = model.diffusion_schedule(s_t)\n",
        "          \n",
        "\n",
        "            u = s_signal_rates / prev_signal_rates * x - s_noise_rates * (tf.math.exp((snr - prev_snr) / 2) - 1) * predicted_noise\n",
        "\n",
        "            # PREDICT IMAGE 2\n",
        "            x_input = tf.math.multiply(u, tf.math.abs(mask - 1))\n",
        "            pred_x0, pred_noise = model.denoise(x_input, s_signal_rates, s_noise_rates, training=False, mask=mask, pixels=pixels)\n",
        "            int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "            embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "            predicted_noise = x_input - embed_pred_x0\n",
        "\n",
        "            x = signal_rates / prev_signal_rates * x - noise_rates * (tf.math.exp((snr - prev_snr)) - 1) * predicted_noise\n",
        "\n",
        "        samples_list.append(pred_x0)\n",
        "        #prev_alphas = cur_alphas\n",
        "        prev_snr = snr\n",
        "        prev_noise_rates, prev_signal_rates = noise_rates, signal_rates\n",
        "    pred_x0, pred_noise = model.denoise(x, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "    return pred_x0, samples_list"
      ],
      "metadata": {
        "id": "RBs0D0Ts6qNn"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = ddpm_solver #@param ['ddpm_sampler', 'ddpm_solver'] {'type': 'raw'}\n",
        "\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                (64, 128),\n",
        "                                sample_batch_size,\n",
        "                                mask=None,\n",
        "                                pixels=None)"
      ],
      "metadata": {
        "id": "T0uowLytML_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aaf3c6c-b878-421f-8a00-99d778d3144e"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:30<00:00,  1.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(sample_batch_size):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(np.argmax(samples[i].numpy(), axis=-1).reshape((64, 128)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "O4GzBJX-MUmk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "edc211cf-2fda-49e4-db90-f4ee1d1c32b3"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAGVCAYAAABjBWf4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAOo0lEQVR4nO3c3Y3rypkFUMlQEAPcMMgsGug0Jgyjw3AaDXQWxTAMTBY1D/fBhkH6Fnvzp0it9chTh/paIiVtFLSftdb6AAAACPzt7AEAAIDrEywAAICYYAEAAMQECwAAICZYAAAAMcECAACICRYAAEBMsAAAAGKCBQAAEHu1Lixl3HOOx/gzzT/ux7DJefZ2lTnvaun5X3tdbfm6HPEYZ9j7Wr/K89bbnFvNc9Z51l5XS7Z6/s96T9n7PEt6u7/2tuX96zXj3/X22bDW4vxjafr/diwAAICYYAEAAMQECwAAICZYAAAAMcECAACIPWuttWXh3q1QV7F3e9XebQJbNa8sOasx5R2ddS1exdWvxXd7fXub86z2qt5elyW9PT9nto2dNdNaez+nV/l+9G7v3Zu1Uf29KS7YsQAAAHKCBQAAEBMsAACAmGABAADEBAsAACCmFQoAAE7QW3vYknEsTevsWAAAADHBAgAAiAkWAABATLAAAABiggUAABB7nT0AAAC8o+7anxZaqh6N5bB2LAAAgJhgAQAAxAQLAAAgJlgAAAAxwQIAAIhphQIAABZbqhpLoexYAAAAOcECAACICRYAAEBMsAAAAGKCBQAAEBMsAACAmGABAADEBAsAACAmWAAAADHBAgAAiAkWAABATLAAAABiggUAABATLAAAgJhgAQAAxAQLAAAgJlgAAACxV3qC8WeaPV4+hvTUAADAQZa+1z/Gtv9vxwIAAIgJFgAAQEywAAAAYoIFAAAQEywAAIDYs9ZaWxaW0vhz8INoowIAgP2NY2laZ8cCAACICRYAAEBMsAAAAGKCBQAAEBMsAACA2OvsAX5L+xMAAPTDjgUAABATLAAAgJhgAQAAxAQLAAAgJlgAAACxy7ZCTdP37PFh+Dx4EgAAwI4FAAAQEywAAICYYAEAAMQECwAAICZYAAAAsWettTat/HrOHi4fw5bzdE8bFQAA72QcS9M6OxYAAEBMsAAAAGKCBQAAEBMsAACAmGABAADEmluhShn3ngUAAOiMVigAAOAwggUAABATLAAAgJhgAQAAxAQLAAAgJlgAAAAxwQIAAIgJFgAAQEywAAAAYoIFAAAQEywAAIDYa68Tjz/T7PHyMez1kAAAwEnsWAAAADHBAgAAiAkWAABATLAAAABiggUAABDbrRVqqf1JW9R/N03fs8eH4fPgSQAAoJ0dCwAAICZYAAAAMcECAACICRYAAEBMsAAAAGK7tULxO9qfAAC4IjsWAABATLAAAABiggUAABATLAAAgJhgAQAAxHZrhRp/pr1O/StL85SP4eBJjvFuf+9Wpul79ri2ru15rrkT1zOAHQsAAGADggUAABATLAAAgJhgAQAAxAQLAAAg9qy11paFpYx7z8KJNJoAV+I9C+A441ia1tmxAAAAYoIFAAAQEywAAICYYAEAAMQECwAAINbcCvX4es4eLh/DlvNwERpZ4NrcwwC00goFAAAcRrAAAABiggUAABATLAAAgJhgAQAAxJpboUoZ956FX9Dswn9yTUDu6vfR1efnr3mNOZJWKAAA4DCCBQAAEBMsAACAmGABAADEBAsAACCmFQrgxjTHXMvS67XE60gvvNfcm1YoAADgMIIFAAAQEywAAICYYAEAAMQECwAAIKYVik1phQDu4CrvZVqkgCNohQIAAA4jWAAAADHBAgAAiAkWAABATLAAAABiWqGAVa7SlgMAbEMrFAAAcBjBAgAAiAkWAABATLAAAABiggUAABB7nT0AfRh/ptnj5WM4eBJ6p/0J4Dia+LgSOxYAAEBMsAAAAGKCBQAAEBMsAACAmGABAADEnrXW2rTy6zl7WGsQvBcNJQAczWfPucaxNK2zYwEAAMQECwAAICZYAAAAMcECAACICRYAAECsuRWqlHHvWQDgUJpmAP6aVigAAOAwggUAABATLAAAgJhgAQAAxAQLAAAg9jp7gN8af6bZ4+VjOHgSAK5K+xPvTjMaW7JjAQAAxAQLAAAgJlgAAAAxwQIAAIgJFgAAQOxZa60tC0sZ954FuIDeGkR6mwf+nesTuINxLE3r7FgAAAAxwQIAAIgJFgAAQEywAAAAYoIFAAAQ260VavyZ5s/zMaw6DwCkfCYBv+G9409aoQAAgMMIFgAAQEywAAAAYoIFAAAQEywAAIBYcyvU4+s5e/jdfhUP70ITBgDweGiFAgAADiRYAAAAMcECAACICRYAAEBMsAAAAGLNrVCljHvPAgC3Mk3fs8eH4fPgSUgsvY6Ph9fyr7gH7kErFAAAcBjBAgAAiAkWAABATLAAAABiggUAABDTCgXckiYSANiGVigAAOAwggUAABATLAAAgJhgAQAAxAQLAAAgJlgAAACx5rrZx9dz9nD5GFY94PgzbXIeADiazzDgHambBQAADiNYAAAAMcECAACICRYAAEBMsAAAAGLNrVCljHvPAgAAzabpe/b4MHwePMm9aYUCAAAOI1gAAAAxwQIAAIgJFgAAQEywAAAAYq+zBwAAgN/Q/tQXOxYAAEBMsAAAAGKCBQAAEBMsAACAmGABAADEBAsAACAmWAAAADHBAgAAiAkWAABATLAAAABiggUAABB7nT0A8J6m6Xv2+DB8HjwJAGzj6p9t48+08A9t/9+OBQAAEBMsAACAmGABAADEBAsAACAmWAAAALFnrbW2LCyl8efgAADAbYxjaVpnxwIAAIgJFgAAQEywAAAAYoIFAAAQEywAAIDY6+wBAADgDsafafZ4+RgOnuQcdiwAAICYYAEAAMQECwAAICZYAAAAMcECAACIaYUCAOBQ0/Q9e3wYPg+eZFvv0v60xI4FAAAQEywAAICYYAEAAMQECwAAICZYAAAAMa1QAAA3cZW2pd7mYRt2LAAAgJhgAQAAxAQLAAAgJlgAAAAxwQIAAIg9a621aeXXc/Zw+Rhmj48/06r1Z7nKnL3xvAEAv3WV9irfd/40jqVpnR0LAAAgJlgAAAAxwQIAAIgJFgAAQEywAAAAYq+9Ttzbr+X9qp/euUbhuty/sE5v7U9L3MPr2LEAAABiggUAABATLAAAgJhgAQAAxAQLAAAg9qy11paFpYx7zwIAAHRmHEvTOjsWAABATLAAAABiggUAABATLAAAgJhgAQAAxF5nD3CU8WeaPV4+hoMnAQC4F9+zeDzsWAAAABsQLAAAgJhgAQAAxAQLAAAgJlgAAACxZ621tiwsZVx1Yu0A7MF1Re9cowDczTiWpnV2LAAAgJhgAQAAxAQLAAAgJlgAAAAxwQIAAIjt1goFsKVp+p49PgyfB08Cx3P9w3vprWFQKxQAAHAYwQIAAIgJFgAAQEywAAAAYoIFAAAQa26Fenw9Zw+f9uv0zn4tDwD0xXcF2IZWKAAA4DCCBQAAEBMsAACAmGABAADEBAsAACD2al24VYOChgaA7U3T9+zxYfg8eBLoR4/fLXwPosVVrxM7FgAAQEywAAAAYoIFAAAQEywAAICYYAEAAMSetdbasrCUce9Zbumqv+oHAH7P5z93Mo6laZ0dCwAAICZYAAAAMcECAACICRYAAEBMsAAAAGLNrVCPr+fs4bXtBlu1JGhbAACA/WmFAgAADiNYAAAAMcECAACICRYAAEBMsAAAAGKvox9QaxMAANyv5dSOBQAAEBMsAACAmGABAADEBAsAACAmWAAAALFnrbW2LCxl3HsWYIW7NUmQcT0AsJdxLE3r7FgAAAAxwQIAAIgJFgAAQEywAAAAYoIFAAAQa26Fenw9Zw8vNY5oKAGOME3fs8eH4fPgSeiB6+EefIfg3fV2D2iFAgAADiNYAAAAMcECAACICRYAAEBMsAAAAGK7tUItWfsr995+Fb/kKnMu6W1+zS7cTW/3GNBu6TPp8fC5RJurfwZohQIAAA4jWAAAADHBAgAAiAkWAABATLAAAABiza1QpYybPODVfxV/V14XAI5258+eO/9tV+D535ZWKAAA4DCCBQAAEBMsAACAmGABAADEBAsAACDW3Ar1+HrOHt7q1/V+vQ/8xjR9zx4fhs+DJ2EPS58NS3r7zPDZdi09vp+4huiBVigAAOAwggUAABATLAAAgJhgAQAAxAQLAAAg9trrxGe1GOz9uFud/ypzwn/q7d5+fNyz/am357m3x13bFrW3pTahu16fWznrelt6vf73//6YPV4e53129vb9ojc9NnldwV7XiR0LAAAgJlgAAAAxwQIAAIgJFgAAQEywAAAAYs2tUGt/JX5W+8BWDSJL59nq7/rH//xz9vjwmG8x2Gr+q7hKM82W51q7frGFZsFS28mSve+BJUt/1/iYn/+uDSga4u7hKk2Ca23VynXW/GvfD4/QW0NWb61KS8/P0mfDmU1ec3q7h/d6XDsWAABATLAAAABiggUAABATLAAAgJhgAQAAxJpbofZutjjLWc0ca9sW3q0h5g4tCXv/DauvoZUNGWddK0ttLVs1vvGnq7wX7G2r52HtdbvWVV4X9+nvndU62Vv701pbNS0u2eo8V2q1TNixAAAAYoIFAAAQEywAAICYYAEAAMQECwAAIPastdaWhaWMmzzg2maId/kV/W9dZf6rzPkbvf1tW7WvnNWccdfzn6W399yr2/v57K3JZqvzn3V/uf7/5d3e467+d/VmHEvTOjsWAABATLAAAABiggUAABATLAAAgJhgAQAAxF6tC7dqgNjKVk0PW82/d2PH2nm2mv/qzSJnNSRtae97aatrd63e3iP2Pk9v7xFbPe7e59n7/Fe5DpdM0/f84z7+WHWevedc+7hXatXrrX3orHvjrHu7t/emvfU+vx0LAAAgJlgAAAAxwQIAAIgJFgAAQEywAAAAYs9aa21ZWMq49yzQrf/WjtFLEwOwv94bWX7rrn8XsI1xLE3r7FgAAAAxwQIAAIgJFgAAQEywAAAAYoIFAAAQa26FAgAAWGLHAgAAiAkWAABATLAAAABiggUAABATLAAAgJhgAQAAxAQLAAAgJlgAAAAxwQIAAIj9PzwSMOivRQPnAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAGVCAYAAABjBWf4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAOCklEQVR4nO3c3W3ryhmGUStQEQFSBtmFALeRMoJdRtowoC7IMgKki8ntxgnpPfTLnxlprUseHnokcst+MNB3K6WUDwAAgMDfrl4AAADQP2EBAADEhAUAABATFgAAQExYAAAAMWEBAADEhAUAABATFgAAQExYAAAAsXvtidM0HrmOj/E5L//cx7Dp/F4c/bpae996f71r1//uZ3z3/+zhVe9lL3p/prfa+jwf/Zm+9TpHn3+Vve5Laz/3Ve/Xd666l3zvFZ6tXfyrVJ1mxwIAAIgJCwAAICYsAACAmLAAAABiwgIAAIjdSilVX/M+eirUVfaa6HPVZKCjHf26Wnv/f3Kd1u59L/dsL1dNm9m6nl4+U/Z6P/f6ub1/hm71qs/Pq/7cnhw9Ma2X9fQysW7NZdOoTIUCAADOIiwAAICYsAAAAGLCAgAAiAkLAAAg9vZToQAAgG+mTpkKBQAAnEVYAAAAMWEBAADEhAUAABATFgAAQExYAAAAMWEBAADEhAUAABATFgAAQExYAAAAMWEBAADE7rUnjs958fj0GHZbDAAA0Cc7FgAAQExYAAAAMWEBAADEhAUAABATFgAAQKx6KpTpTwAAwBo7FgAAQExYAAAAMWEBAADEhAUAABATFgAAQExYAAAAMWEBAADEhAUAABATFgAAQExYAAAAMWEBAADE7lcvAAAAuN70GBaPj5X/vx0LAAAgJiwAAICYsAAAAGLCAgAAiAkLAAAgJiwAAICYsAAAAGLCAgAAiAkLAAAgJiwAAICYsAAAAGLCAgAAiAkLAAAgJiwAAICYsAAAAGLCAgAAiAkLAAAgdr96AbDFPH8tHh+Gz5NXAgDA7+xYAAAAMWEBAADEhAUAABATFgAAQExYAAAAMVOh6IrpTwAAbbJjAQAAxIQFAAAQExYAAEBMWAAAADFhAQAAxKqnQo3PefH49Bh2WwwAANAnOxYAAEBMWAAAADFhAQAAxIQFAAAQExYAAECseirUVdOfTKMCAID22bEAAABiwgIAAIgJCwAAICYsAACAmLAAAABi1VOhrmL6EwAAtM+OBQAAEBMWAABATFgAAAAxYQEAAMSEBQAAEGt+KtRV5vlr8fgwfJ68EgAAaJ8dCwAAICYsAACAmLAAAABiwgIAAIgJCwAAINb8VKjxOS8enx7DoddpbfqTKVUAALTMjgUAABATFgAAQExYAAAAMWEBAADEhAUAABC7lVJK1Zm/bouHt05nYl97TYsydYqzeeZ+xvvGK/E8t8u94XfjOFWdZ8cCAACICQsAACAmLAAAgJiwAAAAYsICAACIVU+FmqZx04XH57x8nY1TpPa6DgD8lck3fXG/fs57R8JUKAAA4DTCAgAAiAkLAAAgJiwAAICYsAAAAGKHTYWCGqZU9Mc9u5b3/zW4j7TOM8rvTIUCAABOIywAAICYsAAAAGLCAgAAiAkLAAAgdthUqPE5L1/nMWy6Dj/T2jSH1tYDAEAdU6EAAIDTCAsAACAmLAAAgJiwAAAAYsICAACIxVOh9pr+1MsUqV7WucZ0JgB4XX7PcwRToQAAgNMICwAAICYsAACAmLAAAABiwgIAAIjFU6FelakKALCN353wmkyFAgAATiMsAACAmLAAAABiwgIAAIgJCwAAIHa/egFnGZ/z4vHpMSweN8ECAJaZ/tQu94Yr2bEAAABiwgIAAIgJCwAAICYsAACAmLAAAABit1JKqTlxmsaj1wIAADRmHKeq8+xYAAAAMWEBAADEhAUAABATFgAAQExYAAAAsfvVC/iT8TkvHp8ew8krAeDdzfPX4vFh+Dx5JbDMM8qV7FgAAAAxYQEAAMSEBQAAEBMWAABATFgAAACxWymlVJ3567Z42HQmAAB4XeM4VZ1nxwIAAIgJCwAAICYsAACAmLAAAABiwgIAAIjda080/QkAoG3z/LV4fBg+T14J78iOBQAAEBMWAABATFgAAAAxYQEAAMSEBQAAEKueCtWa8TkvHje9CgB4V6Y/7cuUrW3sWAAAADFhAQAAxIQFAAAQExYAAEBMWAAAALFbKaXUnDhN49FreUlHTxN4t2kFr/B6X+E1AADvYxynqvPsWAAAADFhAQAAxIQFAAAQExYAAEBMWAAAALHqqVAfv26Lh6fHsOd6ALpm6hcAr8ZUKAAA4DTCAgAAiAkLAAAgJiwAAICYsAAAAGLVU6GmaTx6LfAWTA0CAHpiKhQAAHAaYQEAAMSEBQAAEBMWAABATFgAAACx+9ULgHdj+hMA78IkxPdixwIAAIgJCwAAICYsAACAmLAAAABiwgIAAIgJCwAAIGbcLBzEiD0A3p3fee/FjgUAABATFgAAQExYAAAAMWEBAADEhAUAABC7lVJK1Zm/bouHp8ew53oAaJApZwDvaxynqvPsWAAAADFhAQAAxIQFAAAQExYAAEBMWAAAALHqqVDTNB69FgAAoDGmQgEAAKcRFgAAQExYAAAAMWEBAADEhAUAABATFgAAQExYAAAAMWEBAADEhAUAABATFgAAQExYAAAAsfvVCwCgffP8tXh8GD5PXgkArbJjAQAAxIQFAAAQExYAAEBMWAAAADFhAQAAxEyFAuCPTH8C4E/sWAAAADFhAQAAxIQFAAAQExYAAEBMWAAAALHmp0KNz3nx+PQYTl4JCfcRAPiTef5aPG4yXR/sWAAAADFhAQAAxIQFAAAQExYAAEBMWAAAALFbKaXUnDhN49FrAQAAGjOOU9V5diwAAICYsAAAAGLCAgAAiAkLAAAgJiwAAIDYvfbE8TkvHp8ew26LAQAA+mTHAgAAiAkLAAAgJiwAAICYsAAAAGLCAgAAiFVPhep9+tM8fy0eH4bPk1cCAACvx44FAAAQExYAAEBMWAAAADFhAQAAxIQFAAAQq54K1TvTn4CejM958XjvE/pelfsFYMcCAADYgbAAAABiwgIAAIgJCwAAICYsAACA2K2UUmpOnKbx6LUAAACNGcep6jw7FgAAQExYAAAAMWEBAADEhAUAABATFgAAQOx+9QIAYJ6/Fo8Pw+fJKwHgp+xYAAAAMWEBAADEhAUAABATFgAAQExYAAAAMVOhALic6U/QN5Pd+PiwYwEAAOxAWAAAADFhAQAAxIQFAAAQExYAAEDsVkopVWf+ui0enh7DnuvpVu/TEMbnvHjc/QUAeG/jOFWdZ8cCAACICQsAACAmLAAAgJiwAAAAYsICAACI3WtP3Dod6N2mDPUy/QkAgLb1+ne0HQsAACAmLAAAgJiwAAAAYsICAACICQsAACB2K6WUmhOnaTx0IWvffl/T2rfie/32PgDAq/F32b7Gcao6z44FAAAQExYAAEBMWAAAADFhAQAAxIQFAAAQq54K9fHrtnh467frfUv/HN5nAOBVzPPX4vFh+Dx5Je/JVCgAAOA0wgIAAIgJCwAAICYsAACAmLAAAABi96sXAAAA3zH9qQ92LAAAgJiwAAAAYsICAACICQsAACAmLAAAgFg8FWp8zovHp8eQXhoAAOiEHQsAACAmLAAAgJiwAAAAYsICAACICQsAACAWT4VaszYtin2ZygUAQAvsWAAAADFhAQAAxIQFAAAQExYAAEBMWAAAALHDpkKt2TqtaK+pR71PT5rnr+X/8Pg8dyEAALDAjgUAABATFgAAQExYAAAAMWEBAADEhAUAABCLp0KtTVVam8K01/WP1toUqWEw/QkAeG2t/f21ppd1ns2OBQAAEBMWAABATFgAAAAxYQEAAMSEBQAAEIunQvVi67f0e/lWf2tTCeb5a/G4qVYA0J7Wfm/38vcXy+xYAAAAMWEBAADEhAUAABATFgAAQExYAAAAsVsppdScOE3jpgtfNa1o7eeuMX0ArtHaRDMAYNk4TlXn2bEAAABiwgIAAIgJCwAAICYsAACAmLAAAABi99oTt05w2Wuyyzx/LR7/53//scvPbW0yTWvrWdPLOte0uP61Z30YPk9eCTVafIZ4XUc/b57nn+v9vWtt/a2tp3dnv592LAAAgJiwAAAAYsICAACICQsAACAmLAAAgFj1VKirrE3EmT6Wv82+1/Sqo79FvzYB6OOx/Hpbm5Jw1fu25qrpYT+51pq1Z721e791Pa2tf6vv7v0V9no/t76uve7vVe/nVdOT1s7/99//s3j8qilwvfx7XLPX88zP9fK7oZd17uXsddqxAAAAYsICAACICQsAACAmLAAAgJiwAAAAYrdSSqk5cZrGTRfu5Vv0a9OE9prM0fukil7uI+06+hk6ekpSa8/6u32mHP16W3t+Wvv3snXi3pqjJ+5cNS3tu59x1b1sbeJYa3r5rG/NOE5V59mxAAAAYsICAACICQsAACAmLAAAgJiwAAAAYvfaE3uZ3LD1OkdPSTBl4FotTgo52l5Td854j7Zcp/fPmqOnIW39uVtd9TysedXP1tY+Nz4eK78jn8uHt67/6Nf7jhOAVid27XTP9nLVvXnle98COxYAAEBMWAAAADFhAQAAxIQFAAAQExYAAECseirUVY6eGtDaRJw1R7/erT/3qvettffhJ9c6+jp7Xf/oZ+XoKUmvep297HW/rrr+0Z8dV005a+0zeqvWprG9gqOfodaeud7/zfT+91HKjgUAABATFgAAQExYAAAAMWEBAADEhAUAABC7lVJK1Zm/bouHW5tIARzLv234f/5dXMv7z195JvY1jlPVeXYsAACAmLAAAABiwgIAAIgJCwAAICYsAACAWP1UKAAAgBV2LAAAgJiwAAAAYsICAACICQsAACAmLAAAgJiwAAAAYsICAACICQsAACAmLAAAgNj/APGby7N1C8+/AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data.load_data import ConditionalDataGenerator\n",
        "\n",
        "# Load Data\n",
        "slice_size = (64, 128, 4)\n",
        "dataloader = ConditionalDataGenerator(x_test, 1, slice_size, wells=10, mode=3)\n",
        "pixels, mask, ground_truth = dataloader.__getitem__(0)\n",
        "print(pixels.shape, mask.shape, ground_truth.shape)"
      ],
      "metadata": {
        "id": "LAAAFTajBiUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b56b3b-3dd7-411e-f0e1-8a5db674f546"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 64, 128, 4) (1, 64, 128, 1) (1, 64, 128, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raise Exception(\"DEBUG\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "fJZxsZYbtlnu",
        "outputId": "baeef687-e1b6-4d8e-e8a8-d216312074f0"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-9ee0d806948a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DEBUG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: DEBUG"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = ddpm_sampler #@param ['ddpm_sampler', 'pc_sampler'] {'type': 'raw'}\n",
        "\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                (64, 128),\n",
        "                                sample_batch_size,\n",
        "                                mask=mask,\n",
        "                                pixels=pixels)"
      ],
      "metadata": {
        "id": "iGnksfxx3JRB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(sample_batch_size):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(np.argmax(samples[i].numpy(), axis=-1).reshape((64, 128)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QjbvDyWoK7fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from utils.visualisation import *\n",
        "from data.load_data import get_3d_flumy_data, load_data, ConditionalDataGenerator\n",
        "from models.load_trained_models import load_msgen_horizontal, wgan_horizontal,\\\n",
        "    load_msnwgen_2d_gs_horizontal, load_wgan_gs_horizontal, load_mswgen_sn_3d_horizontal\n",
        "from utils.utils import generate_noise, correct_percentage\n"
      ],
      "metadata": {
        "id": "pP5_ix6TCLdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_simulations = 3\n",
        "cmap, norm = get_color_map(number_of_categories=4)\n",
        "\n",
        "print_conditioned_results(ground_truth, samples, mask, nb_simulations, cmap, norm)"
      ],
      "metadata": {
        "id": "0nla2sslCFTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.axis('off')\n",
        "\n",
        "plt.imshow(np.argmax(ground_truth, axis=-1).reshape((64, 128)),\n",
        "            interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "URj4sIq1BlYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = ddpm_sampler #@param ['ddpm_sampler', 'pc_sampler'] {'type': 'raw'}\n",
        "\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                (128, 256),\n",
        "                                sample_batch_size,)"
      ],
      "metadata": {
        "id": "gzMLizIXPUAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(sample_batch_size):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(np.argmax(samples[i].numpy(), axis=-1).reshape((128, 256)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "p88yuQgCPUA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GIF"
      ],
      "metadata": {
        "id": "8X9v42ujwnt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "images = []\n",
        "for i, batch in enumerate(samples_list):\n",
        "    figure = plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Timestep {0:.3f}\".format(1 - (i / 350)))\n",
        "    plt.imshow(np.argmax(batch[0].numpy(), axis=-1).reshape((64, 128)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.savefig('foo.png', bbox_inches='tight')\n",
        "    images.append(imageio.imread('foo.png'))\n",
        "    plt.show() #close(figure)\n",
        "imageio.mimsave('/movie.gif', images)"
      ],
      "metadata": {
        "id": "m9JzX8pVMf7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Models"
      ],
      "metadata": {
        "id": "GCPKYflD2fyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "SAVE_AND_TAR_RESULTS_WEIGHTS = True\n",
        "\n",
        "if SAVE_AND_TAR_RESULTS_WEIGHTS:\n",
        "  diffusion_checkpoint_path = \"diffusion_weights_horiz/cp-diffusion2d_net_horiz.ckpt\"\n",
        "  diffusion_checkpoint_dir = os.path.dirname(diffusion_checkpoint_path)\n",
        "\n",
        "  model.ema_network.save_weights(diffusion_checkpoint_path)\n",
        "\n",
        "  !tar -czvf diffusion_weights_horiz.tar.gz ./diffusion_weights_horiz\n",
        "\n",
        "  diffusion_checkpoint_path = \"diffusion_weights_horiz/cp-diffusion2d_embed_horiz.ckpt\"\n",
        "  diffusion_checkpoint_dir = os.path.dirname(diffusion_checkpoint_path)\n",
        "\n",
        "  model.embedding_layer.save_weights(diffusion_checkpoint_path)\n",
        "\n",
        "  !tar -czvf diffusion_weights_horiz.tar.gz ./diffusion_weights_horiz"
      ],
      "metadata": {
        "id": "gi9sVjP0QHdb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ddim",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}