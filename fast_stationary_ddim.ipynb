{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pumafi/dl_spatial_gen_geol_facies/blob/main/fast_stationary_ddim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "TA0koXfT2e7k",
        "outputId": "46758689-ed17-4ff5-892a-f6d55209ebef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun  8 14:35:11 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## IDEAS \n",
        "1.   Normalization in embedding ?\n",
        "2.   Formula when I replace beta by t\n",
        "3.   Use keras inference for potential changes\n",
        "\n",
        "## What to try next?\n",
        "\n",
        "If you would like to dive in deeper to the topic, a recommend checking out\n",
        "[this repository](https://github.com/beresandras/clear-diffusion-keras) that I created in\n",
        "preparation for this code example, which implements a wider range of features in a\n",
        "similar style, such as:\n",
        "\n",
        "* stochastic sampling\n",
        "* second-order sampling based on the\n",
        "[differential equation view of DDIMs (Equation 13)](https://arxiv.org/abs/2010.02502)\n",
        "* more diffusion schedules\n",
        "* more network output types: predicting image or\n",
        "[velocity (Appendix D)](https://arxiv.org/abs/2202.00512) instead of noise\n",
        "* more datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "rge41L-HIY-i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqkfNOJcD0Ym"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install deel.lip\n",
        "!python -m pip install -i https://test.pypi.org/simple/ gstlearn"
      ],
      "metadata": {
        "id": "u03Nq4eK2ems",
        "outputId": "a163d8ec-f809-496a-f1be-58add294f631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deel.lip\n",
            "  Downloading deel_lip-1.4.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deel.lip) (1.22.4)\n",
            "Requirement already satisfied: tensorflow~=2.2 in /usr/local/lib/python3.10/dist-packages (from deel.lip) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->deel.lip) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.2->deel.lip) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.2->deel.lip) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.2->deel.lip) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.2->deel.lip) (3.2.2)\n",
            "Installing collected packages: deel.lip\n",
            "Successfully installed deel.lip-1.4.0\n",
            "Looking in indexes: https://test.pypi.org/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gstlearn\n",
            "  Downloading https://test-files.pythonhosted.org/packages/4b/33/9b8e2546cfe286525409a1d17279b325a7a1d2968eba07357a0e30976d29/gstlearn-0.2.1-cp310-cp310-manylinux1_x86_64.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gstlearn) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gstlearn) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gstlearn) (1.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from gstlearn) (5.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gstlearn) (1.5.3)\n",
            "INFO: pip is looking at multiple versions of gstlearn to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://test-files.pythonhosted.org/packages/ea/39/5475c8fac8f0b9897ef6be34b92d40dae6d209dbfe2a7db0fb7a9386fadc/gstlearn-0.1.38-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://test-files.pythonhosted.org/packages/11/e9/eee76162b77c60d4c57aea94c230259605295eb3ad7e76b4b3c67773ff0f/gstlearn-0.1.37-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gstlearn\n",
            "Successfully installed gstlearn-0.1.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RUNNING_IN_COLAB = True\n",
        "\n",
        "if RUNNING_IN_COLAB:\n",
        "    # Uses a private Auth Token, giving read and write access to repo\n",
        "    # TO DELETE IF REPO GOES PUBLIC\n",
        "    REPO_URL = 'https://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@github.com/Pumafi/flumy-wgan-mines'\n",
        "    BRANCH   = 'main'\n",
        "    REPO_DIR = 'flumy-wgan-mines'\n",
        "\n",
        "    from pathlib import Path\n",
        "\n",
        "    %cd /content\n",
        "\n",
        "    if Path(REPO_DIR).is_dir():\n",
        "      !rm -rf {REPO_DIR}\n",
        "\n",
        "    # Download the repository\n",
        "    if not Path(REPO_DIR).is_dir():\n",
        "        !git clone --branch {BRANCH} --depth=1 -- {REPO_URL} {REPO_DIR}\n",
        "    \n",
        "    %cd {REPO_DIR}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqPH8VxsGGrb",
        "outputId": "3aafa7c3-867d-459b-87ed-7ca4d6f005ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'flumy-wgan-mines'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 146 (delta 31), reused 74 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 140.19 MiB | 8.47 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n",
            "Updating files: 100% (121/121), done.\n",
            "/content/flumy-wgan-mines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3E8qnnCGH5K",
        "outputId": "8555c3c6-8bda-4713-d564-91a3e9614502"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2VQcRg8KD0Yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d374b8-3680-47cb-c980-c34c4cd3694e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from data.load_data import load_data\n",
        "from utils.visualisation import get_color_map\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "1eF1rmySGCzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful constants\n",
        "image_size = (64, 128)\n",
        "cmap, norm = get_color_map(number_of_categories=4)\n",
        "facies_names = np.array([\"Sand, Channel lag\", \"Sand, Point bar\", \"Silts, Levee\", \"Shale, Overbank\"])\n",
        "x = load_data(image_size[0], image_size[1], \"./data/horizontal/dataFlumyHoriz.csv\")\n",
        "x_train = x[:2760]\n",
        "x_test = x[2760:]"
      ],
      "metadata": {
        "id": "4yPyDmhUGCTa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7-ElzPrD0Yq"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FBoSc7iCD0Ys"
      },
      "outputs": [],
      "source": [
        "# sampling\n",
        "min_signal_rate = 0.02\n",
        "max_signal_rate = 0.95\n",
        "\n",
        "# architecture\n",
        "widths = [32, 64, 128, 256]\n",
        "block_depth = 2\n",
        "\n",
        "# Data values embedding\n",
        "img_embed_size = 64\n",
        "categories_nb = 4\n",
        "\n",
        "# optimization\n",
        "batch_size = 30\n",
        "ema = 0.999\n",
        "learning_rate = 1e-4\n",
        "embeding_net_lr = 1e-3\n",
        "weight_decay = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diffusion Schedules"
      ],
      "metadata": {
        "id": "acL9rhHAdCCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "class DiffusionSchedule(ABC):\n",
        "    def __init__(self, start_log_snr, end_log_snr):\n",
        "        assert (\n",
        "            start_log_snr > end_log_snr\n",
        "        ), \"The starting SNR has to be higher than the final SNR.\"\n",
        "\n",
        "        self.end_beta = 20\n",
        "        self.start_beta = 0.1\n",
        "\n",
        "        self.start_snr = tf.exp(start_log_snr)\n",
        "        self.end_snr = tf.exp(end_log_snr)\n",
        "\n",
        "        #self.start_noise_power = 1.0 / (1.0 + self.start_snr)\n",
        "        #self.end_noise_power = 1.0 / (1.0 + self.end_snr)\n",
        "\n",
        "    def __call__(self, diffusion_times):\n",
        "        signal_powers = self.get_noise_powers(diffusion_times)\n",
        "        signal_rates = tf.math.exp(signal_powers)\n",
        "\n",
        "        noise_rates = (1 - signal_rates**2)**0.5\n",
        "\n",
        "\n",
        "        # the signal and noise power will always sum to one\n",
        "        #signal_powers = 1.0 - noise_powers\n",
        "\n",
        "        # the rates are the square roots of the powers\n",
        "        # variance**0.5 -> standard deviation\n",
        "        #signal_rates = signal_powers**0.5\n",
        "        #noise_rates = noise_powers**0.5\n",
        "\n",
        "\n",
        "        return noise_rates, signal_rates\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_noise_powers(self, diffusion_times):\n",
        "        pass\n",
        "\n",
        "\n",
        "class SignalStepLinearSchedule(DiffusionSchedule):\n",
        "    # the ratio between next-step and current signal powers decreases approximately linearly to 1\n",
        "    # similar to the \"linear schedule\" of DDPM https://arxiv.org/abs/2006.11239\n",
        "    def get_noise_powers(self, diffusion_times):\n",
        "        \n",
        "        return -(self.end_beta - self.start_beta) / 4 * diffusion_times**2 - self.start_beta / 2 * diffusion_times\n",
        "\n",
        "        #return 1.0 - (1.0 - self.start_noise_power) * (\n",
        "        #    (1.0 - self.end_noise_power) / (1.0 - self.start_noise_power)\n",
        "        #) ** (diffusion_times**2)"
      ],
      "metadata": {
        "id": "Z5U9ClBmdDxI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "QxyZaS_UJ_YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianFourierProjection(tf.keras.layers.Layer):\n",
        "    \"\"\"Gaussian random features for encoding time steps.\"\"\"  \n",
        "    def __init__(self, embed_dim, scale=30.):\n",
        "        super().__init__()\n",
        "        # Randomly sample weights during initialization. These weights are fixed \n",
        "        # during optimization and are not trainable.\n",
        "        self.W = self.add_weight(shape=(embed_dim // 2,),\n",
        "                                 trainable=False,\n",
        "                                 initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.), name=\"GFP\") * tf.constant(scale, dtype=tf.float32)\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        x_proj = x * self.W * tf.constant(2., dtype=tf.float32) * tf.constant(np.pi, dtype=tf.float32)\n",
        "        y = tf.concat([tf.math.sin(x_proj), tf.cos(x_proj)], axis=-1)\n",
        "        return y # Probleme vient pas de là :()\n",
        "\n",
        "class CustomLinear(tf.keras.layers.Layer):\n",
        "    \"\"\"Rhaaah.\"\"\"  \n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.W = tf.random.uniform((input_dim, output_dim), minval=-tf.math.sqrt(1/input_dim), maxval=tf.math.sqrt(1/input_dim))\n",
        "        self.b = tf.random.uniform((1, output_dim, ), minval=-tf.math.sqrt(1/input_dim), maxval=tf.math.sqrt(1/input_dim))\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        y = tf.tensordot(x, self.W, 1) + self.b\n",
        "        y = tf.keras.activations.gelu(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "class EmbedLayerNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, img_embed_size):\n",
        "        super().__init__()\n",
        "        self.beta = self.add_weight(shape=(1, 1, 1, 1),\n",
        "                                    initializer=tf.keras.initializers.Zeros(),\n",
        "                                    dtype=tf.float32,\n",
        "                                    name=\"beta_embed_layer\",\n",
        "                                    trainable=True)\n",
        "        self.gamma = self.add_weight(shape=(1, 1, 1, 1),\n",
        "                                     initializer=tf.keras.initializers.Ones(),\n",
        "                                     dtype=tf.float32,\n",
        "                                     name=\"gamma_embed_layer\",\n",
        "                                     trainable=True)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "\n",
        "        mean, variance = tf.nn.moments(inputs, -1, keepdims=True)\n",
        "        outputs = tf.nn.batch_normalization(\n",
        "            inputs,\n",
        "            mean,\n",
        "            variance,\n",
        "            offset=self.beta,\n",
        "            scale=self.gamma,\n",
        "            variance_epsilon=1e-8,\n",
        "        )\n",
        "\n",
        "        return outputs #* self.gamma + self.beta\n",
        "\n",
        "@tf.function\n",
        "def embedding_normalization(logits):\n",
        "    # normalement vont avoir taille (batch_size, 64, 128, embedding_size)\n",
        "    # axis=-1 is embedding normalement\n",
        "    return (logits / tf.norm(logits, axis=-1, keepdims=True)) * tf.constant(np.sqrt(logits.shape[-1]), dtype=tf.float32)\n",
        "\n",
        "class NormalizedEmbedding(tf.keras.Model):\n",
        "    \"\"\"\"\"\"  \n",
        "    def __init__(self, categories_nb, img_embed_size):\n",
        "        super().__init__()\n",
        "        self.embed_layer = tf.keras.layers.Embedding(categories_nb, img_embed_size)\n",
        "        self.embed_layer2 = layers.Conv2D(img_embed_size, kernel_size=1, padding=\"same\", activation=keras.activations.swish)\n",
        "        self.embed_layer3 = layers.Conv2D(img_embed_size, kernel_size=1, padding=\"same\", activation=keras.activations.swish)\n",
        "        self.embed_layer4 = layers.Conv2D(img_embed_size, kernel_size=1, activation=None)\n",
        "        self.layer_norm = EmbedLayerNormalization(img_embed_size=16)\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        # I tested the embedding it's perfectly pixel by pixel\n",
        "        y = self.embed_layer(x)\n",
        "        y = self.embed_layer2(y)\n",
        "        y = self.embed_layer3(y)\n",
        "        y = self.embed_layer4(y)\n",
        "        #y = embedding_normalization(y)\n",
        "        y = self.layer_norm(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "Ej4nARwoGmme"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network(\n",
        "    image_size,\n",
        "    noise_embedding_max_frequency,\n",
        "    noise_embedding_dims,\n",
        "    image_embedding_dims,\n",
        "    block_depth,\n",
        "    widths,\n",
        "    attentions,\n",
        "    patch_size,\n",
        "    embed_size\n",
        "):\n",
        "    def EmbeddingLayer(embedding_max_frequency, embedding_dims):\n",
        "        def sinusoidal_embedding(x):\n",
        "            embedding_min_frequency = 1.0\n",
        "            frequencies = tf.exp(\n",
        "                tf.linspace(\n",
        "                    tf.math.log(embedding_min_frequency),\n",
        "                    tf.math.log(embedding_max_frequency),\n",
        "                    embedding_dims // 2,\n",
        "                )\n",
        "            )\n",
        "            angular_speeds = 2.0 * math.pi * frequencies\n",
        "            embeddings = tf.concat(\n",
        "                [\n",
        "                    tf.sin(angular_speeds * x),\n",
        "                    tf.cos(angular_speeds * x),\n",
        "                ],\n",
        "                axis=3,\n",
        "            )\n",
        "            return embeddings\n",
        "\n",
        "        def forward(x):\n",
        "            x = layers.Lambda(sinusoidal_embedding)(x)\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def ResidualBlock(width, attention):\n",
        "        def forward(x):\n",
        "            x, n = x\n",
        "            input_width = x.shape[3]\n",
        "            if input_width == width:\n",
        "                residual = x\n",
        "            else:\n",
        "                residual = layers.Conv2D(width, kernel_size=1)(x)\n",
        "\n",
        "            n = layers.Dense(width)(n)\n",
        "\n",
        "            x = tfa.layers.GroupNormalization(groups=8)(x)\n",
        "            x = keras.activations.swish(x)\n",
        "            x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "\n",
        "            x = layers.Add()([x, n])\n",
        "\n",
        "            x = tfa.layers.GroupNormalization(groups=8)(x)\n",
        "            x = keras.activations.swish(x)\n",
        "            x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "\n",
        "            x = layers.Add()([residual, x])\n",
        "\n",
        "            if attention:\n",
        "                residual = x\n",
        "                x = tfa.layers.GroupNormalization(groups=8, center=False, scale=False)(\n",
        "                    x\n",
        "                )\n",
        "                x = layers.MultiHeadAttention(\n",
        "                    num_heads=4, key_dim=width, attention_axes=(1, 2)\n",
        "                )(x, x)\n",
        "\n",
        "                x = layers.Add()([residual, x])\n",
        "\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def DownBlock(block_depth, width, attention):\n",
        "        def forward(x):\n",
        "            x, n, skips = x\n",
        "            for _ in range(block_depth):\n",
        "                x = ResidualBlock(width, attention)([x, n])\n",
        "                skips.append(x)\n",
        "            x = layers.AveragePooling2D(pool_size=2)(x)\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def UpBlock(block_depth, width, attention):\n",
        "        def forward(x):\n",
        "            x, n, skips = x\n",
        "            x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
        "            for _ in range(block_depth):\n",
        "                x = layers.Concatenate()([x, skips.pop()])\n",
        "                x = ResidualBlock(width, attention)([x, n])\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    images = keras.Input(shape=(None, None, embed_size))\n",
        "    noise_powers = keras.Input(shape=(1, 1, 1))\n",
        "    mask = keras.Input(shape=(None, None, 1))\n",
        "    conditioning_pixels = keras.Input(shape=(None, None, embed_size))\n",
        "\n",
        "    x = tf.keras.layers.Concatenate(axis=-1)([images, mask, conditioning_pixels])\n",
        "\n",
        "    x = layers.Conv2D(image_embedding_dims, kernel_size=patch_size, strides=patch_size)(\n",
        "        x\n",
        "    )\n",
        "\n",
        "    # NOISE EMBEDDING\n",
        "    n = EmbeddingLayer(noise_embedding_max_frequency, noise_embedding_dims)(\n",
        "        noise_powers\n",
        "    )\n",
        "    n = layers.Dense(noise_embedding_dims, activation=keras.activations.swish)(n)\n",
        "    n = layers.Dense(noise_embedding_dims, activation=keras.activations.swish)(n)\n",
        "\n",
        "    skips = []\n",
        "    for width, attention in zip(widths[:-1], attentions[:-1]):\n",
        "        x = DownBlock(block_depth, width, attention)([x, n, skips])\n",
        "\n",
        "    for _ in range(block_depth):\n",
        "        x = ResidualBlock(widths[-1], attentions[-1])([x, n])\n",
        "\n",
        "    for width, attention in zip(widths[-2::-1], attentions[-2::-1]):\n",
        "        x = UpBlock(block_depth, width, attention)([x, n, skips])\n",
        "\n",
        "    x = layers.Conv2DTranspose(\n",
        "        4, kernel_size=patch_size, strides=patch_size, kernel_initializer=\"zeros\", activation=\"softmax\"\n",
        "    )(x)\n",
        "\n",
        "    return keras.Model([images, noise_powers, mask, conditioning_pixels], x, name=\"residual_unet\")"
      ],
      "metadata": {
        "id": "IgeH9voVvsN5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sampling\n",
        "def make_mask(image_to_condition):\n",
        "    nb_conditioning_points = np.random.randint(low=1.0, high=image_to_condition.shape[0] * image_to_condition.shape[1])\n",
        "    random_x_coordinates = np.random.choice(image_to_condition.shape[0], nb_conditioning_points)\n",
        "    random_y_coordinates = np.random.choice(image_to_condition.shape[1], nb_conditioning_points)\n",
        "    mask = np.zeros((image_to_condition.shape[0], image_to_condition.shape[1], 1))\n",
        "    mask[random_x_coordinates, random_y_coordinates, :] = 1\n",
        "    return mask"
      ],
      "metadata": {
        "id": "XEMZlA-xdLcT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lZ37576YD0Y5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DiffusionModel(keras.Model):\n",
        "    def __init__(self, image_size, widths, block_depth, img_embed_size,\n",
        "                 categories_nb, embedding_lr=1e-3, batch_size=30,\n",
        "                 large_model=False):\n",
        "        super().__init__()\n",
        "\n",
        "        noise_embedding_max_frequency = 1000.0\n",
        "        noise_embedding_dims = 64\n",
        "        image_embedding_dims = 64\n",
        "        block_depth = 2\n",
        "\n",
        "        if large_model:\n",
        "            widths = [64, 128, 256, 512]\n",
        "            attentions = [False, False, True, True]\n",
        "        else:\n",
        "            widths = [64, 96, 128, 256]\n",
        "            attentions = [False, False, False, False]\n",
        "            \n",
        "        patch_size = 1\n",
        "\n",
        "        self.diffusion_schedule = SignalStepLinearSchedule(start_log_snr=3.0, end_log_snr=-10.0,)\n",
        "        #self.diffusion_schedule = CosineSchedule(start_log_snr=3.0, end_log_snr=-10.0,)\n",
        "        self.network = get_network(image_size, noise_embedding_max_frequency,\n",
        "                                   noise_embedding_dims, image_embedding_dims,\n",
        "                                   block_depth, widths, attentions, patch_size,\n",
        "                                   img_embed_size)\n",
        "        \n",
        "        self.ema_network = keras.models.clone_model(self.network)\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Embedding\n",
        "        self.img_embed_size = img_embed_size\n",
        "        self.embedding_layer = NormalizedEmbedding(categories_nb, img_embed_size)\n",
        "        self.emb_optimiser = tf.keras.optimizers.legacy.Adam(learning_rate=embedding_lr)\n",
        "\n",
        "    def compile(self, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.image_loss_tracker]\n",
        "\n",
        "    def denoise(self, noisy_images, noise_rates, signal_rates, training, mask=None, pixels=None):\n",
        "        # the exponential moving average weights are used at evaluation\n",
        "\n",
        "        if mask is None or pixels is None:\n",
        "            mask = tf.zeros(noisy_images.shape)\n",
        "            pixels = tf.zeros(noisy_images.shape)\n",
        "\n",
        "        if training:\n",
        "            network = self.network\n",
        "        else:\n",
        "            network = self.ema_network\n",
        "\n",
        "        # predict noise component and calculate the image component using it\n",
        "        pred_images = network([noisy_images, noise_rates**2, mask, pixels], training=training)\n",
        "\n",
        "        \n",
        "        int_encoded_img = tf.argmax(pred_images, axis=-1)\n",
        "        embed_pred_images = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "        pred_noises = (noisy_images - signal_rates * embed_pred_images) / noise_rates\n",
        "\n",
        "        return pred_images, pred_noises\n",
        "\n",
        "    def train_step(self, images):\n",
        "        # normalize images to have standard deviation of 1, like the noises\n",
        "        noises = tf.random.normal(shape=(self.batch_size, self.image_size[0], self.image_size[1], self.img_embed_size))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = tf.random.uniform(\n",
        "            shape=(self.batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        mask_uncondi = tf.zeros((self.batch_size // 2, images.shape[1], images.shape[2], 1))\n",
        "        mask_condi = tf.map_fn(make_mask, images[self.batch_size // 2:])\n",
        "        mask = tf.concat([mask_uncondi, mask_condi], axis=0)\n",
        "\n",
        "\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        # mix the images with noises accordingly\n",
        "        int_encoded_img = tf.argmax(images, axis=-1)\n",
        "\n",
        "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "            embed_images = self.embedding_layer(int_encoded_img)\n",
        "            pixels = tf.math.multiply(embed_images, mask)\n",
        "\n",
        "            noisy_images = signal_rates * embed_images + noise_rates * noises\n",
        "            noisy_images = tf.math.multiply(noisy_images, tf.math.abs(mask - 1))\n",
        "\n",
        "            # train the network to separate noisy images to their components\n",
        "            pred_images, pred_noise = self.denoise(\n",
        "                noisy_images, noise_rates, signal_rates, training=True, mask=mask, pixels=pixels\n",
        "            )\n",
        "\n",
        "            image_loss = self.loss(images, pred_images)  # training loss\n",
        "            \n",
        "\n",
        "        gradients_model = tape1.gradient(image_loss, self.network.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients_model, self.network.trainable_weights))\n",
        "\n",
        "        gradients_embeddings = tape2.gradient(image_loss, self.embedding_layer.trainable_weights)\n",
        "        self.emb_optimiser.apply_gradients(zip(gradients_embeddings, self.embedding_layer.trainable_weights))\n",
        "\n",
        "        self.image_loss_tracker.update_state(image_loss)\n",
        "\n",
        "        # track the exponential moving averages of weights\n",
        "        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
        "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
        "\n",
        "        # KID is not measured during the training phase for computational efficiency\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, images):\n",
        "        noises = tf.random.normal(shape=(self.batch_size, self.image_size[0], self.image_size[1], self.img_embed_size))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = tf.random.uniform(\n",
        "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "\n",
        "        mask_uncondi = tf.zeros((self.batch_size // 2, images.shape[1], images.shape[2], 1))\n",
        "        mask_condi = tf.map_fn(make_mask, images[self.batch_size // 2:])\n",
        "        mask = tf.concat([mask_uncondi, mask_condi], axis=0)\n",
        "\n",
        "        int_encoded_img = tf.argmax(images, axis=-1)\n",
        "\n",
        "        embed_images = self.embedding_layer(int_encoded_img)\n",
        "\n",
        "        #std = marginal_prob_std(diffusion_times, sigma=sigma)\n",
        "        pixels = tf.math.multiply(embed_images, mask)\n",
        "\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        noisy_images = signal_rates * embed_images + noise_rates * noises\n",
        "        noisy_images = tf.math.multiply(noisy_images, tf.math.abs(mask - 1))\n",
        "        #noisy_images = embed_images + noises * tf.reshape(std, (-1, 1, 1, 1))\n",
        "\n",
        "        # use the network to separate noisy images to their components\n",
        "        pred_images, pred_noise = self.denoise(\n",
        "            noisy_images, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels\n",
        "        )\n",
        "\n",
        "        image_loss = self.loss(images, pred_images)\n",
        "\n",
        "        self.image_loss_tracker.update_state(image_loss)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqYrG2VPD0Y7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LHcHZit9D0Y8"
      },
      "outputs": [],
      "source": [
        "# create and compile the model\n",
        "model = DiffusionModel(image_size, widths, block_depth, img_embed_size=img_embed_size, categories_nb=categories_nb, large_model=True)\n",
        "\n",
        "learning_rate = 1e-4\n",
        "t_epochs_nb=50\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    loss= tf.keras.losses.CategoricalCrossentropy(),\n",
        ")\n",
        "\n",
        "# run training and plot generated images periodically"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, batch_size=batch_size, epochs=t_epochs_nb, validation_data=(x_test,))"
      ],
      "metadata": {
        "id": "RDSktS3wIy4C",
        "outputId": "932ee64c-f8a3-41a3-8154-763b6c375537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "92/92 [==============================] - 201s 1s/step - i_loss: 0.5953 - val_i_loss: 1.3865\n",
            "Epoch 2/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.2769 - val_i_loss: 1.3859\n",
            "Epoch 3/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.2019 - val_i_loss: 1.3817\n",
            "Epoch 4/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.1678 - val_i_loss: 1.3671\n",
            "Epoch 5/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.1584 - val_i_loss: 1.3378\n",
            "Epoch 6/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.1534 - val_i_loss: 1.2856\n",
            "Epoch 7/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.1445 - val_i_loss: 1.2079\n",
            "Epoch 8/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.1365 - val_i_loss: 1.1291\n",
            "Epoch 9/50\n",
            "92/92 [==============================] - 116s 1s/step - i_loss: 0.1357 - val_i_loss: 1.0260\n",
            "Epoch 10/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.1217 - val_i_loss: 0.8896\n",
            "Epoch 11/50\n",
            "92/92 [==============================] - 116s 1s/step - i_loss: 0.1253 - val_i_loss: 0.7630\n",
            "Epoch 12/50\n",
            "92/92 [==============================] - 116s 1s/step - i_loss: 0.1227 - val_i_loss: 0.6543\n",
            "Epoch 13/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.1205 - val_i_loss: 0.5487\n",
            "Epoch 14/50\n",
            "92/92 [==============================] - 116s 1s/step - i_loss: 0.1131 - val_i_loss: 0.4436\n",
            "Epoch 15/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.1157 - val_i_loss: 0.4000\n",
            "Epoch 16/50\n",
            "92/92 [==============================] - 115s 1s/step - i_loss: 0.1099 - val_i_loss: 0.3027\n",
            "Epoch 17/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.1110 - val_i_loss: 0.2636\n",
            "Epoch 18/50\n",
            "92/92 [==============================] - 116s 1s/step - i_loss: 0.1119 - val_i_loss: 0.2084\n",
            "Epoch 19/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.1089 - val_i_loss: 0.1863\n",
            "Epoch 20/50\n",
            "92/92 [==============================] - 116s 1s/step - i_loss: 0.1009 - val_i_loss: 0.1692\n",
            "Epoch 21/50\n",
            "92/92 [==============================] - 116s 1s/step - i_loss: 0.1001 - val_i_loss: 0.1349\n",
            "Epoch 22/50\n",
            "92/92 [==============================] - 116s 1s/step - i_loss: 0.0982 - val_i_loss: 0.1498\n",
            "Epoch 23/50\n",
            "92/92 [==============================] - 116s 1s/step - i_loss: 0.0956 - val_i_loss: 0.1406\n",
            "Epoch 24/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.0964 - val_i_loss: 0.1023\n",
            "Epoch 25/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.0897 - val_i_loss: 0.0938\n",
            "Epoch 26/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.0912 - val_i_loss: 0.0822\n",
            "Epoch 27/50\n",
            "92/92 [==============================] - 116s 1s/step - i_loss: 0.0863 - val_i_loss: 0.0810\n",
            "Epoch 28/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.0880 - val_i_loss: 0.1027\n",
            "Epoch 29/50\n",
            "92/92 [==============================] - 114s 1s/step - i_loss: 0.0918 - val_i_loss: 0.0837\n",
            "Epoch 30/50\n",
            "92/92 [==============================] - 116s 1s/step - i_loss: 0.0875 - val_i_loss: 0.0829\n",
            "Epoch 31/50\n",
            "82/92 [=========================>....] - ETA: 12s - i_loss: 0.0846"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-fea734d692f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_epochs_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.network.summary()"
      ],
      "metadata": {
        "id": "Jcm7Vh6v9R0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_axis = np.arange(t_epochs_nb)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x_axis, history.history[\"i_loss\"], label=\"Training image CE loss\")\n",
        "plt.plot(x_axis, history.history[\"val_i_loss\"], label=\"Testing image CE loss\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efA4YUlX723d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_snr(signal_rate, noise_rate):\n",
        "    return tf.math.log(signal_rate / noise_rate)\n",
        "\n",
        "def get_t_from_snr(snr):\n",
        "    max_beta = 20.\n",
        "    min_beta = 0.1\n",
        "    return 2 * tf.math.log(tf.math.exp(-2*snr)+1) / tf.math.sqrt(min_beta**2 + 2\n",
        "                                                                 * (max_beta - min_beta)\n",
        "                                                                 * tf.math.log(tf.math.exp(-2*snr)+1)) + min_beta"
      ],
      "metadata": {
        "id": "WKvodkeuyJFb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fky6ewS3D0Y-"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm"
      ],
      "metadata": {
        "id": "PoJyZzMZqAIZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import xla\n",
        "def ddpm_solver(model, img_embed_size, image_size, batch_size=50, num_steps=350, eps=1e-3, mask=None, pixels=None):\n",
        "    second_order_alpha = 1.1\n",
        "    # T and schedule\n",
        "    t = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32)\n",
        "    noise_rates, signal_rates = model.diffusion_schedule(t)\n",
        "\n",
        "    if mask is None:\n",
        "        mask = tf.zeros((batch_size, image_size[0], image_size[1], 1), dtype=tf.float32)\n",
        "        pixels = tf.zeros((batch_size, image_size[0], image_size[1], img_embed_size), dtype=tf.float32)\n",
        "    else:\n",
        "        \n",
        "        pixels = tf.argmax(pixels, axis=-1)\n",
        "        pixels = model.embedding_layer(pixels)\n",
        "        pixels = tf.math.multiply(pixels, mask)\n",
        "\n",
        "        mask = tf.repeat(mask, batch_size, axis=0)\n",
        "        mask = tf.cast(mask,  dtype=tf.float32)\n",
        "        pixels = tf.repeat(pixels, batch_size, axis=0)\n",
        "\n",
        "    # Sample noise\n",
        "    uniform_init_x = tf.random.uniform((batch_size, image_size[0], image_size[1]), 0, 4, dtype=tf.dtypes.int32)\n",
        "    noises = tf.random.normal(shape=(batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "    init_x = signal_rates * model.embedding_layer(uniform_init_x) + noise_rates * noises\n",
        "\n",
        "    # Keep track of the chain\n",
        "    samples_list = []\n",
        "    samples_list.append( tf.keras.backend.constant(keras.utils.to_categorical(uniform_init_x)))\n",
        "\n",
        "    # Steps and other algorithmic variables\n",
        "    time_steps = tf.linspace(1., eps, num_steps)\n",
        "    step_size = time_steps[0] - time_steps[1]\n",
        "    x = init_x\n",
        "    #prev_alphas = signal_rates**2\n",
        "    prev_snr = compute_snr(signal_rates, noise_rates)\n",
        "    prev_noise_rates, prev_signal_rates = noise_rates, signal_rates\n",
        "\n",
        "    # INFERENCE REVERSE LOOP\n",
        "    for time_step in tqdm.tqdm(time_steps):\n",
        "        batch_time_step = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32) * time_step\n",
        "        noise_rates, signal_rates = model.diffusion_schedule(batch_time_step)\n",
        "        #cur_alphas = signal_rates**2\n",
        "        #betas = compute_beta(cur_alphas, prev_alphas)\n",
        "\n",
        "        # PREDICT IMAGE\n",
        "        x_input = tf.math.multiply(x, tf.math.abs(mask - 1))\n",
        "        pred_x0, pred_noise = model.denoise(x_input, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "        int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "        embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "        predicted_noise = x_input - embed_pred_x0\n",
        "\n",
        "        snr = compute_snr(signal_rates, noise_rates)\n",
        "        s_t = get_t_from_snr((snr + prev_snr) / 2)\n",
        "        s_noise_rates, s_signal_rates = model.diffusion_schedule(s_t)\n",
        "\n",
        "\n",
        "\n",
        "        u = s_signal_rates / prev_signal_rates * x - s_noise_rates * (tf.math.exp((snr - prev_snr) / 2) - 1) * predicted_noise\n",
        "\n",
        "        # PREDICT IMAGE 2\n",
        "        x_input = tf.math.multiply(u, tf.math.abs(mask - 1))\n",
        "        pred_x0, pred_noise = model.denoise(x_input, s_signal_rates, s_noise_rates, training=False, mask=mask, pixels=pixels)\n",
        "        int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "        embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "        predicted_noise = x_input - embed_pred_x0\n",
        "\n",
        "        x = signal_rates / prev_signal_rates * x - noise_rates * (tf.math.exp((snr - prev_snr)) - 1) * predicted_noise\n",
        "\n",
        "        # optional second order sampling\n",
        "        #if second_order_alpha is not None:\n",
        "        #    embed_pred_x0, pred_noises = second_order_correction(\n",
        "        #        model,\n",
        "        #        batch_time_step,\n",
        "        #        step_size,\n",
        "        #        x,\n",
        "        #        signal_rates,\n",
        "        #        noise_rates,\n",
        "        #        embed_pred_x0,\n",
        "        #        pred_noise,\n",
        "        #        second_order_alpha,\n",
        "        #        mask,\n",
        "        #        pixels,)\n",
        "\n",
        "        #mean_x0 = tf.math.sqrt(cur_alphas) * betas / (1 - prev_alphas) * embed_pred_x0\n",
        "        #mean_x = tf.math.sqrt(1 - betas) * (1 - cur_alphas) / (1 - prev_alphas) * x\n",
        "        #x = mean_x + mean_x0 + tf.reshape(tf.math.sqrt(betas), (-1, 1, 1, 1)) * tf.random.normal(x.shape)\n",
        "\n",
        "        samples_list.append(pred_x0)\n",
        "        #prev_alphas = cur_alphas\n",
        "        prev_snr = snr\n",
        "        prev_noise_rates, prev_signal_rates = noise_rates, signal_rates\n",
        "\n",
        "    return pred_x0, samples_list"
      ],
      "metadata": {
        "id": "RBs0D0Ts6qNn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def second_order_correction(\n",
        "    model,\n",
        "    diffusion_times,\n",
        "    step_size,\n",
        "    noisy_images,\n",
        "    signal_rates,\n",
        "    noise_rates,\n",
        "    pred_images,\n",
        "    pred_noises,\n",
        "    second_order_alpha,\n",
        "    mask,\n",
        "    pixels,\n",
        "):\n",
        "    # generic second-order Runge-Kutta method\n",
        "    # https://en.wikipedia.org/wiki/List_of_Runge%E2%80%93Kutta_methods#Generic_second-order_method\n",
        "    # based on https://arxiv.org/abs/2206.00364\n",
        "    #batch_size=noisy_images.shape[0]\n",
        "    #mask = tf.zeros((batch_size, image_size[0], image_size[1], 1))\n",
        "    #pixels = tf.zeros((batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "\n",
        "    # use first estimate to sample alpha steps away\n",
        "    alpha_signal_rates, alpha_noise_rates = model.diffusion_schedule(\n",
        "        diffusion_times - second_order_alpha * step_size\n",
        "    )\n",
        "    alpha_noisy_images = (\n",
        "        alpha_signal_rates * pred_images + alpha_noise_rates * pred_noises\n",
        "    )\n",
        "    x_input = tf.math.multiply(noisy_images, tf.math.abs(mask - 1))\n",
        "    pred_x0, alpha_pred_noises = model.denoise(x_input, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "    int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "    embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "    # linearly combine the two noise estimates\n",
        "    pred_noises = (1.0 - 1.0 / (2.0 * second_order_alpha)) * pred_noises + 1.0 / (\n",
        "        2.0 * second_order_alpha\n",
        "        ) * alpha_pred_noises\n",
        "\n",
        "    pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
        "    return pred_images, pred_noises"
      ],
      "metadata": {
        "id": "SEJciKNQzCGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def compute_beta(curr_alphas, prev_alphas):\n",
        "\n",
        "    betas = 1 - (prev_alphas / curr_alphas)\n",
        "    return betas"
      ],
      "metadata": {
        "id": "wVWp820FKVRV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import xla\n",
        "def ddpm_sampler(model, img_embed_size, image_size, batch_size=10, num_steps=200, eps=1e-3, mask=None, pixels=None):\n",
        "    second_order_alpha = 1.1\n",
        "    # T and schedule\n",
        "    t = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32)\n",
        "    noise_rates, signal_rates = model.diffusion_schedule(t)\n",
        "\n",
        "    if mask is None:\n",
        "        mask = tf.zeros((batch_size, image_size[0], image_size[1], 1), dtype=tf.float32)\n",
        "        pixels = tf.zeros((batch_size, image_size[0], image_size[1], img_embed_size), dtype=tf.float32)\n",
        "    else:\n",
        "        \n",
        "        pixels = tf.argmax(pixels, axis=-1)\n",
        "        pixels = model.embedding_layer(pixels)\n",
        "        pixels = tf.math.multiply(pixels, mask)\n",
        "\n",
        "        mask = tf.repeat(mask, batch_size, axis=0)\n",
        "        mask = tf.cast(mask,  dtype=tf.float32)\n",
        "        pixels = tf.repeat(pixels, batch_size, axis=0)\n",
        "\n",
        "    # Sample noise\n",
        "    uniform_init_x = tf.random.uniform((batch_size, image_size[0], image_size[1]), 0, 4, dtype=tf.dtypes.int32)\n",
        "    noises = tf.random.normal(shape=(batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "    init_x = signal_rates * model.embedding_layer(uniform_init_x) + noise_rates * noises\n",
        "\n",
        "    # Keep track of the chain\n",
        "    samples_list = []\n",
        "    samples_list.append( tf.keras.backend.constant(keras.utils.to_categorical(uniform_init_x)))\n",
        "\n",
        "    # Steps and other algorithmic variables\n",
        "    time_steps = tf.linspace(1., eps, num_steps)\n",
        "    step_size = time_steps[0] - time_steps[1]\n",
        "    x = init_x\n",
        "    prev_alphas = signal_rates**2\n",
        "\n",
        "    # INFERENCE REVERSE LOOP\n",
        "    for time_step in tqdm.tqdm(time_steps):\n",
        "        batch_time_step = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32) * time_step\n",
        "        noise_rates, signal_rates = model.diffusion_schedule(batch_time_step)\n",
        "        cur_alphas = signal_rates**2\n",
        "        betas = compute_beta(cur_alphas, prev_alphas)\n",
        "\n",
        "        # PREDICT IMAGE\n",
        "        x_input = tf.math.multiply(x, tf.math.abs(mask - 1))\n",
        "        pred_x0, pred_noise = model.denoise(x_input, noise_rates, signal_rates, training=False, mask=mask, pixels=pixels)\n",
        "        int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "        embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "\n",
        "        # optional second order sampling\n",
        "        if second_order_alpha is not None:\n",
        "            embed_pred_x0, pred_noises = second_order_correction(\n",
        "                model,\n",
        "                batch_time_step,\n",
        "                step_size,\n",
        "                x,\n",
        "                signal_rates,\n",
        "                noise_rates,\n",
        "                embed_pred_x0,\n",
        "                pred_noise,\n",
        "                second_order_alpha,\n",
        "                mask,\n",
        "                pixels,)\n",
        "\n",
        "        mean_x0 = tf.math.sqrt(cur_alphas) * betas / (1 - prev_alphas) * embed_pred_x0\n",
        "        mean_x = tf.math.sqrt(1 - betas) * (1 - cur_alphas) / (1 - prev_alphas) * x\n",
        "        x = mean_x + mean_x0 + tf.reshape(tf.math.sqrt(betas), (-1, 1, 1, 1)) * tf.random.normal(x.shape)\n",
        "\n",
        "        samples_list.append(pred_x0)\n",
        "        prev_alphas = cur_alphas\n",
        "\n",
        "    return pred_x0, samples_list"
      ],
      "metadata": {
        "id": "Krk0-qfIKNk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = ddpm_solver #@param ['ddpm_sampler', 'ddpm_solver'] {'type': 'raw'}\n",
        "\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                (64, 128),\n",
        "                                sample_batch_size,\n",
        "                                mask=None,\n",
        "                                pixels=None)"
      ],
      "metadata": {
        "id": "T0uowLytML_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f2585f-05fa-492b-ffaa-c88d50e73dfd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 350/350 [03:04<00:00,  1.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(sample_batch_size):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(np.argmax(samples[i].numpy(), axis=-1).reshape((64, 128)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "O4GzBJX-MUmk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f181962a-ac23-40a1-bba6-ca3e1135de8f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data.load_data import ConditionalDataGenerator\n",
        "\n",
        "# Load Data\n",
        "slice_size = (64, 128, 4)\n",
        "dataloader = ConditionalDataGenerator(x_test, 1, slice_size, wells=10, mode=3)\n",
        "pixels, mask, ground_truth = dataloader.__getitem__(0)\n",
        "print(pixels.shape, mask.shape, ground_truth.shape)"
      ],
      "metadata": {
        "id": "LAAAFTajBiUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = ddpm_sampler #@param ['ddpm_sampler', 'pc_sampler'] {'type': 'raw'}\n",
        "\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                (64, 128),\n",
        "                                sample_batch_size,\n",
        "                                mask=mask,\n",
        "                                pixels=pixels)"
      ],
      "metadata": {
        "id": "iGnksfxx3JRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(sample_batch_size):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(np.argmax(samples[i].numpy(), axis=-1).reshape((64, 128)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QjbvDyWoK7fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from utils.visualisation import *\n",
        "from data.load_data import get_3d_flumy_data, load_data, ConditionalDataGenerator\n",
        "from models.load_trained_models import load_msgen_horizontal, wgan_horizontal,\\\n",
        "    load_msnwgen_2d_gs_horizontal, load_wgan_gs_horizontal, load_mswgen_sn_3d_horizontal\n",
        "from utils.utils import generate_noise, correct_percentage\n"
      ],
      "metadata": {
        "id": "pP5_ix6TCLdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_simulations = 3\n",
        "cmap, norm = get_color_map(number_of_categories=4)\n",
        "\n",
        "print_conditioned_results(ground_truth, samples, mask, nb_simulations, cmap, norm)"
      ],
      "metadata": {
        "id": "0nla2sslCFTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.axis('off')\n",
        "\n",
        "plt.imshow(np.argmax(ground_truth, axis=-1).reshape((64, 128)),\n",
        "            interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "URj4sIq1BlYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = ddpm_sampler #@param ['ddpm_sampler', 'pc_sampler'] {'type': 'raw'}\n",
        "\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                (128, 256),\n",
        "                                sample_batch_size,)"
      ],
      "metadata": {
        "id": "gzMLizIXPUAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(sample_batch_size):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(np.argmax(samples[i].numpy(), axis=-1).reshape((128, 256)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "p88yuQgCPUA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GIF"
      ],
      "metadata": {
        "id": "8X9v42ujwnt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "images = []\n",
        "for i, batch in enumerate(samples_list):\n",
        "    figure = plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Timestep {0:.3f}\".format(1 - (i / 350)))\n",
        "    plt.imshow(np.argmax(batch[0].numpy(), axis=-1).reshape((64, 128)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.savefig('foo.png', bbox_inches='tight')\n",
        "    images.append(imageio.imread('foo.png'))\n",
        "    plt.show() #close(figure)\n",
        "imageio.mimsave('/movie.gif', images)"
      ],
      "metadata": {
        "id": "m9JzX8pVMf7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Models"
      ],
      "metadata": {
        "id": "GCPKYflD2fyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "SAVE_AND_TAR_RESULTS_WEIGHTS = True\n",
        "\n",
        "if SAVE_AND_TAR_RESULTS_WEIGHTS:\n",
        "  diffusion_checkpoint_path = \"diffusion_weights_horiz/cp-diffusion2d_net_horiz.ckpt\"\n",
        "  diffusion_checkpoint_dir = os.path.dirname(diffusion_checkpoint_path)\n",
        "\n",
        "  model.ema_network.save_weights(diffusion_checkpoint_path)\n",
        "\n",
        "  !tar -czvf diffusion_weights_horiz.tar.gz ./diffusion_weights_horiz\n",
        "\n",
        "  diffusion_checkpoint_path = \"diffusion_weights_horiz/cp-diffusion2d_embed_horiz.ckpt\"\n",
        "  diffusion_checkpoint_dir = os.path.dirname(diffusion_checkpoint_path)\n",
        "\n",
        "  model.embedding_layer.save_weights(diffusion_checkpoint_path)\n",
        "\n",
        "  !tar -czvf diffusion_weights_horiz.tar.gz ./diffusion_weights_horiz"
      ],
      "metadata": {
        "id": "gi9sVjP0QHdb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ddim",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}