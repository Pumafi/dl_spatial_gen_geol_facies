{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pumafi/dl_spatial_gen_geol_facies/blob/main/ddim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## IDEAS \n",
        "1.   Normalization in embedding ?\n",
        "2.   Formula when I replace beta by t\n",
        "3.   Use keras inference for potential changes\n",
        "\n",
        "## What to try next?\n",
        "\n",
        "If you would like to dive in deeper to the topic, a recommend checking out\n",
        "[this repository](https://github.com/beresandras/clear-diffusion-keras) that I created in\n",
        "preparation for this code example, which implements a wider range of features in a\n",
        "similar style, such as:\n",
        "\n",
        "* stochastic sampling\n",
        "* second-order sampling based on the\n",
        "[differential equation view of DDIMs (Equation 13)](https://arxiv.org/abs/2010.02502)\n",
        "* more diffusion schedules\n",
        "* more network output types: predicting image or\n",
        "[velocity (Appendix D)](https://arxiv.org/abs/2202.00512) instead of noise\n",
        "* more datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "rge41L-HIY-i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqkfNOJcD0Ym"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RUNNING_IN_COLAB = True\n",
        "\n",
        "if RUNNING_IN_COLAB:\n",
        "    # Uses a private Auth Token, giving read and write access to repo\n",
        "    # TO DELETE IF REPO GOES PUBLIC\n",
        "    REPO_URL = 'https://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@github.com/Pumafi/flumy-wgan-mines'\n",
        "    BRANCH   = 'main'\n",
        "    REPO_DIR = 'flumy-wgan-mines'\n",
        "\n",
        "    from pathlib import Path\n",
        "\n",
        "    %cd /content\n",
        "\n",
        "    if Path(REPO_DIR).is_dir():\n",
        "      !rm -rf {REPO_DIR}\n",
        "\n",
        "    # Download the repository\n",
        "    if not Path(REPO_DIR).is_dir():\n",
        "        !git clone --branch {BRANCH} --depth=1 -- {REPO_URL} {REPO_DIR}\n",
        "    \n",
        "    %cd {REPO_DIR}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqPH8VxsGGrb",
        "outputId": "5dc6ffff-209e-41c6-97fb-de52d7b38604"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'flumy-wgan-mines'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 146 (delta 31), reused 74 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 140.19 MiB | 14.18 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n",
            "Updating files: 100% (121/121), done.\n",
            "/content/flumy-wgan-mines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3E8qnnCGH5K",
        "outputId": "1d692fa8-b033-459e-988b-2c080a574449"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2VQcRg8KD0Yn",
        "outputId": "d8443f6a-fd2a-4e37-8aad-9a109ea9dc2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from data.load_data import load_data\n",
        "from utils.visualisation import get_color_map\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "1eF1rmySGCzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful constants\n",
        "image_size = (64, 128)\n",
        "cmap, norm = get_color_map(number_of_categories=4)\n",
        "facies_names = np.array([\"Sand, Channel lag\", \"Sand, Point bar\", \"Silts, Levee\", \"Shale, Overbank\"])\n",
        "x = load_data(image_size[0], image_size[1], \"./data/horizontal/dataFlumyHoriz.csv\")\n",
        "x_train = x[:2760]\n",
        "x_test = x[2760:]"
      ],
      "metadata": {
        "id": "4yPyDmhUGCTa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7-ElzPrD0Yq"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FBoSc7iCD0Ys"
      },
      "outputs": [],
      "source": [
        "# sampling\n",
        "min_signal_rate = 0.02\n",
        "max_signal_rate = 0.95\n",
        "\n",
        "# architecture\n",
        "widths = [32, 64, 128, 256]\n",
        "block_depth = 2\n",
        "\n",
        "# Data values embedding\n",
        "img_embed_size = 16\n",
        "categories_nb = 4\n",
        "\n",
        "# optimization\n",
        "batch_size = 30\n",
        "ema = 0.999\n",
        "learning_rate = 1e-4\n",
        "embeding_net_lr = 1e-3\n",
        "weight_decay = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "QxyZaS_UJ_YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianFourierProjection(tf.keras.layers.Layer):\n",
        "    \"\"\"Gaussian random features for encoding time steps.\"\"\"  \n",
        "    def __init__(self, embed_dim, scale=30.):\n",
        "        super().__init__()\n",
        "        # Randomly sample weights during initialization. These weights are fixed \n",
        "        # during optimization and are not trainable.\n",
        "        self.W = self.add_weight(shape=(embed_dim // 2,),\n",
        "                                 trainable=False,\n",
        "                                 initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.), name=\"GFP\") * tf.constant(scale, dtype=tf.float32)\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        x_proj = x * self.W * tf.constant(2., dtype=tf.float32) * tf.constant(np.pi, dtype=tf.float32)\n",
        "        y = tf.concat([tf.math.sin(x_proj), tf.cos(x_proj)], axis=-1)\n",
        "        return y # Probleme vient pas de là :()\n",
        "\n",
        "class CustomLinear(tf.keras.layers.Layer):\n",
        "    \"\"\"Rhaaah.\"\"\"  \n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.W = tf.random.uniform((input_dim, output_dim), minval=-tf.math.sqrt(1/input_dim), maxval=tf.math.sqrt(1/input_dim))\n",
        "        self.b = tf.random.uniform((1, output_dim, ), minval=-tf.math.sqrt(1/input_dim), maxval=tf.math.sqrt(1/input_dim))\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        y = tf.tensordot(x, self.W, 1) + self.b\n",
        "        y = tf.keras.activations.gelu(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "@tf.function\n",
        "def embedding_normalization(logits):\n",
        "    # normalement vont avoir taille (batch_size, sequence_size, embedding_size)\n",
        "    # axis=-1 is embedding normalement\n",
        "    return (logits / tf.norm(logits, axis=-1, keepdims=True)) * tf.constant(np.sqrt(logits.shape[-1]), dtype=tf.float32)\n",
        "\n",
        "class NormalizedEmbedding(tf.keras.layers.Layer):\n",
        "    \"\"\"\"\"\"  \n",
        "    def __init__(self, categories_nb, img_embed_size):\n",
        "        super().__init__()\n",
        "        self.embed_layer = tf.keras.layers.Embedding(categories_nb, img_embed_size)\n",
        "        self.embed_layer2 = layers.Conv2D(img_embed_size, kernel_size=3, padding=\"same\", activation=keras.activations.swish)\n",
        "        self.embed_layer3 = layers.Conv2D(img_embed_size, kernel_size=3, padding=\"same\", activation=keras.activations.swish)\n",
        "        self.embed_layer4 = layers.Conv2D(img_embed_size, kernel_size=1, activation=None)\n",
        "        self.layer_norm = layer = tf.keras.layers.LayerNormalization(axis=[1, 2])\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        y = self.embed_layer(x)\n",
        "        y = self.embed_layer2(y)\n",
        "        y = self.embed_layer3(y)\n",
        "        y = self.embed_layer4(y)\n",
        "        y = embedding_normalization(y)\n",
        "        y = self.layer_norm(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "Ej4nARwoGmme"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7V5eQBuUD0Y0"
      },
      "outputs": [],
      "source": [
        "def sinusoidal_embedding(x):\n",
        "    embedding_min_frequency = 1.0\n",
        "    frequencies = tf.exp(\n",
        "        tf.linspace(\n",
        "            tf.math.log(embedding_min_frequency),\n",
        "            tf.math.log(embedding_max_frequency),\n",
        "            embedding_dims // 2,\n",
        "        )\n",
        "    )\n",
        "    angular_speeds = 2.0 * math.pi * frequencies\n",
        "    embeddings = tf.concat(\n",
        "        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n",
        "    )\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def ResidualBlock(width):\n",
        "    def apply(x):\n",
        "        input_width = x.shape[3]\n",
        "        if input_width == width:\n",
        "            residual = x\n",
        "        else:\n",
        "            residual = layers.Conv2D(width, kernel_size=1)(x)\n",
        "        x = layers.BatchNormalization(center=False, scale=False)(x)\n",
        "        x = layers.Conv2D(\n",
        "            width, kernel_size=3, padding=\"same\", activation=keras.activations.swish\n",
        "        )(x)\n",
        "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "        x = layers.Add()([x, residual])\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def DownBlock(width, block_depth):\n",
        "    def apply(x):\n",
        "        x, skips = x\n",
        "        for _ in range(block_depth):\n",
        "            x = ResidualBlock(width)(x)\n",
        "            skips.append(x)\n",
        "        x = layers.AveragePooling2D(pool_size=2)(x)\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def UpBlock(width, block_depth):\n",
        "    def apply(x):\n",
        "        x, skips = x\n",
        "        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
        "        for _ in range(block_depth):\n",
        "            x = layers.Concatenate()([x, skips.pop()])\n",
        "            x = ResidualBlock(width)(x)\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def get_network(image_size, widths, block_depth, embed_size):\n",
        "    noisy_images = keras.Input(shape=(image_size[0], image_size[1], embed_size))\n",
        "    noise_variances = keras.Input(shape=(1, 1, 1))\n",
        "\n",
        "    e = layers.Lambda(sinusoidal_embedding)(noise_variances)\n",
        "    e = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(e)\n",
        "\n",
        "    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_images)\n",
        "    x = layers.Concatenate()([x, e])\n",
        "\n",
        "    skips = []\n",
        "    for width in widths[:-1]:\n",
        "        x = DownBlock(width, block_depth)([x, skips])\n",
        "\n",
        "    for _ in range(block_depth):\n",
        "        x = ResidualBlock(widths[-1])(x)\n",
        "\n",
        "    for width in reversed(widths[:-1]):\n",
        "        x = UpBlock(width, block_depth)([x, skips])\n",
        "\n",
        "    x = layers.Conv2D(4, kernel_size=1, kernel_initializer=\"zeros\", activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model([noisy_images, noise_variances], x, name=\"residual_unet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network(\n",
        "    image_size,\n",
        "    noise_embedding_max_frequency,\n",
        "    noise_embedding_dims,\n",
        "    image_embedding_dims,\n",
        "    block_depth,\n",
        "    widths,\n",
        "    attentions,\n",
        "    patch_size,\n",
        "    embed_size\n",
        "):\n",
        "    def EmbeddingLayer(embedding_max_frequency, embedding_dims):\n",
        "        def sinusoidal_embedding(x):\n",
        "            embedding_min_frequency = 1.0\n",
        "            frequencies = tf.exp(\n",
        "                tf.linspace(\n",
        "                    tf.math.log(embedding_min_frequency),\n",
        "                    tf.math.log(embedding_max_frequency),\n",
        "                    embedding_dims // 2,\n",
        "                )\n",
        "            )\n",
        "            angular_speeds = 2.0 * math.pi * frequencies\n",
        "            embeddings = tf.concat(\n",
        "                [\n",
        "                    tf.sin(angular_speeds * x),\n",
        "                    tf.cos(angular_speeds * x),\n",
        "                ],\n",
        "                axis=3,\n",
        "            )\n",
        "            return embeddings\n",
        "\n",
        "        def forward(x):\n",
        "            x = layers.Lambda(sinusoidal_embedding)(x)\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def ResidualBlock(width, attention):\n",
        "        def forward(x):\n",
        "            x, n = x\n",
        "            input_width = x.shape[3]\n",
        "            if input_width == width:\n",
        "                residual = x\n",
        "            else:\n",
        "                residual = layers.Conv2D(width, kernel_size=1)(x)\n",
        "\n",
        "            n = layers.Dense(width)(n)\n",
        "\n",
        "            x = tfa.layers.GroupNormalization(groups=8)(x)\n",
        "            x = keras.activations.swish(x)\n",
        "            x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "\n",
        "            x = layers.Add()([x, n])\n",
        "\n",
        "            x = tfa.layers.GroupNormalization(groups=8)(x)\n",
        "            x = keras.activations.swish(x)\n",
        "            x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "\n",
        "            x = layers.Add()([residual, x])\n",
        "\n",
        "            if attention:\n",
        "                residual = x\n",
        "                x = tfa.layers.GroupNormalization(groups=8, center=False, scale=False)(\n",
        "                    x\n",
        "                )\n",
        "                x = layers.MultiHeadAttention(\n",
        "                    num_heads=4, key_dim=width, attention_axes=(1, 2)\n",
        "                )(x, x)\n",
        "\n",
        "                x = layers.Add()([residual, x])\n",
        "\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def DownBlock(block_depth, width, attention):\n",
        "        def forward(x):\n",
        "            x, n, skips = x\n",
        "            for _ in range(block_depth):\n",
        "                x = ResidualBlock(width, attention)([x, n])\n",
        "                skips.append(x)\n",
        "            x = layers.AveragePooling2D(pool_size=2)(x)\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    def UpBlock(block_depth, width, attention):\n",
        "        def forward(x):\n",
        "            x, n, skips = x\n",
        "            x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
        "            for _ in range(block_depth):\n",
        "                x = layers.Concatenate()([x, skips.pop()])\n",
        "                x = ResidualBlock(width, attention)([x, n])\n",
        "            return x\n",
        "\n",
        "        return forward\n",
        "\n",
        "    images = keras.Input(shape=(image_size[0], image_size[1], embed_size))\n",
        "    noise_powers = keras.Input(shape=(1, 1, 1))\n",
        "\n",
        "    x = layers.Conv2D(image_embedding_dims, kernel_size=patch_size, strides=patch_size)(\n",
        "        images\n",
        "    )\n",
        "\n",
        "    n = EmbeddingLayer(noise_embedding_max_frequency, noise_embedding_dims)(\n",
        "        noise_powers\n",
        "    )\n",
        "    n = layers.Dense(noise_embedding_dims, activation=keras.activations.swish)(n)\n",
        "    n = layers.Dense(noise_embedding_dims, activation=keras.activations.swish)(n)\n",
        "\n",
        "    skips = []\n",
        "    for width, attention in zip(widths[:-1], attentions[:-1]):\n",
        "        x = DownBlock(block_depth, width, attention)([x, n, skips])\n",
        "\n",
        "    for _ in range(block_depth):\n",
        "        x = ResidualBlock(widths[-1], attentions[-1])([x, n])\n",
        "\n",
        "    for width, attention in zip(widths[-2::-1], attentions[-2::-1]):\n",
        "        x = UpBlock(block_depth, width, attention)([x, n, skips])\n",
        "\n",
        "    x = layers.Conv2DTranspose(\n",
        "        4, kernel_size=patch_size, strides=patch_size, kernel_initializer=\"zeros\", activation=\"softmax\"\n",
        "    )(x)\n",
        "\n",
        "    return keras.Model([images, noise_powers], x, name=\"residual_unet\")"
      ],
      "metadata": {
        "id": "IgeH9voVvsN5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lZ37576YD0Y5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DiffusionModel(keras.Model):\n",
        "    def __init__(self, image_size, widths, block_depth, img_embed_size, categories_nb, embedding_lr=1e-3, batch_size=30):\n",
        "        super().__init__()\n",
        "\n",
        "        #embedding_dims = 32\n",
        "        #embedding_max_frequency = 1000.0\n",
        "        noise_embedding_max_frequency = 1000.0\n",
        "        noise_embedding_dims = 32\n",
        "        image_embedding_dims = 64\n",
        "        block_depth = 2\n",
        "        widths = [64, 128, 256, 512]\n",
        "        attentions = [False, False, True, True]\n",
        "        patch_size = 1\n",
        "\n",
        "        self.network = get_network(image_size, noise_embedding_max_frequency,\n",
        "                                   noise_embedding_dims, image_embedding_dims,\n",
        "                                   block_depth, widths, attentions, patch_size,\n",
        "                                   img_embed_size)\n",
        "        \n",
        "        self.ema_network = keras.models.clone_model(self.network)\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Embedding\n",
        "        self.img_embed_size = img_embed_size\n",
        "        self.embedding_layer = NormalizedEmbedding(categories_nb, img_embed_size)\n",
        "        self.emb_optimiser = tf.keras.optimizers.legacy.Adam(learning_rate=embedding_lr)\n",
        "\n",
        "    def compile(self, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.image_loss_tracker]\n",
        "\n",
        "\n",
        "    def diffusion_schedule(self, diffusion_times):\n",
        "        # diffusion times -> angles\n",
        "        start_angle = tf.acos(max_signal_rate)\n",
        "        end_angle = tf.acos(min_signal_rate)\n",
        "\n",
        "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
        "\n",
        "        # angles -> signal and noise rates\n",
        "        signal_rates = tf.cos(diffusion_angles)\n",
        "        noise_rates = tf.sin(diffusion_angles)\n",
        "        # note that their squared sum is always: sin^2(x) + cos^2(x) = 1\n",
        "\n",
        "        return noise_rates, signal_rates\n",
        "\n",
        "    def denoise(self, noisy_images, noise_rates, signal_rates, training):\n",
        "        # the exponential moving average weights are used at evaluation\n",
        "        if training:\n",
        "            network = self.network\n",
        "        else:\n",
        "            network = self.ema_network\n",
        "\n",
        "        # predict noise component and calculate the image component using it\n",
        "        pred_images = network([noisy_images, noise_rates**2], training=training)\n",
        "\n",
        "        return pred_images\n",
        "\n",
        "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
        "        # reverse diffusion = sampling\n",
        "        num_images = initial_noise.shape[0]\n",
        "        step_size = 1.0 / diffusion_steps\n",
        "\n",
        "        # important line:\n",
        "        # at the first sampling step, the \"noisy image\" is pure noise\n",
        "        # but its signal rate is assumed to be nonzero (min_signal_rate)\n",
        "        next_noisy_images = initial_noise\n",
        "        for step in range(diffusion_steps):\n",
        "            noisy_images = next_noisy_images\n",
        "\n",
        "            # separate the current noisy image to its components\n",
        "            diffusion_times = tf.ones((num_images, 1, 1, 1)) - step * step_size\n",
        "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "            pred_noises, pred_images = self.denoise(\n",
        "                noisy_images, noise_rates, signal_rates, training=False\n",
        "            )\n",
        "            # network used in eval mode\n",
        "\n",
        "            # remix the predicted components using the next signal and noise rates\n",
        "            next_diffusion_times = diffusion_times - step_size\n",
        "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
        "                next_diffusion_times\n",
        "            )\n",
        "            next_noisy_images = (\n",
        "                next_signal_rates * pred_images + next_noise_rates * pred_noises\n",
        "            )\n",
        "            # this new noisy image will be used in the next step\n",
        "\n",
        "        return pred_images\n",
        "\n",
        "    def generate(self, num_images, diffusion_steps):\n",
        "        # noise -> images -> denormalized images\n",
        "        initial_noise = tf.random.normal(shape=(num_images, image_size, image_size, 3))\n",
        "        generated_images = self.reverse_diffusion(initial_noise, diffusion_steps)\n",
        "        generated_images = self.denormalize(generated_images)\n",
        "        return generated_images\n",
        "\n",
        "    def train_step(self, images):\n",
        "        # normalize images to have standard deviation of 1, like the noises\n",
        "        noises = tf.random.normal(shape=(self.batch_size, self.image_size[0], self.image_size[1], self.img_embed_size))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = tf.random.uniform(\n",
        "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        # mix the images with noises accordingly\n",
        "        int_encoded_img = tf.argmax(images, axis=-1)\n",
        "\n",
        "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "            embed_images = self.embedding_layer(int_encoded_img)\n",
        "            noisy_images = signal_rates * embed_images + noise_rates * noises\n",
        "\n",
        "            # train the network to separate noisy images to their components\n",
        "            pred_images = self.denoise(\n",
        "                noisy_images, noise_rates, signal_rates, training=True\n",
        "            )\n",
        "\n",
        "            image_loss = self.loss(images, pred_images)  # training loss\n",
        "            \n",
        "\n",
        "        gradients_model = tape1.gradient(image_loss, self.network.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients_model, self.network.trainable_weights))\n",
        "\n",
        "        gradients_embeddings = tape2.gradient(image_loss, self.embedding_layer.trainable_weights)\n",
        "        self.emb_optimiser.apply_gradients(zip(gradients_embeddings, self.embedding_layer.trainable_weights))\n",
        "\n",
        "        self.image_loss_tracker.update_state(image_loss)\n",
        "\n",
        "        # track the exponential moving averages of weights\n",
        "        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
        "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
        "\n",
        "        # KID is not measured during the training phase for computational efficiency\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, images):\n",
        "        noises = tf.random.normal(shape=(self.batch_size, self.image_size[0], self.image_size[1], self.img_embed_size))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = tf.random.uniform(\n",
        "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "        int_encoded_img = tf.argmax(images, axis=-1)\n",
        "\n",
        "        embed_images = self.embedding_layer(int_encoded_img)\n",
        "\n",
        "        #std = marginal_prob_std(diffusion_times, sigma=sigma)\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        noisy_images = signal_rates * embed_images + noise_rates * noises\n",
        "        #noisy_images = embed_images + noises * tf.reshape(std, (-1, 1, 1, 1))\n",
        "\n",
        "        # use the network to separate noisy images to their components\n",
        "        pred_images = self.denoise(\n",
        "            noisy_images, noise_rates, signal_rates, training=False\n",
        "        )\n",
        "\n",
        "        image_loss = self.loss(images, pred_images)\n",
        "\n",
        "        self.image_loss_tracker.update_state(image_loss)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqYrG2VPD0Y7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LHcHZit9D0Y8"
      },
      "outputs": [],
      "source": [
        "# create and compile the model\n",
        "model = DiffusionModel(image_size, widths, block_depth, img_embed_size=img_embed_size, categories_nb=categories_nb)\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    loss= tf.keras.losses.CategoricalCrossentropy(),\n",
        ")\n",
        "\n",
        "# run training and plot generated images periodically"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, batch_size=batch_size, epochs=100, validation_data=(x_test,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDSktS3wIy4C",
        "outputId": "4a33e82f-a01f-45b7-b56b-0a5bb7bf55fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 107s 1s/step - i_loss: 0.0219 - val_i_loss: 0.0112\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 109s 1s/step - i_loss: 0.0186 - val_i_loss: 0.0149\n",
            "Epoch 3/100\n",
            "10/92 [==>...........................] - ETA: 1:34 - i_loss: 0.0229"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fky6ewS3D0Y-"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "def compute_beta(curr_alphas, prev_alphas):\n",
        "\n",
        "    betas = 1 - (prev_alphas / curr_alphas)\n",
        "    return betas"
      ],
      "metadata": {
        "id": "wVWp820FKVRV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import xla\n",
        "def ddpm_sampler(model, img_embed_size, batch_size=10, num_steps=350, eps=1e-3):\n",
        "    # T and schedule\n",
        "    t = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32)\n",
        "    noise_rates, signal_rates = model.diffusion_schedule(t)\n",
        "\n",
        "    # Sample noise\n",
        "    uniform_init_x = tf.random.uniform((batch_size, 64, 128), 0, 4, dtype=tf.dtypes.int32)\n",
        "    noises = tf.random.normal(shape=(batch_size, image_size[0], image_size[1], img_embed_size))\n",
        "    init_x = signal_rates * model.embedding_layer(uniform_init_x) + noise_rates * noises\n",
        "\n",
        "    # Keep track of the chain\n",
        "    samples_list = []\n",
        "    samples_list.append( tf.keras.backend.constant(keras.utils.to_categorical(uniform_init_x)))\n",
        "\n",
        "    # Steps and other algorithmic variables\n",
        "    time_steps = tf.linspace(1., eps, num_steps)\n",
        "    step_size = time_steps[0] - time_steps[1]\n",
        "    x = init_x\n",
        "    prev_alphas = signal_rates**2\n",
        "\n",
        "    # INFERENCE REVERSE LOOP\n",
        "    for time_step in tqdm.tqdm(time_steps):\n",
        "        batch_time_step = tf.ones((batch_size, 1, 1, 1), dtype=tf.float32) * time_step\n",
        "        noise_rates, signal_rates = model.diffusion_schedule(batch_time_step)\n",
        "        cur_alphas = signal_rates**2\n",
        "        betas = compute_beta(cur_alphas, prev_alphas)\n",
        "\n",
        "        # PREDICT IMAGE\n",
        "        pred_x0 = model.denoise(x, noise_rates, signal_rates, training=False)\n",
        "        int_encoded_img = tf.argmax(pred_x0, axis=-1)\n",
        "        embed_pred_x0 = model.embedding_layer(int_encoded_img)\n",
        "        \n",
        "        mean_x0 = tf.math.sqrt(cur_alphas) * betas / (1 - prev_alphas) * embed_pred_x0\n",
        "        mean_x = tf.math.sqrt(1 - betas) * (1 - cur_alphas) / (1 - prev_alphas) * x\n",
        "        x = mean_x + mean_x0 + tf.reshape(tf.math.sqrt(betas), (-1, 1, 1, 1)) * tf.random.normal(x.shape)\n",
        "\n",
        "        samples_list.append(pred_x0)\n",
        "        prev_alphas = cur_alphas\n",
        "\n",
        "    return pred_x0, samples_list"
      ],
      "metadata": {
        "id": "Krk0-qfIKNk1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Sampling (double click to expand or collapse)\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "\n",
        "sample_batch_size = 10 #@param {'type':'integer'}\n",
        "sampler = ddpm_sampler\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples, samples_list = sampler(model,\n",
        "                                img_embed_size,\n",
        "                                sample_batch_size,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0uowLytML_4",
        "outputId": "66075f19-864f-49e1-cf85-2ba4a377d85b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 350/350 [01:27<00:00,  3.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.axis('off')\n",
        "\n",
        "plt.imshow(np.argmax(samples[0].numpy(), axis=-1).reshape((64, 128)),\n",
        "            interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "O4GzBJX-MUmk",
        "outputId": "56e95421-e1d7-4b67-ce54-7508dd0040f8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.axis('off')\n",
        "\n",
        "plt.imshow(np.argmax(samples[1].numpy(), axis=-1).reshape((64, 128)),\n",
        "            interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "3Wt_CCXrMeje",
        "outputId": "d37a6f2b-3ab7-479f-f8a1-c32630a6daf8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "images = []\n",
        "for i, batch in enumerate(samples_list):\n",
        "    figure = plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Timestep {0:.3f}\".format(1 - (i / 350)))\n",
        "    plt.imshow(np.argmax(batch[0].numpy(), axis=-1).reshape((64, 128)),\n",
        "                interpolation='nearest', cmap=cmap, norm=norm)\n",
        "    plt.savefig('foo.png', bbox_inches='tight')\n",
        "    images.append(imageio.imread('foo.png'))\n",
        "    plt.show() #close(figure)\n",
        "imageio.mimsave('/movie.gif', images)"
      ],
      "metadata": {
        "id": "m9JzX8pVMf7L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ddim",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}